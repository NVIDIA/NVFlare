readme_am: |
  *********************************
  Admin Client package
  *********************************
  The package includes at least the following files:
  readme.txt
  rootCA.pem
  client.crt
  client.key
  fl_admin.sh
  
  Please install the nvflare package by 'python3 -m pip nvflare.'  This will install a set of Python codes
  in your environment.  After installation, you can run the fl_admin.sh file to start communicating to the admin server.

  The rootCA.pem file is pointed by "ca_cert" in fl_admin.sh.  If you plan to move/copy it to a different place,
  you will need to modify fl_admin.sh.  The same applies to the other two files, client.crt and client.key.

  The email in your submission to participate this Federated Learning project is embedded in the CN field of client
  certificate, which uniquely identifies the participant.  As such, please safeguard its private key, client.key.

readme_fc: |
  *********************************
  Federated Learning Client package
  *********************************
  The package includes at least the following files:
  readme.txt
  rootCA.pem
  client.crt
  client.key
  fed_client.json
  start.sh
  sub_start.sh
  stop_fl.sh

  Run start.sh to start the client.

  The rootCA.pem file is pointed by "ssl_root_cert" in fed_client.json.  If you plan to move/copy it to a different place,
  you will need to modify fed_client.json.  The same applies to the other two files, client.crt and client.key.

  The client name in your submission to participate this Federated Learning project is embedded in the CN field of client
  certificate, which uniquely identifies the participant.  As such, please safeguard its private key, client.key.

readme_fs: |
  *********************************
  Federated Learning Server package
  *********************************
  The package includes at least the following files:
  readme.txt
  rootCA.pem
  server.crt
  server.key
  authorization.json
  fed_server.json
  start.sh
  sub_start.sh
  stop_fl.sh
  signature.json

  Run start.sh to start the server.

  The rootCA.pem file is pointed by "ssl_root_cert" in fed_server.json.  If you plan to move/copy it to a different place,
  you will need to modify fed_server.json.  The same applies to the other two files, server.crt and server.key.

  Please always safeguard the server.key.

gunicorn_conf_py: |
  bind="0.0.0.0:{~~port~~}"
  cert_reqs=2
  do_handshake_on_connect=True
  timeout=30
  worker_class="nvflare.ha.overseer.worker.ClientAuthWorker"
  workers=1
  wsgi_app="nvflare.ha.overseer.overseer:app"

local_client_resources: |
  {
    "format_version": 2,
    "client": {
      "retry_timeout": 30,
      "compression": "Gzip"
    },
    "components": [
      {
        "id": "resource_manager",
        "path": "nvflare.app_common.resource_managers.gpu_resource_manager.GPUResourceManager",
        "args": {
          "num_of_gpus": 0,
          "mem_per_gpu_in_GiB": 0
        }
      },
      {
        "id": "resource_consumer",
        "path": "nvflare.app_common.resource_consumers.gpu_resource_consumer.GPUResourceConsumer",
        "args": {}
      }
    ]
  }

fed_client: |
  {
    "format_version": 2,
    "servers": [
      {
        "name": "spleen_segmentation",
        "service": {
          "options": [
            ["grpc.max_send_message_length",    2147483647],
            ["grpc.max_receive_message_length", 2147483647]
          ]
        }
      }
    ],
    "client": {
      "ssl_private_key": "client.key",
      "ssl_cert": "client.crt",
      "ssl_root_cert": "rootCA.pem"
    }
  }

sample_privacy: |
  {
    "scopes": [
      {
        "name": "public",
        "properties": {
          "train_dataset": "/data/public/train",
          "val_dataset": "/data/public/val"
        },
        "task_result_filters": [
          {
            "name": "AddNoiseToMinMax",
            "args": {
              "min_noise_level": 0.2,
              "max_noise_level": 0.2
            }
          },
          {
            "name": "PercentilePrivacy",
            "args": {
              "percentile": 10,
              "gamma": 0.02
            }
          }
        ],
        "task_data_filters": [
          {
            "name": "BadModelDetector"
          }
        ]
      },
      {
        "name": "private",
        "properties": {
          "train_dataset": "/data/private/train",
          "val_dataset": "/data/private/val"
        },
        "task_result_filters": [
          {
            "name": "AddNoiseToMinMax",
            "args": {
              "min_noise_level": 0.1,
              "max_noise_level": 0.1
            }
          },
          {
            "name": "SVTPrivacy",
            "args": {
              "fraction": 0.1,
              "epsilon": 0.2
            }
          }
        ]
      }
    ],
    "default_scope": "public"
  }

local_server_resources: |
  {
      "format_version": 2,
      "servers": [
          {
              "admin_storage": "transfer",
              "max_num_clients": 100,
              "heart_beat_timeout": 600,
              "num_server_workers": 4,
              "download_job_url": "http://download.server.com/",
              "compression": "Gzip"
          }
      ],
      "snapshot_persistor": {
          "path": "nvflare.app_common.state_persistors.storage_state_persistor.StorageStatePersistor",
          "args": {
              "uri_root": "/",
              "storage": {
                  "path": "nvflare.app_common.storages.filesystem_storage.FilesystemStorage",
                  "args": {
                      "root_dir": "/tmp/nvflare/snapshot-storage",
                      "uri_root": "/"
                  }
              }
          }
      },
      "components": [
          {
              "id": "job_scheduler",
              "path": "nvflare.app_common.job_schedulers.job_scheduler.DefaultJobScheduler",
              "args": {
                  "max_jobs": 4
              }
          },
          {
              "id": "job_manager",
              "path": "nvflare.apis.impl.job_def_manager.SimpleJobDefManager",
              "args": {
                  "uri_root": "/tmp/nvflare/jobs-storage",
                  "job_store_id": "job_store"
              }
          },
          {
              "id": "job_store",
              "path": "nvflare.app_common.storages.filesystem_storage.FilesystemStorage"
          }
      ]
  }

fed_server: |
  {
    "format_version": 2,
    "servers": [
        {
            "name": "spleen_segmentation",
            "service": {
                "target": "localhost:8002",
                "options": [
                    ["grpc.max_send_message_length",    2147483647],
                    ["grpc.max_receive_message_length", 2147483647]
                ]
            },
            "admin_host": "localhost",
            "admin_port": 5005,
            "ssl_private_key": "server.key",
            "ssl_cert": "server.crt",
            "ssl_root_cert": "rootCA.pem"
        }
    ] 
  }

fed_admin: |
  {
    "format_version": 1,
    "admin": {
      "with_file_transfer": true,
      "upload_dir": "transfer",
      "download_dir": "transfer",
      "with_login": true,
      "with_ssl": true,
      "cred_type": "cert",
      "client_key": "client.key",
      "client_cert": "client.crt",
      "ca_cert": "rootCA.pem",
      "prompt": "> "
    }
  }

default_authz: |
  {
    "format_version": "1.0",
    "permissions": {
      "project_admin": "any",
      "org_admin": {
        "submit_job": "none",
        "clone_job": "none",
        "manage_job": "o:submitter",
        "download_job": "o:submitter",
        "view": "any",
        "operate": "o:site",
        "shell_commands": "o:site",
        "byoc": "none"
      },
      "lead": {
        "submit_job": "any",
        "clone_job": "n:submitter",
        "manage_job": "n:submitter",
        "download_job": "n:submitter",
        "view": "any",
        "operate": "o:site",
        "shell_commands": "o:site",
        "byoc": "any"
      },
      "member": {
        "view": "any"
      }
    }
  }

authz_def: |
  {
    "rules": {
      "allow_byoc": {
        "desc": "allow BYOC in APP",
        "type": "bool",
        "default": false
      },
      "allow_custom_datalist": {
        "desc": "allow custom datalist",
        "default": true
      }
    },
    "rights": {
      "train_self": {
        "desc": "do training operations on own site",
        "default": false,
        "precond": "selfOrg"
      },
      "train_all": {
        "desc": "do training ops on all sites",
        "default": false
      },
      "view_self": {
        "desc": "view log files of own org",
        "default": true,
        "precond": "selfOrg"
      },
      "view_all": {
        "desc": "view log files of all sites",
        "default": false
      },
      "operate_all": {
        "desc": "start/stop all sites",
        "default": false
      },
      "operate_self": {
        "desc": "start/stop own site",
        "default": false,
        "precond": "selfOrg"
      }
    }
  }

fl_admin_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  mkdir -p $DIR/../transfer
  python3 -m nvflare.fuel.hci.tools.admin -m $DIR/.. -s fed_admin.json

log_config: |
  [loggers]
  keys=root

  [handlers]
  keys=consoleHandler

  [formatters]
  keys=fullFormatter

  [logger_root]
  level=INFO
  handlers=consoleHandler

  [handler_consoleHandler]
  class=StreamHandler
  level=DEBUG
  formatter=fullFormatter
  args=(sys.stdout,)

  [formatter_fullFormatter]
  format=%(asctime)s - %(name)s - %(levelname)s - %(message)s

start_ovsr_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  NVFL_OVERSEER_HEARTBEAT_TIMEOUT=10 AUTHZ_FILE=$DIR/privilege.yml gunicorn -c $DIR/gunicorn.conf.py --keyfile $DIR/overseer.key --certfile $DIR/overseer.crt --ca-certs $DIR/rootCA.pem

start_cln_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  $DIR/sub_start.sh &

start_svr_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  $DIR/sub_start.sh &

stop_fl_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  echo "Please use FL admin console to issue shutdown client command to properly stop this client."
  echo "This stop_fl.sh script can only be used as the last resort to stop this client."
  echo "It will not properly deregister the client to the server."
  echo "The client status on the server after this shell script will be incorrect."
  read -n1 -p "Would you like to continue (y/N)? " answer
  case $answer in
    y|Y)
      echo
      echo "Shutdown request created.  Wait for local FL process to shutdown."
      touch $DIR/../shutdown.fl
      ;;
    n|N|*)
      echo
      echo "Not continue"
      ;;
  esac

sub_start_cln_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  echo "WORKSPACE set to $DIR/.."
  mkdir -p $DIR/../transfer
  export PYTHONPATH=/local/custom:$PYTHONPATH
  echo "PYTHONPATH is $PYTHONPATH"

  SECONDS=0
  lst=-400
  restart_count=0
  start_fl() {
    if [[ $(( $SECONDS - $lst )) -lt 300 ]]; then
      ((restart_count++))
    else
      restart_count=0
    fi
    if [[ $(($SECONDS - $lst )) -lt 300 && $restart_count -ge 5 ]]; then
      echo "System is in trouble and unable to start the task!!!!!"
      rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl $DIR/../daemon_pid.fl
      exit
    fi
    lst=$SECONDS
  ((python3 -u -m nvflare.private.fed.app.client.client_train -m $DIR/.. -s fed_client.json --set secure_train=true uid={~~client_name~~} org={~~org_name~~} config_folder={~~config_folder~~} 2>&1 & echo $! >&3 ) 3>$DIR/../pid.fl )
    pid=`cat $DIR/../pid.fl`
    echo "new pid ${pid}"
  }

  stop_fl() {
    if [[ ! -f "$DIR/../pid.fl" ]]; then
      echo "No pid.fl.  No need to kill process."
      return
    fi
    pid=`cat $DIR/../pid.fl`
    sleep 5
    kill -0 ${pid} 2> /dev/null 1>&2
    if [[ $? -ne 0 ]]; then
      echo "Process already terminated"
      return
    fi
    kill -9 $pid
    rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl 2> /dev/null 1>&2
  }
    
  if [[ -f "$DIR/../daemon_pid.fl" ]]; then
    dpid=`cat $DIR/../daemon_pid.fl`
    kill -0 ${dpid} 2> /dev/null 1>&2
    if [[ $? -eq 0 ]]; then
      echo "There seems to be one instance, pid=$dpid, running."
      echo "If you are sure it's not the case, please kill process $dpid and then remove daemon_pid.fl in $DIR/.."
      exit
    fi
    rm -f $DIR/../daemon_pid.fl
  fi

  echo $BASHPID > $DIR/../daemon_pid.fl

  while true
  do
    sleep 5
    if [[ ! -f "$DIR/../pid.fl" ]]; then
      echo "start fl because of no pid.fl"
      start_fl
      continue
    fi
    pid=`cat $DIR/../pid.fl`
    kill -0 ${pid} 2> /dev/null 1>&2
    if [[ $? -ne 0 ]]; then
      if [[ -f "$DIR/../shutdown.fl" ]]; then
        echo "Gracefully shutdown."
        break
      fi
      echo "start fl because process of ${pid} does not exist"
      start_fl
      continue
    fi
    if [[ -f "$DIR/../shutdown.fl" ]]; then
      echo "About to shutdown."
      stop_fl
      break
    fi
    if [[ -f "$DIR/../restart.fl" ]]; then
      echo "About to restart."
      stop_fl
    fi
  done

  rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl $DIR/../daemon_pid.fl

sub_start_svr_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  echo "WORKSPACE set to $DIR/.."
  mkdir -p $DIR/../transfer 

  SECONDS=0
  lst=-400
  restart_count=0
  start_fl() {
    if [[ $(( $SECONDS - $lst )) -lt 300 ]]; then
      ((restart_count++))
    else
      restart_count=0
    fi
    if [[ $(($SECONDS - $lst )) -lt 300 && $restart_count -ge 5 ]]; then
      echo "System is in trouble and unable to start the task!!!!!"
      rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl $DIR/../daemon_pid.fl
      exit
    fi
    lst=$SECONDS
  ((python3 -u -m nvflare.private.fed.app.server.server_train -m $DIR/.. -s fed_server.json --set secure_train=true org={~~org_name~~} config_folder={~~config_folder~~} 2>&1 & echo $! >&3 ) 3>$DIR/../pid.fl )
    pid=`cat $DIR/../pid.fl`
    echo "new pid ${pid}"
  }

  stop_fl() {
    if [[ ! -f "$DIR/../pid.fl" ]]; then
      echo "No pid.fl.  No need to kill process."
      return
    fi
    pid=`cat $DIR/../pid.fl`
    sleep 5
    kill -0 ${pid} 2> /dev/null 1>&2
    if [[ $? -ne 0 ]]; then
      echo "Process already terminated"
      return
    fi
    kill -9 $pid
    rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl
  }
    
  if [[ -f "$DIR/../daemon_pid.fl" ]]; then
    dpid=`cat $DIR/../daemon_pid.fl`
    kill -0 ${dpid} 2> /dev/null 1>&2
    if [[ $? -eq 0 ]]; then
      echo "There seems to be one instance, pid=$dpid, running."
      echo "If you are sure it's not the case, please kill process $dpid and then remove daemon_pid.fl in $DIR/.."
      exit
    fi
    rm -f $DIR/../daemon_pid.fl
  fi

  echo $BASHPID > $DIR/../daemon_pid.fl

  while true
  do
    sleep 5
    if [[ ! -f "$DIR/../pid.fl" ]]; then
      echo "start fl because of no pid.fl"
      start_fl
      continue
    fi
    pid=`cat $DIR/../pid.fl`
    kill -0 ${pid} 2> /dev/null 1>&2
    if [[ $? -ne 0 ]]; then
      if [[ -f "$DIR/../shutdown.fl" ]]; then
        echo "Gracefully shutdown."
        break
      fi
      echo "start fl because process of ${pid} does not exist"
      start_fl
      continue
    fi
    if [[ -f "$DIR/../shutdown.fl" ]]; then
      echo "About to shutdown."
      stop_fl
      break
    fi
    if [[ -f "$DIR/../restart.fl" ]]; then
      echo "About to restart."
      stop_fl
    fi
  done

  rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl $DIR/../daemon_pid.fl

docker_cln_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  # docker run script for FL client
  # local data directory
  MY_DATA_DIR=/home/flclient/data
  # for all gpus use line below 
  GPU2USE=all 
  # for 2 gpus use line below
  #GPU2USE=2 
  # for specific gpus as gpu#0 and gpu#2 use line below
  #GPU2USE='"device=0,2"'
  # to use host network, use line below
  NETARG="--net=host"
  # FL clients do not need to open ports, so the following line is not needed.
  #NETARG="-p 443:443 -p 8003:8003"
  DOCKER_IMAGE={~~docker_image~~}
  echo "Starting docker with $DOCKER_IMAGE"
  docker run --rm -it --name={~~client_name~~} --gpus=$GPU2USE -u $(id -u):$(id -g) -v /etc/passwd:/etc/passwd -v /etc/group:/etc/group -v $DIR/..:/workspace/ -v $MY_DATA_DIR:/data/:ro -w /workspace/ --ipc=host $NETARG $DOCKER_IMAGE /bin/bash

docker_svr_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  # docker run script for FL server
  # to use host network, use line below
  NETARG="--net=host"
  # or to expose specific ports, use line below
  #NETARG="-p {~~admin_port~~}:{~~admin_port~~} -p {~~fed_learn_port~~}:{~~fed_learn_port~~}"
  DOCKER_IMAGE={~~docker_image~~}
  echo "Starting docker with $DOCKER_IMAGE"
  docker run --rm -it --name=flserver -v $DIR/..:/workspace/ -w /workspace/ --ipc=host $NETARG $DOCKER_IMAGE /bin/bash

docker_adm_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  # docker run script for FL admin
  # to use host network, use line below
  #NETARG="--net=host"
  # Admin clients do not need to open ports, so the following line is not needed.
  #NETARG="-p 8003:8003"
  DOCKER_IMAGE={~~docker_image~~}
  echo "Starting docker with $DOCKER_IMAGE"
  docker run --rm -it --name=fladmin -v $DIR/..:/workspace/ -w /workspace/ $DOCKER_IMAGE /bin/bash

compose_yaml: |
  services:
    __overseer__:
      build: ./nvflare
      image: nvflare-service
      volumes:
        - .:/workspace
      command: ["/workspace/startup/start.sh"]
      ports:
        - "8443:8443"

    __flserver__:
      image: nvflare-service
      ports:
        - "8002:8002"
        - "8003:8003"
      volumes:
        - .:/workspace
        - nvflare_svc_persist:/tmp/nvflare/
      command: ["/usr/local/bin/python3",
            "-u",
            "-m",
            "nvflare.private.fed.app.server.server_train",
            "-m",
            "/workspace",
            "-s",
            "fed_server.json",
            "--set",
            "secure_train=true",
            "config_folder=config",
            "org=__org_name__",
          ]

    __flclient__:
      image: nvflare-service
      volumes:
        - .:/workspace
      command: ["/usr/local/bin/python3",
            "-u",
            "-m",
            "nvflare.private.fed.app.client.client_train",
            "-m",
            "/workspace",
            "-s",
            "fed_client.json",
            "--set",
            "secure_train=true",
            "uid=__flclient__",
            "org=__org_name__",
            "config_folder=config",
          ]

  volumes:
    nvflare_svc_persist:

dockerfile: |
  RUN pip install -U pip
  RUN pip install nvflare
  COPY requirements.txt requirements.txt
  RUN pip install -r requirements.txt

helm_chart_chart: |
  apiVersion: v2
  name: nvflare
  description: A Helm chart for NVFlare overseer and servers
  type: application
  version: 0.1.0
  appVersion: "2.2.0"

helm_chart_service_overseer: |
  apiVersion: v1
  kind: Service
  metadata:
    name: overseer
  spec:
    selector:
      system: overseer
    ports:
      - protocol: TCP
        port: 8443
        targetPort: overseer-port

helm_chart_service_server: |
  apiVersion: v1
  kind: Service
  metadata:
    name: server
    labels:
      system: server
  spec:
    selector:
      system: server
    ports:
      - name: fl-port
        protocol: TCP
        port: 8002
        targetPort: fl-port
      - name: admin-port
        protocol: TCP
        port: 8003
        targetPort: admin-port

helm_chart_deployment_overseer: |
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: overseer
    labels:
      system: overseer
  spec:
    replicas: 1
    selector:
      matchLabels:
        system: overseer
    template:
      metadata:
        labels:
          system: overseer
      spec:
        volumes:
          - name: workspace
            hostPath:
              path:
              type: Directory
        containers:
          - name: overseer
            image: nvflare-min:2.2.0
            imagePullPolicy: IfNotPresent
            volumeMounts:
              - name: workspace
                mountPath: /workspace
            command: ["/workspace/overseer/startup/start.sh"]
            ports:
              - name: overseer-port
                containerPort: 8443
                protocol: TCP
helm_chart_deployment_server: |
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: server
    labels:
      system: server
  spec:
    replicas: 1
    selector:
      matchLabels:
        system: server
    template:
      metadata:
        labels:
          system: server
      spec:
        volumes:
          - name: workspace
            hostPath:
              path:
              type: Directory
          - name: persist
            hostPath:
              path: /tmp/nvflare
              type: Directory
        containers:
          - name: server1
            image: nvflare-min:2.2.0
            imagePullPolicy: IfNotPresent
            volumeMounts:
              - name: workspace
                mountPath: /workspace
              - name: persist
                mountPath: /tmp/nvflare
            command: ["/usr/local/bin/python3"]
            args:
              [
                "-u",
                "-m",
                "nvflare.private.fed.app.server.server_train",
                "-m",
                "/workspace/server",
                "-s",
                "fed_server.json",
                "--set",
                "secure_train=true",
                "config_folder=config",
                "org=__org_name__",

              ]
            ports:
              - containerPort: 8002
                protocol: TCP
              - containerPort: 8003
                protocol: TCP
helm_chart_values: |
  workspace: /home/nvflare
  persist: /home/nvflare


