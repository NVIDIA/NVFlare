readme_am: |
  *********************************
  Admin Client package
  *********************************
  The package includes at least the following files:
  readme.txt
  rootCA.pem
  client.crt
  client.key
  fl_admin.sh
  
  Please install the nvflare package by 'python3 -m pip nvflare.'  This will install a set of Python codes
  in your environment.  After installation, you can run the fl_admin.sh file to start communicating to the admin server.

  The rootCA.pem file is pointed by "ca_cert" in fl_admin.sh.  If you plan to move/copy it to a different place,
  you will need to modify fl_admin.sh.  The same applies to the other two files, client.crt and client.key.

  The email in your submission to participate this Federated Learning project is embedded in the CN field of client
  certificate, which uniquely identifies the participant.  As such, please safeguard its private key, client.key.

readme_fc: |
  *********************************
  Federated Learning Client package
  *********************************
  The package includes at least the following files:
  readme.txt
  rootCA.pem
  client.crt
  client.key
  fed_client.json
  start.sh
  sub_start.sh
  stop_fl.sh

  Run start.sh to start the client.

  The rootCA.pem file is pointed by "ssl_root_cert" in fed_client.json.  If you plan to move/copy it to a different place,
  you will need to modify fed_client.json.  The same applies to the other two files, client.crt and client.key.

  The client name in your submission to participate this Federated Learning project is embedded in the CN field of client
  certificate, which uniquely identifies the participant.  As such, please safeguard its private key, client.key.

readme_fs: |
  *********************************
  Federated Learning Server package
  *********************************
  The package includes at least the following files:
  readme.txt
  rootCA.pem
  server.crt
  server.key
  authorization.json
  fed_server.json
  start.sh
  sub_start.sh
  stop_fl.sh
  signature.json

  Run start.sh to start the server.

  The rootCA.pem file is pointed by "ssl_root_cert" in fed_server.json.  If you plan to move/copy it to a different place,
  you will need to modify fed_server.json.  The same applies to the other two files, server.crt and server.key.

  Please always safeguard the server.key.

gunicorn_conf_py: |
  bind="0.0.0.0:{~~port~~}"
  cert_reqs=2
  do_handshake_on_connect=True
  timeout=30
  worker_class="nvflare.ha.overseer.worker.ClientAuthWorker"
  workers=1
  wsgi_app="nvflare.ha.overseer.overseer:app"

local_client_resources: |
  {
    "format_version": 2,
    "client": {
      "retry_timeout": 30,
      "compression": "Gzip"
    },
    "components": [
      {
        "id": "resource_manager",
        "path": "nvflare.app_common.resource_managers.gpu_resource_manager.GPUResourceManager",
        "args": {
          "num_of_gpus": 0,
          "mem_per_gpu_in_GiB": 0
        }
      },
      {
        "id": "resource_consumer",
        "path": "nvflare.app_common.resource_consumers.gpu_resource_consumer.GPUResourceConsumer",
        "args": {}
      }
    ]
  }

fed_client: |
  {
    "format_version": 2,
    "servers": [
      {
        "name": "spleen_segmentation",
        "service": {
        }
      }
    ],
    "client": {
      "ssl_private_key": "client.key",
      "ssl_cert": "client.crt",
      "ssl_root_cert": "rootCA.pem"
    }
  }

sample_privacy: |
  {
    "scopes": [
      {
        "name": "public",
        "properties": {
          "train_dataset": "/data/public/train",
          "val_dataset": "/data/public/val"
        },
        "task_result_filters": [
          {
            "name": "AddNoiseToMinMax",
            "args": {
              "min_noise_level": 0.2,
              "max_noise_level": 0.2
            }
          },
          {
            "name": "PercentilePrivacy",
            "args": {
              "percentile": 10,
              "gamma": 0.02
            }
          }
        ],
        "task_data_filters": [
          {
            "name": "BadModelDetector"
          }
        ]
      },
      {
        "name": "private",
        "properties": {
          "train_dataset": "/data/private/train",
          "val_dataset": "/data/private/val"
        },
        "task_result_filters": [
          {
            "name": "AddNoiseToMinMax",
            "args": {
              "min_noise_level": 0.1,
              "max_noise_level": 0.1
            }
          },
          {
            "name": "SVTPrivacy",
            "args": {
              "fraction": 0.1,
              "epsilon": 0.2
            }
          }
        ]
      }
    ],
    "default_scope": "public"
  }

local_server_resources: |
  {
      "format_version": 2,
      "servers": [
          {
              "admin_storage": "transfer",
              "max_num_clients": 100,
              "heart_beat_timeout": 600,
              "num_server_workers": 4,
              "download_job_url": "http://download.server.com/",
              "compression": "Gzip"
          }
      ],
      "snapshot_persistor": {
          "path": "nvflare.app_common.state_persistors.storage_state_persistor.StorageStatePersistor",
          "args": {
              "uri_root": "/",
              "storage": {
                  "path": "nvflare.app_common.storages.filesystem_storage.FilesystemStorage",
                  "args": {
                      "root_dir": "/tmp/nvflare/snapshot-storage",
                      "uri_root": "/"
                  }
              }
          }
      },
      "components": [
          {
              "id": "job_scheduler",
              "path": "nvflare.app_common.job_schedulers.job_scheduler.DefaultJobScheduler",
              "args": {
                  "max_jobs": 4
              }
          },
          {
              "id": "job_manager",
              "path": "nvflare.apis.impl.job_def_manager.SimpleJobDefManager",
              "args": {
                  "uri_root": "/tmp/nvflare/jobs-storage",
                  "job_store_id": "job_store"
              }
          },
          {
              "id": "job_store",
              "path": "nvflare.app_common.storages.filesystem_storage.FilesystemStorage"
          }
      ]
  }

fed_server: |
  {
    "format_version": 2,
    "servers": [
        {
            "name": "spleen_segmentation",
            "service": {
                "target": "localhost:8002"
            },
            "admin_host": "localhost",
            "admin_port": 5005,
            "ssl_private_key": "server.key",
            "ssl_cert": "server.crt",
            "ssl_root_cert": "rootCA.pem"
        }
    ] 
  }

fed_admin: |
  {
    "format_version": 1,
    "admin": {
      "with_file_transfer": true,
      "upload_dir": "transfer",
      "download_dir": "transfer",
      "with_login": true,
      "with_ssl": true,
      "cred_type": "cert",
      "client_key": "client.key",
      "client_cert": "client.crt",
      "ca_cert": "rootCA.pem",
      "prompt": "> "
    }
  }

default_authz: |
  {
    "format_version": "1.0",
    "permissions": {
      "project_admin": "any",
      "org_admin": {
        "submit_job": "none",
        "clone_job": "none",
        "manage_job": "o:submitter",
        "download_job": "o:submitter",
        "view": "any",
        "operate": "o:site",
        "shell_commands": "o:site",
        "byoc": "none"
      },
      "lead": {
        "submit_job": "any",
        "clone_job": "n:submitter",
        "manage_job": "n:submitter",
        "download_job": "n:submitter",
        "view": "any",
        "operate": "o:site",
        "shell_commands": "o:site",
        "byoc": "any"
      },
      "member": {
        "view": "any"
      }
    }
  }

fl_admin_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  mkdir -p $DIR/../transfer
  python3 -m nvflare.fuel.hci.tools.admin -m $DIR/.. -s fed_admin.json

log_config: |
  [loggers]
  keys=root

  [handlers]
  keys=consoleHandler

  [formatters]
  keys=fullFormatter

  [logger_root]
  level=INFO
  handlers=consoleHandler

  [handler_consoleHandler]
  class=StreamHandler
  level=DEBUG
  formatter=fullFormatter
  args=(sys.stdout,)

  [formatter_fullFormatter]
  format=%(asctime)s - %(name)s - %(levelname)s - %(message)s

start_ovsr_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  NVFL_OVERSEER_HEARTBEAT_TIMEOUT=10 AUTHZ_FILE=$DIR/privilege.yml gunicorn -c $DIR/gunicorn.conf.py --keyfile $DIR/overseer.key --certfile $DIR/overseer.crt --ca-certs $DIR/rootCA.pem

start_cln_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  all_arguments="${@}"
  doCloud=false
  # parse arguments
  while [[ $# -gt 0 ]]
  do
      key="$1"
      case $key in
        --cloud)
          doCloud=true
          csp=$2
          shift
        ;;
      esac
      shift
  done

  if [ $doCloud == true ]
  then
    case $csp in
      azure)
        $DIR/azure_start.sh ${all_arguments}
        ;;
      aws)
        $DIR/aws_start.sh ${all_arguments}
        ;;
      *)
        echo "Only on-prem or azure or aws is currently supported."
    esac
  else
    $DIR/sub_start.sh &
  fi

start_svr_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  all_arguments="${@}"
  doCloud=false
  ha_mode={~~ha_mode~~}
  # parse arguments
  while [[ $# -gt 0 ]]
  do
    key="$1"
    case $key in
      --cloud)
        if [ $ha_mode == false ]
        then
          doCloud=true
          csp=$2
          shift
        else
          echo "Cloud launch does not support NVFlare HA mode."
          exit 1
        fi
      ;;
    esac
    shift
  done

  if [ $doCloud == true ]
  then
    case $csp in
      azure)
        $DIR/azure_start.sh ${all_arguments}
        ;;
      aws)
        $DIR/aws_start.sh ${all_arguments}
        ;;
      *)
        echo "Only on-prem or azure or aws is currently supported."
    esac
  else
    $DIR/sub_start.sh &
  fi
    
stop_fl_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  echo "Please use FL admin console to issue shutdown client command to properly stop this client."
  echo "This stop_fl.sh script can only be used as the last resort to stop this client."
  echo "It will not properly deregister the client to the server."
  echo "The client status on the server after this shell script will be incorrect."
  read -n1 -p "Would you like to continue (y/N)? " answer
  case $answer in
    y|Y)
      echo
      echo "Shutdown request created.  Wait for local FL process to shutdown."
      touch $DIR/../shutdown.fl
      ;;
    n|N|*)
      echo
      echo "Not continue"
      ;;
  esac

sub_start_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  echo "WORKSPACE set to $DIR/.."
  mkdir -p $DIR/../transfer
  export PYTHONPATH=/local/custom:$PYTHONPATH
  echo "PYTHONPATH is $PYTHONPATH"

  SECONDS=0
  lst=-400
  restart_count=0
  start_fl() {
    if [[ $(( $SECONDS - $lst )) -lt 300 ]]; then
      ((restart_count++))
    else
      restart_count=0
    fi
    if [[ $(($SECONDS - $lst )) -lt 300 && $restart_count -ge 5 ]]; then
      echo "System is in trouble and unable to start the task!!!!!"
      rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl $DIR/../daemon_pid.fl
      exit
    fi
    lst=$SECONDS
  ((python3 -u -m nvflare.private.fed.app.{~~type~~}.{~~type~~}_train -m $DIR/.. -s fed_{~~type~~}.json --set secure_train=true {~~cln_uid~~} org={~~org_name~~} config_folder={~~config_folder~~} 2>&1 & echo $! >&3 ) 3>$DIR/../pid.fl )
    pid=`cat $DIR/../pid.fl`
    echo "new pid ${pid}"
  }

  stop_fl() {
    if [[ ! -f "$DIR/../pid.fl" ]]; then
      echo "No pid.fl.  No need to kill process."
      return
    fi
    pid=`cat $DIR/../pid.fl`
    sleep 5
    kill -0 ${pid} 2> /dev/null 1>&2
    if [[ $? -ne 0 ]]; then
      echo "Process already terminated"
      return
    fi
    kill -9 $pid
    rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl 2> /dev/null 1>&2
  }
    
  if [[ -f "$DIR/../daemon_pid.fl" ]]; then
    dpid=`cat $DIR/../daemon_pid.fl`
    kill -0 ${dpid} 2> /dev/null 1>&2
    if [[ $? -eq 0 ]]; then
      echo "There seems to be one instance, pid=$dpid, running."
      echo "If you are sure it's not the case, please kill process $dpid and then remove daemon_pid.fl in $DIR/.."
      exit
    fi
    rm -f $DIR/../daemon_pid.fl
  fi

  echo $BASHPID > $DIR/../daemon_pid.fl

  while true
  do
    sleep 5
    if [[ ! -f "$DIR/../pid.fl" ]]; then
      echo "start fl because of no pid.fl"
      start_fl
      continue
    fi
    pid=`cat $DIR/../pid.fl`
    kill -0 ${pid} 2> /dev/null 1>&2
    if [[ $? -ne 0 ]]; then
      if [[ -f "$DIR/../shutdown.fl" ]]; then
        echo "Gracefully shutdown."
        break
      fi
      echo "start fl because process of ${pid} does not exist"
      start_fl
      continue
    fi
    if [[ -f "$DIR/../shutdown.fl" ]]; then
      echo "About to shutdown."
      stop_fl
      break
    fi
    if [[ -f "$DIR/../restart.fl" ]]; then
      echo "About to restart."
      stop_fl
    fi
  done

  rm -f $DIR/../pid.fl $DIR/../shutdown.fl $DIR/../restart.fl $DIR/../daemon_pid.fl

docker_cln_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  # docker run script for FL client
  # local data directory
  : ${MY_DATA_DIR:="/home/flclient/data"}
  # The syntax above is to set MY_DATA_DIR to /home/flcient/data if this
  # environment variable is not set previously.
  # Therefore, users can set their own MY_DATA_DIR with
  # export MY_DATA_DIR=$SOME_DIRECTORY
  # before running docker.sh

  # for all gpus use line below 
  #GPU2USE='--gpus=all'
  # for 2 gpus use line below
  #GPU2USE='--gpus=2' 
  # for specific gpus as gpu#0 and gpu#2 use line below
  #GPU2USE='--gpus="device=0,2"'
  # to use host network, use line below
  NETARG="--net=host"
  # FL clients do not need to open ports, so the following line is not needed.
  #NETARG="-p 443:443 -p 8003:8003"
  DOCKER_IMAGE={~~docker_image~~}
  echo "Starting docker with $DOCKER_IMAGE"
  mode="${1:--r}"
  if [ $mode = "-d" ]
  then
    docker run -d --rm --name={~~client_name~~} $GPU2USE -u $(id -u):$(id -g) \
    -v /etc/passwd:/etc/passwd -v /etc/group:/etc/group -v $DIR/..:/workspace/ \
    -v $MY_DATA_DIR:/data/:ro -w /workspace/ --ipc=host $NETARG $DOCKER_IMAGE \
    /bin/bash -c "python -u -m nvflare.private.fed.app.client.client_train -m /workspace -s fed_client.json --set uid={~~client_name~~} secure_train=true config_folder=config org={~~org_name~~}"
  else
    docker run --rm -it --name={~~client_name~~} $GPU2USE -u $(id -u):$(id -g) \
    -v /etc/passwd:/etc/passwd -v /etc/group:/etc/group -v $DIR/..:/workspace/ \
    -v $MY_DATA_DIR:/data/:ro -w /workspace/ --ipc=host $NETARG $DOCKER_IMAGE /bin/bash
  fi

docker_svr_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  # docker run script for FL server
  # to use host network, use line below
  NETARG="--net=host"
  # or to expose specific ports, use line below
  #NETARG="-p {~~admin_port~~}:{~~admin_port~~} -p {~~fed_learn_port~~}:{~~fed_learn_port~~}"
  DOCKER_IMAGE={~~docker_image~~}
  echo "Starting docker with $DOCKER_IMAGE"
  svr_name="${SVR_NAME:-flserver}"
  mode="${1:-r}"
  if [ $mode = "-d" ]
  then
    docker run -d --rm --name=$svr_name -v $DIR/..:/workspace/ -w /workspace \
    --ipc=host $NETARG $DOCKER_IMAGE /bin/bash -c \
    "python -u -m nvflare.private.fed.app.server.server_train -m /workspace -s fed_server.json --set secure_train=true config_folder=config org={~~org_name~~}"
  else
    docker run --rm -it --name=$svr_name -v $DIR/..:/workspace/ -w /workspace/ --ipc=host $NETARG $DOCKER_IMAGE /bin/bash
  fi

docker_adm_sh: |
  #!/usr/bin/env bash
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  # docker run script for FL admin
  # to use host network, use line below
  #NETARG="--net=host"
  # Admin clients do not need to open ports, so the following line is not needed.
  #NETARG="-p 8003:8003"
  DOCKER_IMAGE={~~docker_image~~}
  echo "Starting docker with $DOCKER_IMAGE"
  docker run --rm -it --name=fladmin -v $DIR/..:/workspace/ -w /workspace/ $DOCKER_IMAGE /bin/bash

compose_yaml: |
  services:
    __overseer__:
      build: ./nvflare
      image: ${IMAGE_NAME}
      volumes:
        - .:/workspace
      command: ["${WORKSPACE}/startup/start.sh"]
      ports:
        - "8443:8443"

    __flserver__:
      image: ${IMAGE_NAME}
      ports:
        - "8002:8002"
        - "8003:8003"
      volumes:
        - .:/workspace
        - nvflare_svc_persist:/tmp/nvflare/
      command: ["${PYTHON_EXECUTABLE}",
            "-u",
            "-m",
            "nvflare.private.fed.app.server.server_train",
            "-m",
            "${WORKSPACE}",
            "-s",
            "fed_server.json",
            "--set",
            "secure_train=true",
            "config_folder=config",
            "org=__org_name__",
          ]

    __flclient__:
      image: ${IMAGE_NAME}
      volumes:
        - .:/workspace
      command: ["${PYTHON_EXECUTABLE}",
            "-u",
            "-m",
            "nvflare.private.fed.app.client.client_train",
            "-m",
            "${WORKSPACE}",
            "-s",
            "fed_client.json",
            "--set",
            "secure_train=true",
            "uid=__flclient__",
            "org=__org_name__",
            "config_folder=config",
          ]

  volumes:
    nvflare_svc_persist:

dockerfile: |
  RUN pip install -U pip
  RUN pip install nvflare
  COPY requirements.txt requirements.txt
  RUN pip install -r requirements.txt

helm_chart_chart: |
  apiVersion: v2
  name: nvflare
  description: A Helm chart for NVFlare overseer and servers
  type: application
  version: 0.1.0
  appVersion: "2.2.0"

helm_chart_service_overseer: |
  apiVersion: v1
  kind: Service
  metadata:
    name: overseer
  spec:
    selector:
      system: overseer
    ports:
      - protocol: TCP
        port: 8443
        targetPort: overseer-port

helm_chart_service_server: |
  apiVersion: v1
  kind: Service
  metadata:
    name: server
    labels:
      system: server
  spec:
    selector:
      system: server
    ports:
      - name: fl-port
        protocol: TCP
        port: 8002
        targetPort: fl-port
      - name: admin-port
        protocol: TCP
        port: 8003
        targetPort: admin-port

helm_chart_deployment_overseer: |
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: overseer
    labels:
      system: overseer
  spec:
    replicas: 1
    selector:
      matchLabels:
        system: overseer
    template:
      metadata:
        labels:
          system: overseer
      spec:
        volumes:
          - name: workspace
            hostPath:
              path:
              type: Directory
        containers:
          - name: overseer
            image: nvflare-min:2.2.0
            imagePullPolicy: IfNotPresent
            volumeMounts:
              - name: workspace
                mountPath: /workspace
            command: ["/workspace/overseer/startup/start.sh"]
            ports:
              - name: overseer-port
                containerPort: 8443
                protocol: TCP
helm_chart_deployment_server: |
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: server
    labels:
      system: server
  spec:
    replicas: 1
    selector:
      matchLabels:
        system: server
    template:
      metadata:
        labels:
          system: server
      spec:
        volumes:
          - name: workspace
            hostPath:
              path:
              type: Directory
          - name: persist
            hostPath:
              path: /tmp/nvflare
              type: Directory
        containers:
          - name: server1
            image: nvflare-min:2.2.0
            imagePullPolicy: IfNotPresent
            volumeMounts:
              - name: workspace
                mountPath: /workspace
              - name: persist
                mountPath: /tmp/nvflare
            command: ["/usr/local/bin/python3"]
            args:
              [
                "-u",
                "-m",
                "nvflare.private.fed.app.server.server_train",
                "-m",
                "/workspace/server",
                "-s",
                "fed_server.json",
                "--set",
                "secure_train=true",
                "config_folder=config",
                "org=__org_name__",

              ]
            ports:
              - containerPort: 8002
                protocol: TCP
              - containerPort: 8003
                protocol: TCP
helm_chart_values: |
  workspace: /home/nvflare
  persist: /home/nvflare


cloud_script_header: |
  #!/usr/bin/env bash
  
  DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
  function report_status() {
    status="$1"
    if [ "${status}" -ne 0 ]
    then
      echo "$2 failed"
      exit "${status}"
    fi
  }

  function check_binary() {
    echo -n "Checking if $1 exists. => "
    if ! command -v $1 &> /dev/null
    then
      echo "not found. $2"
      exit 1
    else
      echo "found"
    fi
  }

  function prompt() {
    # usage:  prompt NEW_VAR "Prompt message" ["${PROMPT_VALUE}"]
    local __resultvar=$1
    local __prompt=$2
    local __default=${3:-}
    local __result
    if [[ ${BASH_VERSINFO[0]} -ge 4 && -n "$__default" ]]
      then
      read -e -i "$__default" -p "$__prompt: " __result
    else
      __default=${3:-${!__resultvar:-}}
      if [[ -n $__default ]]
        then
        printf "%s [%s]: " "$__prompt" "$__default"
      else
        printf "%s: " "$__prompt"
      fi
      IFS= read -r __result
      if [[ -z "$__result" && -n "$__default" ]]
        then
        __result="$__default"
      fi
    fi
    eval $__resultvar="'$__result'"
  }

  function get_resources_file() {
    local rfile="${DIR}/../local/resources.json"
    if [ -f "${rfile}" ]
      then
      echo "${rfile}"
    elif [ -f "${rfile}.default" ]
      then
      echo "${rfile}.default"
    else
      echo ""
      exit 1
    fi
  }

  # parse arguments
  while [[ $# -gt 0 ]]
  do
    key="$1"
    case $key in
      --config)
        config_file=$2
        shift
      ;;
      --image)
        image_name=$2
        shift
      ;;
      --vpc-id)
        vpc_id=$2
        shift
      ;;
      --subnet-id)
        subnet_id=$2
        shift
      ;;
    esac
    shift
  done

adm_notebook: |
  {
  "cells": [
    {
    "cell_type": "markdown",
    "id": "b758695b",
    "metadata": {},
    "source": [
      "# System Info"
    ]
    },
    {
    "cell_type": "markdown",
    "id": "9f7cd9e6",
    "metadata": {},
    "source": [
      "In this notebook, System Info is checked with the FLARE API."
    ]
    },
    {
    "cell_type": "markdown",
    "id": "ea50ba28",
    "metadata": {},
    "source": [
      "#### 1. Connect to the FL System with the FLARE API\n",
      "\n",
      "Use `new_secure_session()` to initiate a session connecting to the FL Server with the FLARE API. The necessary arguments are the username of the admin user you are using and the corresponding startup kit location.\n",
      "\n",
      "In the code example below, we get the `admin_user_dir` by concatenating the workspace root with the default directories that are created if you provision a project with a given project name. You can change the values to what applies to your system if needed.\n",
      "\n",
      "Note that if debug mode is not enabled, there is no output after initiating a session successfully, so instead we print the output of `get_system_info()`. If you are unable to connect and initiate a session, make sure that your FL Server is running and that the configurations are correct with the right path to the admin startup kit directory."
    ]
    },
    {
    "cell_type": "code",
    "execution_count": null,
    "id": "0166942d",
    "metadata": {
      "collapsed": true
    },
    "outputs": [],
    "source": [
      "# Run this pip install if NVFlare is not installed in your Jupyter Notebook\n",
      "\n",
      "# !python3 -m pip install -U nvflare"
    ]
    },
    {
    "cell_type": "code",
    "execution_count": null,
    "id": "c3dbde69",
    "metadata": {},
    "outputs": [],
    "source": [
      "import os\n",
      "from nvflare.fuel.flare_api.flare_api import new_secure_session\n",
      "\n",
      "username = \"{~~admin_name~~}\"  # change this to your own username\n",
      "\n",
      "sess = new_secure_session(\n",
      "    username=username,\n",
      "    startup_kit_location=os.getcwd()\n",
      ")\n",
      "print(sess.get_system_info())"
    ]
    },
    {
    "cell_type": "markdown",
    "id": "31ccb6a6",
    "metadata": {},
    "source": [
      "### 2. Shutting Down the FL System\n",
      "\n",
      "As of now, there is no specific FLARE API command for shutting down the FL system, but the FLARE API can use the `do_command()` function of the underlying AdminAPI to submit any commands that the FLARE Console supports including shutdown commands to the clients and server:"
    ]
    },
    {
    "cell_type": "code",
    "execution_count": null,
    "id": "b0d8aa9c",
    "metadata": {},
    "outputs": [],
    "source": [
      "print(sess.api.do_command(\"shutdown client\"))\n",
      "print(sess.api.do_command(\"shutdown server\"))\n",
      "\n",
      "sess.close()"
    ]
    }
  ],
  "metadata": {
    "kernelspec": {
    "display_name": "Python 3 (ipykernel)",
    "language": "python",
    "name": "python3"
    },
    "language_info": {
    "codemirror_mode": {
      "name": "ipython",
      "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.13"
    },
    "vscode": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
  }

