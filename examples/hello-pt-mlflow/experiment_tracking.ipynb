{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e129ede5",
   "metadata": {},
   "source": [
    "   # Hello PyTorch with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf7e391",
   "metadata": {},
   "source": [
    "In this example, we like to demonstrate that the example code used in hello-pt with MLFlow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec76f4",
   "metadata": {},
   "source": [
    "\n",
    "Example of using [NVIDIA FLARE](https://nvflare.readthedocs.io/en/main/index.html) to train an image classifier using federated averaging ([FedAvg]([FedAvg](https://arxiv.org/abs/1602.05629))) and [PyTorch](https://pytorch.org/) as the deep learning training framework. \n",
    "\n",
    "This example also highlights the streaming capability from the clients to the server with MLFLow \n",
    "\n",
    "> **_NOTE:_** This example uses the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset and will load its data within the trainer code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04895127",
   "metadata": {},
   "source": [
    "### 1. Examaples overview \n",
    "\n",
    "We have two examples under hello-pt-mlflow\n",
    "\n",
    "* hello-pt-mlflow-job \n",
    "  This example demonstrate the user can use MLFLow API syntax to log_metric(s), log_parameter(s), log_text and set_tag(s)\n",
    "  The experiment trackinig logs will be streamed Federated Server and handled by MLFlowReceiver, which then delivered to\n",
    "  MLFlow tracking Server with MLflow tracking URL. \n",
    "    \n",
    "  Even with the same sytax, if user prefer to display via Tensorboard, the example, also demonstrated that the same tracking log\n",
    "  can be displayed to Tensorboard without code changes\n",
    "    \n",
    "\n",
    "* hello-pt-tb-mlflow-job \n",
    "\n",
    "  In this example, user uses Tensorboard SummaryWriter API sytax: add_scalar, add_scalars. the same log event, can be displayed\n",
    "  in MLFlow without user change the code. No MLFlow trackinig server.     \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca0050",
   "metadata": {},
   "source": [
    "### 2. Install NVIDIA FLARE\n",
    "\n",
    "Follow the [Installation](https://nvflare.readthedocs.io/en/main/getting_started.html#installation) instructions.\n",
    "Install additional requirements:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install torch torchvision tensorboard mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa6d7c",
   "metadata": {},
   "source": [
    "### 3. hello-pt-mlflow-job\n",
    "#### 3.1 configuration\n",
    "\n",
    "**Client Configuration**\n",
    "\n",
    "```\n",
    " \"components\": [\n",
    "    {\n",
    "      \"id\": \"pt_learner\",\n",
    "      \"path\": \"pt_learner.PTLearner\",\n",
    "      \"args\": {\n",
    "        \"lr\": 0.01,\n",
    "        \"epochs\": 5,\n",
    "        \"analytic_sender_id\": \"mlflow_sender\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"mlflow_sender\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_sender.MLFlowSender\",\n",
    "      \"args\": {\"event_type\": \"analytix_log_stats\"}\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"event_to_fed\",\n",
    "      \"name\": \"ConvertToFedEvent\",\n",
    "      \"args\": {\"events_to_convert\": [\"analytix_log_stats\"], \"fed_event_prefix\": \"fed.\"}\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "to use MLflow API syntax, we need to register with MLFlowSender\n",
    "\n",
    "\n",
    "**Server Configuration**\n",
    "\n",
    "  in addition to the other normal configuration for training, we need to add the following component to handle\n",
    "  the streamed events. \n",
    "  \n",
    "  If the MLfLow tracking server is used, we need to specify the tracking URL, \n",
    "  If the MLflow tracking server is not user, we don't need to specify tracking URL in the argument. \n",
    "  \n",
    "  **with tracking server**\n",
    "  \n",
    "``` \n",
    "  {\n",
    "      \"id\": \"mlflow_receiver\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_receiver.MLFlowReceiver\",\n",
    "      \"args\": {\n",
    "        \"kwargs\": {\"experiment_name\": \"hello-pt-experiments\"},\n",
    "        \"artifact_location\": \"artifacts\",\n",
    "        \"tracking_uri\" : \"http://<tracking_server_host>:5000\"\n",
    "      }\n",
    "    }\n",
    "```    \n",
    "   **without tracking server**\n",
    "```   \n",
    "  \n",
    "  {\n",
    "      \"id\": \"mlflow_receiver\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_receiver.MLFlowReceiver\",\n",
    "      \"args\": {\n",
    "        \"kwargs\": {\"experiment_name\": \"hello-pt-experiments\"},\n",
    "        \"artifact_location\": \"artifacts\",\n",
    "      }\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284b4a2",
   "metadata": {},
   "source": [
    "#### 3.2 MLFlow Tracking Server\n",
    " \n",
    "MLFLow Tracking Server can be setup and deployed separately. For example, in Azure ML Workspace, the MLFlow tracking server is already setup, all one needs is to find out the tracking URL\n",
    " \n",
    "In this example, we will setup a simple tracking server with SQLite database: \n",
    "\n",
    "```\n",
    "mlflow server --backend-store-uri=sqlite:///mlrunsdb15.db  --host localhost --port 5000\n",
    "\n",
    "```\n",
    "the user then can go to http://localhost:5000 to monitoring the experiments during job run\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3165d",
   "metadata": {},
   "source": [
    "#### 3.3  Run the experiment\n",
    "\n",
    "Use nvflare simulator to run the hello-examples, assuming NVFLARE_HOME is setup and point to the github clone of the NVFLARE code base. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f08cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd $NVFLARE_HOME\n",
    "\n",
    "nvflare simulator -w /tmp/nvflare/ -n 2 -t 2 examples/hello-pt-mlflow/hello-pt-mlflow-job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b9f60",
   "metadata": {},
   "source": [
    "#### 3.4. Runing experiments without tracking server and tracking URL\n",
    "\n",
    "if we don't specify tracking URL and no tracking server. \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "  {\n",
    "      \"id\": \"mlflow_receiver\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_receiver.MLFlowReceiver\",\n",
    "      \"args\": {\n",
    "        \"kwargs\": {\"experiment_name\": \"hello-pt-experiments\"},\n",
    "        \"artifact_location\": \"artifacts\",\n",
    "      }\n",
    "    }\n",
    "```\n",
    "we can simply run the experiments as before. Meanwhile, we can track the progress by the following command\n",
    "(notice our workspace is point to /tmp/nvflare) \n",
    "\n",
    "\n",
    "**mlflow ui --backend-store-uri=/tmp/nvflare/mlrun**\n",
    "\n",
    "\n",
    "run above from terminal ( it doesn't work running from Notebook)\n",
    "```\n",
    " mlflow ui --backend-store-uri=/tmp/nvflare/mlruns\n",
    " \n",
    "[2023-01-05 15:30:38 -0800] [71735] [INFO] Starting gunicorn 20.1.0\n",
    "[2023-01-05 15:30:38 -0800] [71735] [INFO] Listening at: http://127.0.0.1:5000 (71735)\n",
    "[2023-01-05 15:30:38 -0800] [71735] [INFO] Using worker: sync\n",
    "[2023-01-05 15:30:38 -0800] [71737] [INFO] Booting worker with pid: 71737\n",
    "[2023-01-05 15:30:38 -0800] [71738] [INFO] Booting worker with pid: 71738\n",
    "[2023-01-05 15:30:38 -0800] [71739] [INFO] Booting worker with pid: 71739\n",
    "[2023-01-05 15:30:38 -0800] [71740] [INFO] Booting worker with pid: 71740\n",
    "\n",
    "```\n",
    "\n",
    "then user should open http://127.0.0.1:5000 via browser check the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36936541",
   "metadata": {},
   "source": [
    "#### 3.5.  Tensorboard Reciver\n",
    "\n",
    "In this example, we uses the log_params(), log_text(), log_metrics(), set_tags() in various places in the code. \n",
    "You should be able to see them in the MLFlow UI http://localhost:5000 \n",
    "\n",
    "What happens if we replace MLFlow Receiver with Tensorboard Reciever ? \n",
    "\n",
    "**Server Config**\n",
    "\n",
    "Replace the following component\n",
    "```\n",
    "  {\n",
    "      \"id\": \"mlflow_receiver\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_receiver.MLFlowReceiver\",\n",
    "      \"args\": {\n",
    "        \"kwargs\": {\"experiment_name\": \"hello-pt-experiments\"},\n",
    "        \"artifact_location\": \"artifacts\",\n",
    "        \"tracking_uri\" : \"http://<tracking_server_host>:5000\"\n",
    "      }\n",
    "    }\n",
    "```\n",
    "with\n",
    "```\n",
    "    {\n",
    "      \"id\": \"tb_analytics_receiver\",\n",
    "      \"name\": \"TBAnalyticsReceiver\",\n",
    "      \"args\": {\"events\": [\"fed.analytix_log_stats\"]}\n",
    "    },\n",
    "```\n",
    "\n",
    "\n",
    "re-run the example without any code changes and then launch Tensorboard to view the result\n",
    "\n",
    "```\n",
    "tensorboard --logdir=/tmp/nvflare/simulate_job/tb_events\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cd1cc8",
   "metadata": {},
   "source": [
    "### 4. hello-pt-tb-mlflow-job\n",
    "\n",
    "This example is the same as hello-pt-tb. Except that we add one more component in the server configuration:\n",
    "**MLFlowReceiver**. This is on top of the tensorboard receiver that is already in place. \n",
    "\n",
    "In other words, we will have two receivers for the same tracking data from Tensorboard summary writer. The client code has no change. \n",
    "\n",
    "#### 4.1 Configuration\n",
    "\n",
    "**Server Configuration**\n",
    "\n",
    "    {\n",
    "      \"id\": \"mlflow_receiver\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_receiver.MLFlowReceiver\",\n",
    "      \"args\": {\n",
    "        \"kwargs\": {\"experiment_name\": \"hello-pt-experiments\"},\n",
    "        \"artifact_location\": \"artifacts\"\n",
    "      }\n",
    "    }\n",
    "\n",
    "#### 4.2 Run the experiment\n",
    "\n",
    "Use nvflare simulator to run the hello-examples, assuming NVFLARE_HOME is setup and point to the github clone of the NVFLARE code base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9059ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd $NVFLARE_HOME\n",
    "\n",
    "nvflare simulator -w /tmp/nvflare/ -n 2 -t 2 examples/hello-pt-mlflow/hello-pt-tb-mlflow-job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d00295",
   "metadata": {},
   "source": [
    "#### 4.3 View Results\n",
    "\n",
    "**Tensorboard View**\n",
    "\n",
    "from terminal: \n",
    "```\n",
    "tensorboard --logdir=/tmp/nvflare/simulate_job/tb_events\n",
    "TensorFlow installation not found - running with reduced feature set.\n",
    "\n",
    "NOTE: Using experimental fast data loading logic. To disable, pass\n",
    "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
    "    https://github.com/tensorflow/tensorboard/issues/4784\n",
    "\n",
    "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "TensorBoard 2.11.0 at http://localhost:6007/ (Press CTRL+C to quit)\n",
    "\n",
    "```\n",
    "then open broser with  http://localhost:6007/ URL\n",
    "\n",
    "**MLFlow View**\n",
    "\n",
    "From terminal: \n",
    "\n",
    "```\n",
    "mlflow ui --backend-store-uri=/tmp/nvflare/mlruns\n",
    "```\n",
    "\n",
    "Then, look at the URL in browser http://localhost:5000/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d7879",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
