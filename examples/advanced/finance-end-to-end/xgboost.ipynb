{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66676f50-e23a-44bf-b75b-7fa917ab7055",
   "metadata": {},
   "source": [
    "# End-to-end credit card fraud detection with Federated XGBoost\n",
    "\n",
    "This notebooks shows the how do we convert and existing tabular credit data, enrich and pre-process data using one-site (like centralized dataset) and then convert this centralized process into a federated ETL steps, easily. Then construct a federated XGBoost, the only thing user need to define is the XGboost data loader. \n",
    "\n",
    "## Install requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dd6a9-ca60-42c8-b85f-f4a7b11c3a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d994614-7d32-40c0-9645-ed62eed2654b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Data Preparation \n",
    "First, we prepare the data by adding random transactional information to the base creditcard dataset following the below script:\n",
    "\n",
    "* [prepare data](./prepare_data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c008a-b19b-4c1a-b3c4-c376eccf53ba",
   "metadata": {},
   "source": [
    "## Step 2, Option 1: Feature Enrichment\n",
    "\n",
    "### Single-site Enrichment and Additional Processing\n",
    "The detailed feature enrichment step is illustrated using one site as example: \n",
    "\n",
    "* [feature_enrichments with-one-site](./feature_enrichment.ipynb)\n",
    "\n",
    "Similarly, we examine the additional pre-processing step using one site: \n",
    "\n",
    "* [pre-processing with one-site](./pre_process.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8bb99-a253-415e-8953-91af62ef22a2",
   "metadata": {},
   "source": [
    "### Federated Job to Perform on All Sites\n",
    "In order to run feature enrichment and processing job on each site similar to above steps, we wrote federated ETL job scripts for client-side based on single-site implementations.\n",
    "\n",
    "* [enrichment script](./enrich.py)\n",
    "* [pre-processing script](./pre_process.py) \n",
    "\n",
    "Then we define job scripts on server-side to trigger and coordinate running client-side scripts on each site: \n",
    "\n",
    "* [enrich_job.py](./enrich_job.py)\n",
    "* [pre-processing-job](./pre_process_job.py)\n",
    "\n",
    "Example script as below:\n",
    "```\n",
    "# Define the enrich_ctrl workflow and send to server\n",
    "    enrich_ctrl = ETLController(task_name=\"enrich\")\n",
    "    job.to(enrich_ctrl, \"server\", id=\"enrich\")\n",
    "\n",
    "    # Add clients\n",
    "    for site_name in site_names:\n",
    "        executor = ScriptExecutor(task_script_path=task_script_path, task_script_args=task_script_args)\n",
    "        job.to(executor, site_name, tasks=[\"enrich\"], gpu=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068c808-fce8-4f27-b0cf-76b486a24903",
   "metadata": {},
   "source": [
    "## Step 2, Option 2: Feature Encoding\n",
    "Rather than hand-crafting the derived features as Option 1, we can also use machine learning models to encode the features.\n",
    "\n",
    "In this example, we use federated graph neural network (GNN) to learn and generate the feature embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a102a09-1424-4f24-bb37-f8c65040950d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Federated XGBoost \n",
    "\n",
    "Now that we have the data ready, either enriched and normalized features, or GNN feature embeddings, we can fit them with XGBoost. NVIDIA FLARE has already has written XGBoost Controller and Executor for the job. All we need to provide is the data loader to fit into the XGBoost.\n",
    "\n",
    "To specify the controller and executor, we need to define a Job. You can find the job construction in\n",
    "\n",
    "* [xgb_job.py](./xgb_job.py). \n",
    "\n",
    "Below is main part of the code\n",
    "\n",
    "```\n",
    "    controller = XGBFedController(\n",
    "        num_rounds=num_rounds,\n",
    "        training_mode=\"horizontal\",\n",
    "        xgb_params=xgb_params,\n",
    "        xgb_options={\"early_stopping_rounds\": early_stopping_rounds},\n",
    "    )\n",
    "    job.to(controller, \"server\")\n",
    "\n",
    "    # Add clients\n",
    "    for site_name in site_names:\n",
    "        executor = FedXGBHistogramExecutor(data_loader_id=\"data_loader\")\n",
    "        job.to(executor, site_name, gpu=0)\n",
    "        data_loader = CreditCardDataLoader(root_dir=root_dir, file_postfix=file_postfix)\n",
    "        job.to(data_loader, site_name, id=\"data_loader\")\n",
    "```\n",
    "> file_postfix\n",
    "  file_postfix is default to \"_normalized.csv\", we are loading the normalized csv files normalized by pre-processing step. \n",
    "  the files are \n",
    "  * train__normalized.csv\n",
    "  * test__normalized.csv\n",
    "  \n",
    "\n",
    "Notice we assign defined a [```CreditCardDataLoader```](./xgb_data_loader.py), this a XGBLoader we defined to load the credit card dataset. \n",
    "\n",
    "```\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost.core import DataSplitMode\n",
    "\n",
    "from nvflare.app_opt.xgboost.data_loader import XGBDataLoader\n",
    "\n",
    "\n",
    "class CreditCardDataLoader(XGBDataLoader):\n",
    "    def __init__(self, root_dir: str, file_postfix: str):\n",
    "        self.dataset_names = [\"train\", \"test\"]\n",
    "        self.base_file_names = {}\n",
    "        self.root_dir = root_dir\n",
    "        self.file_postfix = file_postfix\n",
    "        for name in self.dataset_names:\n",
    "            self.base_file_names[name] = name + file_postfix\n",
    "        self.numerical_columns = [\n",
    "            \"Timestamp\",\n",
    "            \"Amount\",\n",
    "            \"trans_volume\",\n",
    "            \"total_amount\",\n",
    "            \"average_amount\",\n",
    "            \"hist_trans_volume\",\n",
    "            \"hist_total_amount\",\n",
    "            \"hist_average_amount\",\n",
    "            \"x2_y1\",\n",
    "            \"x3_y2\",\n",
    "        ]\n",
    "\n",
    "    def load_data(self, client_id: str, split_mode: int) -> Tuple[xgb.DMatrix, xgb.DMatrix]:\n",
    "        data = {}\n",
    "        for ds_name in self.dataset_names:\n",
    "            print(\"\\nloading for site = \", client_id, f\"{ds_name} dataset \\n\")\n",
    "            file_name = os.path.join(self.root_dir, client_id, self.base_file_names[ds_name])\n",
    "            df = pd.read_csv(file_name)\n",
    "            data_num = len(data)\n",
    "\n",
    "            # split to feature and label\n",
    "            y = df[\"Class\"]\n",
    "            x = df[self.numerical_columns]\n",
    "            data[ds_name] = (x, y, data_num)\n",
    "\n",
    "\n",
    "        # training\n",
    "        x_train, y_train, total_train_data_num = data[\"train\"]\n",
    "        data_split_mode = DataSplitMode(split_mode)\n",
    "        dmat_train = xgb.DMatrix(x_train, label=y_train, data_split_mode=data_split_mode)\n",
    "\n",
    "        # validation\n",
    "        x_valid, y_valid, total_valid_data_num = data[\"test\"]\n",
    "        dmat_valid = xgb.DMatrix(x_valid, label=y_valid, data_split_mode=data_split_mode)\n",
    "\n",
    "        return dmat_train, dmat_valid\n",
    "```\n",
    "\n",
    "We are now ready to run all the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036417d1-ad58-4835-b59b-fae94aafded3",
   "metadata": {},
   "source": [
    "## Run All the Jobs End-to-end\n",
    "Here we are going to run each job in sequence. For real-world use case,\n",
    "\n",
    "* prepare data is not needed, as you already have the data\n",
    "* feature enrichment / encoding scripts need to be defined based on your own technique\n",
    "* for XGBoost Job, you will need to write your own data loader \n",
    "\n",
    "Note: All Sender SICs are considered clients: they are \n",
    "* 'ZHSZUS33_Bank_1'\n",
    "* 'SHSHKHH1_Bank_2'\n",
    "* 'YXRXGB22_Bank_3'\n",
    "* 'WPUWDEFF_Bank_4'\n",
    "* 'YMNYFRPP_Bank_5'\n",
    "* 'FBSFCHZH_Bank_6'\n",
    "* 'YSYCESMM_Bank_7'\n",
    "* 'ZNZZAU3M_Bank_8'\n",
    "* 'HCBHSGSG_Bank_9'\n",
    "* 'XITXUS33_Bank_10' \n",
    "Total 10 banks\n",
    "\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e5788-8985-4c89-ba34-987bb407be9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python3 prepare_data.py -i ./creditcard.csv -o /tmp/nvflare/xgb/credit_card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa1f2c-e3ca-4ced-b94e-6a68cf4b809e",
   "metadata": {},
   "source": [
    "### Enrich data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335c190-6db1-499b-b1e3-6667675a45a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python enrich_job.py -c 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'FBSFCHZH_Bank_6' 'YMNYFRPP_Bank_5' 'WPUWDEFF_Bank_4' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'YSYCESMM_Bank_7' 'ZHSZUS33_Bank_1' 'HCBHSGSG_Bank_9' -p enrich.py  -a \"-i /tmp/nvflare/xgb/credit_card/ -o /tmp/nvflare/xgb/credit_card/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8cba2c-9018-410b-93c2-930816d65a33",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515a542-cb96-46a8-a3ac-4eb7cee5b46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python pre_process_job.py -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4' -p pre_process.py -a \"-i /tmp/nvflare/xgb/credit_card  -o /tmp/nvflare/xgb/credit_card/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5236b-0f40-4b91-9fc2-4f2836b52537",
   "metadata": {},
   "source": [
    "### Run XGBoost Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5151e2-fad7-4982-9007-db8531b1367e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python xgb_job.py -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4' -i /tmp/nvflare/xgb/credit_card  -w /tmp/nvflare/workspace/xgb/credit_card/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9090a-d50a-46d7-bc8f-388717d18f96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Job for POC and Production\n",
    "\n",
    "With job running well in simulator, we are ready to run in a POC mode, so we can simulate the deployment in localhost or simply deploy to production. \n",
    "\n",
    "All we need is the job definition. we can use job.export_job() method to generate the job configuration and export to given directory. For example, in xgb_job.py, we have the following\n",
    "\n",
    "```\n",
    "    if work_dir:\n",
    "        print(\"work_dir=\", work_dir)\n",
    "        job.export_job(work_dir)\n",
    "\n",
    "    if not args.config_only:\n",
    "        job.simulator_run(work_dir)\n",
    "```\n",
    "\n",
    "let's try this out and then look at the directory. We use ```tree``` command if you have it. othewise, simply use ```ls -al ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ffde2-e7b8-4666-86cb-8f579e5818da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python xgb_job.py -co -w /tmp/nvflare/workspace/xgb/credit_card/config -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4'  -i /tmp/nvflare/xgb/credit_card  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6922e7e-cb15-4842-8093-ef9b030621df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tree /tmp/nvflare/workspace/xgb/credit_card/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96a68d-f0d5-4cdb-b454-78384bb5cc72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/workspace/xgb/credit_card/config/xgb_job/meta.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743b92d-3711-4197-bb05-7cb3618d4539",
   "metadata": {},
   "source": [
    "Now we have the job definition, you can either run it in POC mode or production setup. \n",
    "\n",
    "* setup POC\n",
    "``` \n",
    "    nvfalre poc prepare -c <list of clients>\n",
    "    nvflare poc start -ex admin@nvidia.com  \n",
    "```\n",
    "  \n",
    "* submit job using NVFLARE console \n",
    "        \n",
    "    from different terminal \n",
    "   \n",
    "   ```\n",
    "   nvflare poc start -p admin@nvidia.com\n",
    "   ```\n",
    "   using submit job command\n",
    "    \n",
    "* use nvflare job submit command  to submit job\n",
    "\n",
    "* use NVFLARE API to submit job\n",
    "\n",
    "The exact same process for production. Please look at this site for documentation or tuturial examples: https://nvidia.github.io/NVFlare/\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf10abe-e7c3-4121-b312-a59897d5742e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
