{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66676f50-e23a-44bf-b75b-7fa917ab7055",
   "metadata": {},
   "source": [
    "# End-to-end credit card fraud detection with Federated XGBoost\n",
    "\n",
    "This notebooks shows the how do we convert and existing tabular credit data, enrich and pre-process data using one-site (like centralized dataset) and then convert this centralized process into a federated ETL steps, easily. Then construct a federated XGBoost, the only thing user need to define is the XGboost data loader. \n",
    "\n",
    "## Install requirements\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dd6a9-ca60-42c8-b85f-f4a7b11c3a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d994614-7d32-40c0-9645-ed62eed2654b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Prepare Data \n",
    "\n",
    "* [prepare data](./prepare_data.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c008a-b19b-4c1a-b3c4-c376eccf53ba",
   "metadata": {},
   "source": [
    "## Feature Enrichment\n",
    "\n",
    "We can first examine how the feature enrichment is processed using just one-site. \n",
    "\n",
    "* [feature_enrichments with-one-site](./feature_enrichment.ipynb)\n",
    "\n",
    "in order to run feature job on each site similar to above feature enrichment steps, we wrote an enrichment ETL job.\n",
    "\n",
    "[enrichment script](./enrich.py)\n",
    "\n",
    "Define a job to trigger running enrichnment script on each site: \n",
    "\n",
    "[enrich_job.py](./enrich_job.py)\n",
    "\n",
    "```\n",
    "# Define the enrich_ctrl workflow and send to server\n",
    "    enrich_ctrl = ETLController(task_name=\"enrich\")\n",
    "    job.to(enrich_ctrl, \"server\", id=\"enrich\")\n",
    "\n",
    "    # Add clients\n",
    "    for site_name in site_names:\n",
    "        executor = ScriptExecutor(task_script_path=task_script_path, task_script_args=task_script_args)\n",
    "        job.to(executor, site_name, tasks=[\"enrich\"], gpu=0)\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8bb99-a253-415e-8953-91af62ef22a2",
   "metadata": {},
   "source": [
    "## Pre-Processing \n",
    "\n",
    "We exam examine the steps for pre-processing using only one-site (one client) \n",
    "\n",
    "* [pre-processing with one-site](./pre_process.ipynb)\n",
    "\n",
    "Based on one-site, we create the pre-processing script\n",
    "\n",
    "* [pre-processing script](./pre_process.py) \n",
    "\n",
    "then we define the pre-processing job to coordinate the pre-processing for all sites\n",
    "\n",
    "* [pre-processing-job](./pre_process_job.py)\n",
    "\n",
    "```\n",
    "    pre_process_ctrl = ETLController(task_name=\"pre_process\")\n",
    "    job.to(pre_process_ctrl, \"server\", id=\"pre_process\")\n",
    "\n",
    "    # Add clients\n",
    "    for site_name in site_names:\n",
    "        executor = ScriptExecutor(task_script_path=task_script_path, task_script_args=task_script_args)\n",
    "        job.to(executor, site_name, tasks=[\"pre_process\"], gpu=0)\n",
    "\n",
    "```\n",
    " Similarly to the ETL job, we simply issue a task to trigger pre-process running pre-process script. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a102a09-1424-4f24-bb37-f8c65040950d",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "    def load_data(self, client_id: str, split_mode: int) -> Tuple[xgb.DMatrix, xgb.DMatrix]:\n",
    "        data = {}\n",
    "        for ds_name in self.dataset_names:\n",
    "            print(\"\\nloading for site = \", client_id, f\"{ds_name} dataset \\n\")\n",
    "            file_name = os.path.join(self.root_dir, client_id, self.base_file_names[ds_name])\n",
    "            df = pd.read_csv(file_name)\n",
    "            data_num = len(data)\n",
    "\n",
    "            # split to feature and label\n",
    "            y = df[\"Class\"]\n",
    "            x = df[self.numerical_columns]\n",
    "            data[ds_name] = (x, y, data_num)\n",
    "\n",
    "\n",
    "        # training\n",
    "        x_train, y_train, total_train_data_num = data[\"train\"]\n",
    "        data_split_mode = DataSplitMode(split_mode)\n",
    "        dmat_train = xgb.DMatrix(x_train, label=y_train, data_split_mode=data_split_mode)\n",
    "\n",
    "        # validation\n",
    "        x_valid, y_valid, total_valid_data_num = data[\"test\"]\n",
    "        dmat_valid = xgb.DMatrix(x_valid, label=y_valid, data_split_mode=data_split_mode)\n",
    "\n",
    "        return dmat_train, dmat_valid\n",
    "## Define XGBoost Job \n",
    "\n",
    "Now that we have the data ready, We can fit the data into XGBoost. NVIDIA FLARE has already has written XGBoost Controller and Executor for the job. All we need to provide is the data loader to fit into the XGBoost\n",
    "To specify the controller and executor, we need to define a Job.  You can find the job construction can be find in\n",
    "\n",
    "* [xgb_job.py](./xgb_job.py). \n",
    "\n",
    "Here is main part of the code\n",
    "\n",
    "```\n",
    "    controller = XGBFedController(\n",
    "        num_rounds=num_rounds,\n",
    "        training_mode=\"horizontal\",\n",
    "        xgb_params=xgb_params,\n",
    "        xgb_options={\"early_stopping_rounds\": early_stopping_rounds},\n",
    "    )\n",
    "    job.to(controller, \"server\")\n",
    "\n",
    "    # Add clients\n",
    "    for site_name in site_names:\n",
    "        executor = FedXGBHistogramExecutor(data_loader_id=\"data_loader\")\n",
    "        job.to(executor, site_name, gpu=0)\n",
    "        data_loader = CreditCardDataLoader(root_dir=root_dir, file_postfix=file_postfix)\n",
    "        job.to(data_loader, site_name, id=\"data_loader\")\n",
    "```\n",
    "> file_postfix\n",
    "  file_postfix is default to \"_normalized.csv\", we are loading the normalized csv files normalized by pre-processing step. \n",
    "  the files are \n",
    "  * train__normalized.csv\n",
    "  * test__normalized.csv\n",
    "  \n",
    "\n",
    "Notice we assign defined a [```CreditCardDataLoader```](./xgb_data_loader.py), this a XGBLoader we defined to load the credit card dataset. \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost.core import DataSplitMode\n",
    "\n",
    "from nvflare.app_opt.xgboost.data_loader import XGBDataLoader\n",
    "\n",
    "\n",
    "class CreditCardDataLoader(XGBDataLoader):\n",
    "    def __init__(self, root_dir: str, file_postfix: str):\n",
    "        self.dataset_names = [\"train\", \"test\"]\n",
    "        self.base_file_names = {}\n",
    "        self.root_dir = root_dir\n",
    "        self.file_postfix = file_postfix\n",
    "        for name in self.dataset_names:\n",
    "            self.base_file_names[name] = name + file_postfix\n",
    "        self.numerical_columns = [\n",
    "            \"Timestamp\",\n",
    "            \"Amount\",\n",
    "            \"trans_volume\",\n",
    "            \"total_amount\",\n",
    "            \"average_amount\",\n",
    "            \"hist_trans_volume\",\n",
    "            \"hist_total_amount\",\n",
    "            \"hist_average_amount\",\n",
    "            \"x2_y1\",\n",
    "            \"x3_y2\",\n",
    "        ]\n",
    "\n",
    "    def load_data(self, client_id: str, split_mode: int) -> Tuple[xgb.DMatrix, xgb.DMatrix]:\n",
    "        data = {}\n",
    "        for ds_name in self.dataset_names:\n",
    "            print(\"\\nloading for site = \", client_id, f\"{ds_name} dataset \\n\")\n",
    "            file_name = os.path.join(self.root_dir, client_id, self.base_file_names[ds_name])\n",
    "            df = pd.read_csv(file_name)\n",
    "            data_num = len(data)\n",
    "\n",
    "            # split to feature and label\n",
    "            y = df[\"Class\"]\n",
    "            x = df[self.numerical_columns]\n",
    "            data[ds_name] = (x, y, data_num)\n",
    "\n",
    "\n",
    "        # training\n",
    "        x_train, y_train, total_train_data_num = data[\"train\"]\n",
    "        data_split_mode = DataSplitMode(split_mode)\n",
    "        dmat_train = xgb.DMatrix(x_train, label=y_train, data_split_mode=data_split_mode)\n",
    "\n",
    "        # validation\n",
    "        x_valid, y_valid, total_valid_data_num = data[\"test\"]\n",
    "        dmat_valid = xgb.DMatrix(x_valid, label=y_valid, data_split_mode=data_split_mode)\n",
    "\n",
    "        return dmat_train, dmat_valid\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "We are now ready to run all the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036417d1-ad58-4835-b59b-fae94aafded3",
   "metadata": {},
   "source": [
    "## Run all the Jobs\n",
    "Here we are going to run each job in sequence. For real-world use case,\n",
    "\n",
    "* prepare data is not needed, as you already have the data\n",
    "* feature enrichment scripts need to be define based on your own enrichment rules\n",
    "* pre-processing, you also need to change the pre-process script to define normalization and categorical encodeing\n",
    "* for XGBoost Job, you will need to write your own data loader \n",
    "\n",
    "Note: All Sender SICs are considered clients: they are \n",
    "* 'ZHSZUS33_Bank_1'\n",
    "* 'SHSHKHH1_Bank_2'\n",
    "* 'YXRXGB22_Bank_3'\n",
    "* 'WPUWDEFF_Bank_4'\n",
    "* 'YMNYFRPP_Bank_5'\n",
    "* 'FBSFCHZH_Bank_6'\n",
    "* 'YSYCESMM_Bank_7'\n",
    "* 'ZNZZAU3M_Bank_8'\n",
    "* 'HCBHSGSG_Bank_9'\n",
    "* 'XITXUS33_Bank_10' \n",
    "Total 10 banks\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e5788-8985-4c89-ba34-987bb407be9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python3 prepare_data.py -i ./creditcard.csv -o /tmp/nvflare/xgb/credit_card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa1f2c-e3ca-4ced-b94e-6a68cf4b809e",
   "metadata": {},
   "source": [
    "### Enrich data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335c190-6db1-499b-b1e3-6667675a45a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python enrich_job.py -c 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'FBSFCHZH_Bank_6' 'YMNYFRPP_Bank_5' 'WPUWDEFF_Bank_4' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'YSYCESMM_Bank_7' 'ZHSZUS33_Bank_1' 'HCBHSGSG_Bank_9' -p enrich.py  -a \"-i /tmp/nvflare/xgb/credit_card/ -o /tmp/nvflare/xgb/credit_card/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8cba2c-9018-410b-93c2-930816d65a33",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515a542-cb96-46a8-a3ac-4eb7cee5b46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python pre_process_job.py -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4' -p pre_process.py -a \"-i /tmp/nvflare/xgb/credit_card  -o /tmp/nvflare/xgb/credit_card/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5236b-0f40-4b91-9fc2-4f2836b52537",
   "metadata": {},
   "source": [
    "### Run XGBoost Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5151e2-fad7-4982-9007-db8531b1367e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python xgb_job.py -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4' -i /tmp/nvflare/xgb/credit_card  -w /tmp/nvflare/workspace/xgb/credit_card/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9090a-d50a-46d7-bc8f-388717d18f96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Job for POC and Production\n",
    "\n",
    "This seems to work well with Job running in simulator. Now we are ready to run in a POC mode, so we can simulate the deployment in localhost or simply deploy to production. \n",
    "\n",
    "All we need is the job definition. we can use job.export_job() method to generate the job configuration and export to given directory. For example, in xgb_job.py, we have the following\n",
    "\n",
    "```\n",
    "    if work_dir:\n",
    "        print(\"work_dir=\", work_dir)\n",
    "        job.export_job(work_dir)\n",
    "\n",
    "    if not args.config_only:\n",
    "        job.simulator_run(work_dir)\n",
    "```\n",
    "\n",
    "let's try this out and then look at the directory. We use ```tree``` command if you have it. othewise, simply use ```ls -al ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ffde2-e7b8-4666-86cb-8f579e5818da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python xgb_job.py -co -w /tmp/nvflare/workspace/xgb/credit_card/config -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4'  -i /tmp/nvflare/xgb/credit_card  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6922e7e-cb15-4842-8093-ef9b030621df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tree /tmp/nvflare/workspace/xgb/credit_card/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96a68d-f0d5-4cdb-b454-78384bb5cc72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/workspace/xgb/credit_card/config/xgb_job/meta.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743b92d-3711-4197-bb05-7cb3618d4539",
   "metadata": {},
   "source": [
    "Now we have the job definition, you can either run it in POC mode or production setup. \n",
    "\n",
    "* setup POC\n",
    "``` \n",
    "    nvfalre poc prepare -c <list of clients>\n",
    "    nvflare poc start -ex admin@nvidia.com  \n",
    "```\n",
    "  \n",
    "* submit job using NVFLARE console \n",
    "        \n",
    "    from different terminal \n",
    "   \n",
    "   ```\n",
    "   nvflare poc start -p admin@nvidia.com\n",
    "   ```\n",
    "   using submit job command\n",
    "    \n",
    "* use nvflare job submit command  to submit job\n",
    "\n",
    "* use NVFLARE API to submit job\n",
    "\n",
    "The exact same process for production. Please look at this site for documentation or tuturial examples: https://nvidia.github.io/NVFlare/\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf10abe-e7c3-4121-b312-a59897d5742e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
