{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d892b5e-2f3b-4182-bedb-d332bfc3a353",
   "metadata": {},
   "source": [
    "# GNN Training and Encoding\n",
    "\n",
    "* Train a GNN based on enriched features in an unsupervised fashion, and use the resulting model to encode the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8498bf1-d368-4d15-a5bf-559eb6e3918b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d04f0-a64d-457b-aacf-1a3737e07e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_input_dir = \"/tmp/dataset/horizontal_credit_fraud_data/\"\n",
    "site_name = \"ZHSZUS33_Bank_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d84f89f-fe0a-4387-92a2-49ca9143c141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "dataset_names = [\"train\", \"test\"]\n",
    "df_feats = {}\n",
    "df_edges = {}\n",
    "for ds_name in dataset_names:\n",
    "    # Get feature and class\n",
    "    file_name = os.path.join(site_input_dir, site_name, f\"{ds_name}_normalized.csv\")\n",
    "    df = pd.read_csv(file_name, index_col=0)\n",
    "    # Drop irrelevant columns\n",
    "    df = df.drop(columns=[\"Currency_Country\",\n",
    "                          \"Beneficiary_BIC\",\n",
    "                          \"Currency\",\n",
    "                          \"Receiver_BIC\",\n",
    "                          \"Sender_BIC\"])  \n",
    "    df_feats[ds_name] = df\n",
    "    # Get edge map\n",
    "    file_name = os.path.join(site_input_dir, site_name, f\"{ds_name}_edgemap.csv\")\n",
    "    df = pd.read_csv(file_name, header=None)\n",
    "    # Add column names to the edge map\n",
    "    df.columns = [\"UETR_1\", \"UETR_2\"]\n",
    "    df_edges[ds_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95b6b9d-7046-4ed4-8a7e-ce1f74ddf694",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepared Data for Unsupervised GNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd5be54-c5e7-43c7-ad4f-de29a09bc7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "node_ids = {}\n",
    "node_features = {}\n",
    "edge_indices = {}\n",
    "weights = {}\n",
    "labels = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    df_feat_class = df_feats[ds_name]\n",
    "    df_edge = df_edges[ds_name]\n",
    "\n",
    "    # Sort the data by UETR\n",
    "    df_feat_class = df_feat_class.sort_values(by=\"UETR\").reset_index(drop=True)\n",
    "\n",
    "    # Generate UETR-index map with the feature list\n",
    "    node_id = df_feat_class[\"UETR\"].values\n",
    "    map_id = {j: i for i, j in enumerate(node_id)}  # mapping nodes to indexes\n",
    "    node_ids[ds_name] = node_id\n",
    "    \n",
    "    # Get class labels\n",
    "    labels[ds_name] = df_feat_class[\"Class\"].values\n",
    "\n",
    "    # Map UETR to indexes in the edge map\n",
    "    edges = df_edge.copy()\n",
    "    edges.UETR_1 = edges.UETR_1.map(map_id)\n",
    "    edges.UETR_2 = edges.UETR_2.map(map_id)\n",
    "    edges = edges.astype(int)\n",
    "\n",
    "    # for undirected graph\n",
    "    edge_index = np.array(edges.values).T\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
    "    edge_indices[ds_name] = edge_index\n",
    "    weights[ds_name] = torch.tensor([1] * edge_index.shape[1], dtype=torch.float)\n",
    "\n",
    "    # UETR mapped to corresponding indexes, drop UETR and class\n",
    "    node_feature = df_feat_class.drop([\"UETR\", \"Class\"], axis=1).copy()\n",
    "    node_feature = torch.tensor(np.array(node_feature.values), dtype=torch.float)\n",
    "    node_features[ds_name] = node_feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b192a7-05be-4591-b937-7bab878277ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Unsupervised GNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326a613-e683-4f67-810d-aece3d90349e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "\n",
    "output_dir = os.path.join(site_input_dir, site_name)\n",
    "DEVICE = \"cuda:0\"\n",
    "writer = SummaryWriter(output_dir)\n",
    "epochs = 100\n",
    "\n",
    "# Converting data to PyG graph data format\n",
    "train_data = Data(\n",
    "    x=node_features['train'], edge_index=edge_indices['train'], edge_attr=weights['train']\n",
    ")\n",
    "\n",
    "# Define the dataloader for graphsage training\n",
    "loader = LinkNeighborLoader(\n",
    "    train_data,\n",
    "    batch_size=2048,\n",
    "    shuffle=True,\n",
    "    neg_sampling_ratio=1.0,\n",
    "    num_neighbors=[10, 10],\n",
    "    num_workers=6,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = GraphSAGE(\n",
    "    in_channels=node_features['train'].shape[1],\n",
    "    hidden_channels=64,\n",
    "    num_layers=2,\n",
    "    out_channels=64,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "model.to(DEVICE)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = instance_count = 0\n",
    "\n",
    "    for data in loader:\n",
    "        # get the inputs data\n",
    "        data = data.to(DEVICE)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        h = model(data.x, data.edge_index)\n",
    "        h_src = h[data.edge_label_index[0]]\n",
    "        h_dst = h[data.edge_label_index[1]]\n",
    "        link_pred = (h_src * h_dst).sum(dim=-1)  # Inner product.\n",
    "        loss = F.binary_cross_entropy_with_logits(link_pred, data.edge_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # add record\n",
    "        running_loss += float(loss.item()) * link_pred.numel()\n",
    "        instance_count += link_pred.numel()\n",
    "    print(f\"Epoch: {epoch:02d}, Loss: {running_loss / instance_count:.4f}\")\n",
    "    writer.add_scalar(\"train_loss\", running_loss / instance_count, epoch)\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), os.path.join(output_dir, \"model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a5b581-1688-4c43-a83a-f3b152d05729",
   "metadata": {
    "tags": []
   },
   "source": [
    "## GNN Inference - Encoding the Raw Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe6156-1049-41c5-82d5-b81fa1814160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model and perform inference / encoding\n",
    "model_enc = GraphSAGE(\n",
    "    in_channels=node_features['train'].shape[1],\n",
    "    hidden_channels=64,\n",
    "    num_layers=2,\n",
    "    out_channels=64,\n",
    ")\n",
    "model_enc.load_state_dict(torch.load(os.path.join(output_dir, \"model.pt\")))\n",
    "model_enc.eval()\n",
    "\n",
    "embeds = {}\n",
    "# Perform encoding\n",
    "for ds_name in dataset_names:\n",
    "    h = model_enc(node_features[ds_name], edge_indices[ds_name])\n",
    "    embed = pd.DataFrame(h.cpu().detach().numpy())\n",
    "    # Add column names as V_0, V_1, ... V_63\n",
    "    embed.columns = [f\"V_{i}\" for i in range(embed.shape[1])]\n",
    "    # Concatenate the node ids and class labels with the encoded features\n",
    "    embed[\"UETR\"] = node_ids[ds_name]\n",
    "    embed[\"Class\"] = labels[ds_name]\n",
    "    # Move the UETR and Class columns to the front\n",
    "    embed = embed[[\"UETR\", \"Class\"] + [col for col in embed.columns if col not in [\"UETR\", \"Class\"]]]\n",
    "    embed.to_csv(os.path.join(output_dir, f\"{ds_name}_embedding.csv\"), index=False)\n",
    "    embeds[ds_name] = embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8925c-6890-4a45-a9c4-f80399b463cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tree /tmp/dataset/horizontal_credit_fraud_data/ZHSZUS33_Bank_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adcd468-edaf-4759-ac2d-09902811c97a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeds[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591e4e1-74b1-465c-8124-eaf9829a6a8e",
   "metadata": {},
   "source": [
    "Let's go back to the [XGBoost Notebook](../xgboost.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926970e-a4e9-41a7-a166-0d11f8e9e320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
