{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d892b5e-2f3b-4182-bedb-d332bfc3a353",
   "metadata": {},
   "source": [
    "# PreProcess Step\n",
    "\n",
    "* Encode categorical data\n",
    "* normalize the features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8498bf1-d368-4d15-a5bf-559eb6e3918b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d04f0-a64d-457b-aacf-1a3737e07e12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_input_dir = \"/tmp/dataset/horizontal_credit_fraud_data/\"\n",
    "site_name = \"ZHSZUS33_Bank_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d84f89f-fe0a-4387-92a2-49ca9143c141",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "dataset_names = [\"train\", \"test\"]\n",
    "datasets = {}\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    file_name = os.path.join(site_input_dir, site_name, f\"{ds_name}_enrichment.csv\" )\n",
    "    df = pd.read_csv(file_name)\n",
    "    datasets[ds_name] = df\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200c438-f285-4037-ad22-b496d57588ca",
   "metadata": {},
   "source": [
    "### Categorical encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a87c30-5ebe-4fea-9d69-2f31b07f863a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_columns = ['Currency_Country', 'Beneficiary_BIC', 'Currency', 'UETR', 'Receiver_BIC', 'Sender_BIC']\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    df = datasets[ds_name]\n",
    "    df_encoded = pd.get_dummies(df, columns=category_columns)\n",
    "    print(df_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51062ad-cf73-4ddf-86a9-87ce14d216e4",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdc785e-9597-4083-b74a-2cacb25b20cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd5be54-c5e7-43c7-ad4f-de29a09bc7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "\n",
    "processed_dfs = {}\n",
    "\n",
    "numerical_columns = ['Timestamp', 'Class', 'Amount', 'trans_volume', 'total_amount', 'average_amount', 'hist_trans_volume',\n",
    "       'hist_total_amount', 'hist_average_amount', 'x2_y1', 'x3_y2']\n",
    "\n",
    "for ds_name in dataset_names:\n",
    "    df = datasets[ds_name]\n",
    "    \n",
    "    # Convert 'Time' column to datetime\n",
    "    df['Time'] = pd.to_datetime(df['Time'])\n",
    "    # Convert datetime to Unix timestamp\n",
    "    df['Timestamp'] = df['Time'].astype(int) / 10**9  # convert to seconds\n",
    "    \n",
    "    # Separate numerical and categorical features\n",
    "    numerical_features = df[numerical_columns]\n",
    "    categorical_features = df[category_columns]\n",
    "\n",
    "    # Initialize the MinMaxScaler (or StandardScaler)\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Fit and transform the numerical data\n",
    "    numerical_normalized = pd.DataFrame(scaler.fit_transform(numerical_features), columns=numerical_features.columns)\n",
    "    \n",
    "    # Combine the normalized numerical features with the categorical features\n",
    "    df_combined = pd.concat([categorical_features, numerical_normalized], axis=1)\n",
    "        \n",
    "#     # one-hot encoding\n",
    "#     df_combined = pd.get_dummies(df_combined, columns=category_columns)\n",
    "\n",
    "    print(\"Combined DataFrame with Normalized Numerical Features:\")\n",
    "    print(df_combined)\n",
    "    \n",
    "    processed_dfs[ds_name] = df_combined\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326a613-e683-4f67-810d-aece3d90349e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in processed_dfs:\n",
    "    site_dir = os.path.join(site_input_dir, site_name)\n",
    "    os.makedirs(site_dir, exist_ok=True)\n",
    "    pre_processed_file_name = os.path.join(site_dir, f\"{name}_normalized.csv\")\n",
    "    print(pre_processed_file_name)\n",
    "    processed_dfs[name].to_csv(pre_processed_file_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8925c-6890-4a45-a9c4-f80399b463cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tree /tmp/dataset/horizontal_credit_fraud_data/ZHSZUS33_Bank_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8591e4e1-74b1-465c-8124-eaf9829a6a8e",
   "metadata": {},
   "source": [
    "Let's go back to the [XGBoost Notebook](../xgboost.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26989ac6-cedf-4c9d-8b25-60e0af758cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
