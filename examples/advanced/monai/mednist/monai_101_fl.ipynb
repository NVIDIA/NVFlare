{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2026, NVIDIA CORPORATION.  All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "MONAI Example adopted from https://github.com/Project-MONAI/tutorials/blob/main/2d_classification/monai_101.ipynb\n",
    "\n",
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# MONAI 101 tutorial with Federated Learning\n",
    "\n",
    "In this example, we use NVFlare's [`FedAvgRecipe`](https://nvflare.readthedocs.io/en/main/programming_guide/recipes.html) to configure and execute federated learning. The recipe simplifies FL job creation by handling the following:\n",
    "\n",
    "1. **Initialize the federated learning job** with an initial model (DenseNet121), training script, and configuration parameters such as number of rounds and minimum clients.\n",
    "2. **Configure experiment tracking** (optional) using TensorBoard and/or MLflow to monitor training metrics across clients.\n",
    "3. **Setup simulation environment** using [`SimEnv`](https://nvflare.readthedocs.io/en/main/programming_guide/simulation.html) to simulate multiple clients in parallel threads.\n",
    "4. **Execute FedAvg workflow** which:\n",
    "   - Sends the global model to participating clients each round\n",
    "   - Aggregates client updates using weighted averaging based on local training samples\n",
    "   - Updates the global model with aggregated results\n",
    "   - Repeats for the specified number of rounds\n",
    "\n",
    "The **clients** implement the local training logic using NVFlare's [Client\n",
    "API](https://nvflare.readthedocs.io/en/main/programming_guide/execution_api_type.html#client-api)\n",
    "[here](./code/client.py). The Client API\n",
    "allows the user to add minimum `nvflare`-specific codes to turn a typical\n",
    "centralized training script to a federated client-side local training\n",
    "script.\n",
    "1. During local training, each client receives a copy of the global\n",
    "  model sent by the server using `flare.receive()` API. The received\n",
    "  global model is an instance of `FLModel`.\n",
    "2. A local validation is first performed, where validation metrics\n",
    "  (accuracy and precision) are streamed to server using the\n",
    "  [`SummaryWriter`](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.client.tracking.html#nvflare.client.tracking.SummaryWriter). The\n",
    "  streamed metrics can be loaded and visualized using [TensorBoard](https://www.tensorflow.org/tensorboard) or [MLflow](https://mlflow.org/).\n",
    "3. Then, each client performs local training as in the non-federated training [notebook](./monai_101.ipynb). At the end of each FL round, each client then sends the computed results (always in\n",
    "  `FLModel` format) to the server for aggregation, using the `flare.send()`\n",
    "  API.\n",
    "\n",
    "This tutorial will use about 7GB of GPU memory and 10 minutes to run.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/NVFlare/blob/main/integration/monai/examples/mednist/monai_101_fl.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Federated Learning\n",
    "\n",
    "We use NVFlare's FedAvgRecipe to configure and run the federated learning job while tracking the training results in both TensorBoard and MLFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python job.py --n_clients 2 --num_rounds 5 --tracking both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the streamed metrics\n",
    "\n",
    "The accuracy metrics streamed to the server during training can be visualized using either\n",
    "\n",
    "#### 1. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir fedavg_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/tb.png\" alt=\"TensorBoard Plot\" width=30% height=30% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or\n",
    "\n",
    "#### 2. MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow ui --backend-store-uri fedavg_workspace/mednist_fedavg/server/simulate_job/mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/mlflow.png\" alt=\"MLflow Plot\" width=50% height=30% />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
