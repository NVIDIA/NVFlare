{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Protein Downstream Fine-tuning\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> This notebook was tested on a DGX with 8 A100 GPUs with 80 GB memory each and is compatible with BioNeMo Framework v2.5. To leverage additional or higher-performance GPUs, you can modify the configuration files and simulation script to accommodate multiple devices and increase thread utilization respectively. To run with less memory consumption, you can reduce the micro-batch sizes in the `run_*.py` scripts.</div>\n",
    "\n",
    "The example datasets used here are made available by [Therapeutics Data Commons](https://tdcommons.ai/) through PyTDC.\n",
    "\n",
    "This example shows three different downstream tasks for fine-tuning a BioNeMo ESM-style model on different datasets.\n",
    "We separate the scripts and job configurations into three folders based on the dataset names:\n",
    "\n",
    "\n",
    "1. `tap`: therapeutic antibody profiling\"\n",
    "2. `sabdab`: SAbDab: the structural antibody database\"\n",
    "3. `scl`: \"subcellular location prediction\"\n",
    "\n",
    "## Setup\n",
    "\n",
    "Ensure that you have read through the Getting Started section, can run the BioNeMo Framework docker container, and have configured the NGC Command Line Interface (CLI) within the container. It is assumed that this notebook is being executed from within the container.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output.  We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output.  Comment or delete this line in the cells below to restore full output.</div>\n",
    "\n",
    "### Import and install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! pip install fuzzywuzzy PyTDC --no-dependencies  # install tdc without dependencies to avoid version conflicts in the BioNeMo container\n",
    "! pip install nvflare~=2.6rc\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 1: Cross-endpoint multi-task fitting\n",
    "\n",
    "#### Data: Five computational developability guidelines for therapeutic antibody profiling\n",
    "See https://tdcommons.ai/single_pred_tasks/develop/#tap\n",
    "- 241 Antibodies (both chains)\n",
    "\n",
    "#### Task Description: *Regression*. \n",
    "Given the antibody's heavy chain and light chain sequence, predict its developability. The input X is a list of two sequences where the first is the heavy chain and the second light chain.\n",
    "\n",
    "Includes five metrics measuring developability of an antibody: \n",
    " - Complementarity-determining regions (CDR) length - Trivial (excluded)\n",
    " - patches of surface hydrophobicity (PSH) - Run on site-1\n",
    " - patches of positive charge (PPC) - Run on site-2\n",
    " - patches of negative charge (PNC) - Run on site-3\n",
    " - structural Fv charge symmetry parameter (SFvCSP) - Run on site-4\n",
    "\n",
    "As indicated, we run each endpoint regression task on a different client. This simulates the multi-task fitting scenario with multiple endpoints where all client jointly train a shared ESM encoder trunk but keep their private regression heads for different endpoints (see the `BioNeMoExcludeParamsFilter` in [run_sum_tap.py](tap/run_sum_tap.py).\n",
    "\n",
    "<img src=\"./tap/figs/esm_multi_task.svg\" alt=\"ESM Cross-endpoint multi-task fitting\" width=\"400\"/>\n",
    "\n",
    "In the data preparation script, one can choose between uniform sampling of the data among clients and\n",
    "heterogeneous data splits using a Dirichlet sampling strategy. \n",
    "Here, different values of alpha control the level of heterogeneity. Below, we show a Dirichlet sampling of `alpha=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /bionemo_nvflare_examples/downstream/tap && python prepare_tap_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                                Uniform sampling                                 |                                    Dirichlet sampling                                     |\n",
    "|:-------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------:|\n",
    "| <img src=\"./tap/figs/tap_uniform.svg\" alt=\"Uniform data sampling\" width=\"300\"/> | <img src=\"./tap/figs/tap_alpha1.0.svg\" alt=\"Dirichlet sampling (alpha=1.0)\" width=\"300\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run training (central, local, & FL)**\n",
    "\n",
    "You can change the FL job that's going to be simulated by changing the arguments of `run_sim_tap.py` script. You can choose which ESM2 model to download (8M or 650M parameters). The ESM2 finetuning arguments such as learning rate and others can be modified inside the script itself.\n",
    "\n",
    "First, let's check its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /bionemo_nvflare_examples/downstream/tap && python run_sim_tap.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Central training**\n",
    "\n",
    "To simulate central training, we use four clients, running one round of training for several steps on a different regression task using the full dataset. Note that if the `--exp_name` argument contains `\"central\"`, the combined training dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cd /bionemo_nvflare_examples/downstream/tap && python run_sim_tap.py --num_clients=4 --num_rounds=1 --local_steps=1000 --exp_name central"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Local training**\n",
    "\n",
    "To simulate local training, we use four clients, each running one round of training for several steps using the split datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /bionemo_nvflare_examples/downstream/tap && python run_sim_tap.py --num_clients=4 --num_rounds=1 --local_steps=1000 --exp_name local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Federated training with FedAvg**\n",
    "\n",
    "To simulate federated training, we use four clients, running several rounds with FedAvg, each with a smaller number of local steps. The number of rounds and local steps matches the setting of the local training scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /bionemo_nvflare_examples/downstream/tap && python run_sim_tap.py --num_clients=4 --num_rounds=10 --local_steps=100 --exp_name fedavg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the results in TensorBoard using `tensorboard --logdir /tmp/nvflare/bionemo/tap`. Note, that for the FedAvg, you can sort the x-axis by wall-time as each FL round is creating a new TensorBoard output folder.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> This public dataset is very small, and therefore, we only use it to illustrate the code example. The regression results are likely not reliable in practice. Hence, we skip the visualization here.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 2: Cross-compound task fitting\n",
    "\n",
    "#### Data: Predicting Antibody Developability from Sequence using Machine Learning\n",
    "See https://tdcommons.ai/single_pred_tasks/develop/#sabdab-chen-et-al\n",
    "- 2,409 Antibodies (both chains)\n",
    "\n",
    "#### Task Description: *Binary classification*. \n",
    "Given the antibody's heavy chain and light chain sequence, predict its developability. The input X is a list of two sequences where the first is the heavy chain and the second light chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to fix these paths to your own scripts\n",
    "! cd /bionemo_nvflare_examples/downstream/sabdab && python prepare_sabdab_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we are using the Dirichlet sampling strategy to generate heterogeneous data distributions among clients.\n",
    "Lower values of `alpha` generate higher levels of heterogeneity.\n",
    "\n",
    "|                                            Alpha 10.0                                             |                                            Alpha 1.0                                            |\n",
    "|:-------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------:|\n",
    "| <img src=\"./sabdab/figs/sabdab_alpha10.0.svg\" alt=\"Dirichlet sampling (alpha=10.0)\" width=\"150\"/> | <img src=\"./sabdab/figs/sabdab_alpha1.0.svg\" alt=\"Dirichlet sampling (alpha=1.0)\" width=\"150\"/> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Run training (central, local, & FL)**\n",
    "\n",
    "You can change the FL job that's going to be simulated by changing the arguments of `run_sim_sabdab.py` script. You can choose which ESM2 model to download (8M or 650M parameters). The ESM2 finetuning arguments such as learning rate and others can be modified inside the script itself.\n",
    "\n",
    "First, let's check its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /bionemo_nvflare_examples/downstream/sabdab && python run_sim_sabdab.py --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Central training**\n",
    "\n",
    "To simulate central training, we use one client, running one round of training for several steps. Note that if the `--exp_name` argument contains `\"central\"`, the combined training dataset is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /bionemo_nvflare_examples/downstream/sabdab && python run_sim_sabdab.py --num_clients=1 --num_rounds=1 --local_steps=3000 --exp_name central"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Local training**\n",
    "\n",
    "To simulate central training, we use six clients, each running one round of training for several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /bionemo_nvflare_examples/downstream/sabdab && python run_sim_sabdab.py --num_clients=6 --num_rounds=1 --local_steps=3000 --exp_name local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Federated training with FedAvg**\n",
    "\n",
    "To simulate federated training, we use six clients, running several rounds with FedAvg, each with a smaller number of local steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /bionemo_nvflare_examples/downstream/sabdab && python run_sim_sabdab.py --num_clients=6 --num_rounds=10 --local_steps=300 --exp_name fedavg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the results in TensorBoard using `tensorboard --logdir /tmp/nvflare/bionemo/sabdab`. Note that for the FedAvg, you can display a continuous training curve streamed to the server by selecting a `server` subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results with heterogeneous data sampling (alpha=1.0)\n",
    "| Setting  | Accuracy  |\n",
    "|:--------:|:---------:|\n",
    "|  Central |   *0.8504*   |\n",
    "|  Local   |   0.8099   |\n",
    "|   FedAvg | **0.8341** |\n",
    "\n",
    "\n",
    "|                                Central & Local                                 |                                    FedAvg                                     |\n",
    "|:-------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------:|\n",
    "| <img src=\"./sabdab/figs/tb_curve_sabdab_central_local.png\" alt=\"sabdab central and local training\" width=\"600\"/> | <img src=\"./sabdab/figs/tb_curve_sabdab_fedavg.png\" alt=\"sabdab FedAvg training\" width=\"600\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Task 3. Subcellular location prediction with ESM2nv 650M\n",
    "In this example, we use the `--encoder-frozen` option inside the `run_sim_scl.py` script. You can specify different base ESM2 models using the `--model` option.\n",
    "Follow the data download and preparation in [task_fitting.ipynb](../task_fitting/task_fitting.ipynb).\n",
    "\n",
    "Here, we use a heterogeneous sampling with `alpha=1.0`.\n",
    "\n",
    "<img src=\"./scl/figs/scl_alpha1.0.svg\" alt=\"Dirichlet sampling (alpha=10.0)\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Local training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this to work run the task_fitting notebook first in ../nvflare_with_bionemo/task_fitting/task_fitting.ipynb in order to download the SCL dataset\n",
    "!cd /bionemo_nvflare_examples/downstream/scl && python run_sim_scl.py --num_clients=3 --num_rounds=1 --local_steps=5000 --exp_name \"local\" --model \"650m\" --sim_gpus=\"0,1,2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Federated training with FedAvg**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd /bionemo_nvflare_examples/downstream/scl && python run_sim_scl.py --num_clients=3 --num_rounds=10 --local_steps=500 --exp_name \"fedavg\" --model \"650m\" --sim_gpus=\"0,1,2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the results in TensorBoard using `tensorboard --logdir /tmp/nvflare/bionemo/scl`. Note that for the FedAvg, you can display a continuous training curve streamed to the server by selecting a `server` subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results with heterogeneous data sampling (alpha=1.0)\n",
    "|  Client   | Site-1  | Site-2 | Site-3 | Average    |\n",
    "|:---------:|:-------:|:------:|:------:|:----------:|\n",
    "| # Samples |  1844   | 2921   | 2151   | Accuracy   |\n",
    "| Local     |  0.7819 |\t0.7885 | 0.7921 | 0.7875     |\n",
    "| FedAvg    |  0.8179 |\t0.8131 | 0.8209 | **0.8173** |\n",
    "\n",
    "<img src=\"./scl/figs/tb_curve_scl.png\" alt=\"SCL Training curve with Dirichlet sampling (alpha=1.0)\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
