{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5b0a0e2",
   "metadata": {},
   "source": [
    "# Federated Protein Embeddings and Task Model Fitting with BioNeMo\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> This notebook was tested on a single A1000 GPU and is compatible with BioNeMo Framework v2.4. To leverage additional or higher-performance GPUs, you can modify the configuration files and simulation script to accommodate multiple devices and increase thread utilization respectively. </div>\n",
    "\n",
    "This example notebook shows how to obtain protein learned representations in the form of embeddings using the ESM-1nv pre-trained model in a federated learning (FL) setting. The model is trained with NVIDIA's BioNeMo framework for Large Language Model training and inference. For more details, please visit NVIDIA BioNeMo Service at https://www.nvidia.com/en-us/gpu-cloud/bionemo.\n",
    "\n",
    "This example is based on NVIDIA BioNeMo Service [example](https://github.com/NVIDIA/BioNeMo/blob/main/examples/service/notebooks/task-fitting-predictor.ipynb) \n",
    "but runs inference locally (on the FL clients) instead of using BioNeMo's cloud API.\n",
    "\n",
    "This notebook will walk you through the task fitting workflow in the following sections:\n",
    "\n",
    "* Dataset sourcing & Data splitting\n",
    "* Federated embedding extraction\n",
    "* Training an MLP to predict subcellular location\n",
    "\n",
    "## Setup\n",
    "\n",
    "Ensure that you have read through the Getting Started section, can run the BioNeMo Framework docker container, and have configured the NGC Command Line Interface (CLI) within the container. It is assumed that this notebook is being executed from within the container.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output.  We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output.  Comment or delete this line in the cells below to restore full output.</div>\n",
    "\n",
    "### Import and install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc1b1f-42ab-4a10-959a-3d14163fa974",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "! pip install nvflare~=2.6\n",
    "! pip install biopython --no-dependencies\n",
    "\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import requests\n",
    "import split_data\n",
    "\n",
    "from Bio import SeqIO\n",
    "from nvflare import SimulatorRunner  \n",
    "from split_data import split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c2167",
   "metadata": {},
   "source": [
    "### Obtaining the protein embeddings using the BioNeMo ESM-1nv model\n",
    "Using BioNeMo, each FL client can obtain numerical vector representations of protein sequences called embeddings. Protein embeddings can then be used for visualization or making downstream predictions.\n",
    "\n",
    "Here we are interested in training a neural network to predict subcellular location from an embedding.\n",
    "\n",
    "The data we will be using comes from the paper [Light attention predicts protein location from the language of life](https://academic.oup.com/bioinformaticsadvances/article/1/1/vbab035/6432029) by StÃ¤rk et al. In this paper, the authors developed a machine learning algorithm to predict the subcellular location of proteins from sequence through protein langage models that are similar to those hosted by BioNeMo. Protein subcellular location refers to where the protein localizes in the cell, for example a protein may be expressed in the Nucleus or in the Cytoplasm. Knowing where proteins localize can provide insights into the underlying mechanisms of cellular processes and help identify potential targets for drug development. The following image includes a few examples of subcellular locations in an animal cell:\n",
    "\n",
    "\n",
    "(Image freely available at https://pixabay.com/images/id-48542)\n",
    "\n",
    "### Dataset sourcing\n",
    "For our target input sequences, we will point to FASTA sequences in a benchmark dataset called Fitness Landscape Inference for Proteins (FLIP). FLIP encompasses experimental data across adeno-associated virus stability for gene therapy, protein domain B1 stability and immunoglobulin binding, and thermostability from multiple protein families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8407b137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example protein dataset location\n",
    "fasta_url= \"http://data.bioembeddings.com/public/FLIP/fasta/scl/mixed_soft.fasta\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe63c2",
   "metadata": {},
   "source": [
    "First, we define the source of example protein dataset with the FASTA sequences. This data follows the [biotrainer](https://github.com/sacdallago/biotrainer/blob/main/docs/data_standardization.md) standard, so it includes information about the class in the FASTA header, and the protein sequence. Here are two example sequences in this file:\n",
    "\n",
    "```\n",
    ">Sequence1 TARGET=Cell_membrane SET=train VALIDATION=False\n",
    "MMKTLSSGNCTLNVPAKNSYRMVVLGASRVGKSSIVSRFLNGRFEDQYTPTIEDFHRKVYNIHGDMYQLDILDTSGNHPFPAM\n",
    "RRLSILTGDVFILVFSLDSRESFDEVKRLQKQILEVKSCLKNKTKEAAELPMVICGNKNDHSELCRQVPAMEAELLVSGDENC\n",
    "AYFEVSAKKNTNVNEMFYVLFSMAKLPHEMSPALHHKISVQYGDAFHPRPFCMRRTKVAGAYGMVSPFARRPSVNSDLKYIKA\n",
    "KVLREGQARERDKCSIQ\n",
    ">Sequence4833 TARGET=Nucleus SET=train VALIDATION=False\n",
    "MARTKQTARKSTGGKAPRKQLATKAARKSAPATGGVKKPHRFRPGTVALREIRKYQKSTELLIRKLPFQRLVREIAQDFKTDL\n",
    "RFQSSAVAALQEAAEAYLVGLFEDTNLCAIHAKRVTIMPKDIQLARRIRGERA\n",
    "Note the following attributes in the FASTA header:\n",
    "```\n",
    "\n",
    "* `TARGET` attribute holds the subcellular location classification for the sequence, for instance Cell_membrane and Nucleus. This dataset includes a total of ten subcellelular location classes -- more on that below.\n",
    "* `SET` attribute defines whether the sequence should be used for training (train) or testing (test)\n",
    "* `VALIDATION` attribute defines whether the sequence should be used for validation (all sequences where this is True are also in set=train)\n",
    "\n",
    "### Downloading the protein sequences and subcellular location annotations\n",
    "In this step we download the FASTA file defined above and parse the sequences into a list of BioPython SeqRecord objects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7bfca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download the FASTA file from FLIP: https://github.com/J-SNACKKB/FLIP/tree/main/splits/scl\n",
    "fasta_content = requests.get(fasta_url, headers={\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x86)'\n",
    "}).content.decode('utf-8')\n",
    "fasta_stream = io.StringIO(fasta_content)\n",
    "\n",
    "# Obtain a list of SeqRecords/proteins which contain sequence and attributes\n",
    "# from the FASTA header\n",
    "proteins = list(SeqIO.parse(fasta_stream, \"fasta\"))\n",
    "print(f\"Downloaded {len(proteins)} sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e480628f",
   "metadata": {},
   "source": [
    "### Download Model Checkpoints\n",
    "\n",
    "The following code will download the pre-trained model, `esm2n/650m:2.0`, from the NGC registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4916a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bionemo.core.data.load import load\n",
    "\n",
    "checkpoint_path = load(\"esm2/8m:2.0\")\n",
    "print(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd955150",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "Next, we prepare the data for simulating federated learning using `n_clients`. Note that a copy of the same test set is shared between the clients in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fbc811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clients = 3\n",
    "# limiting to the proteins with sequence length<512 for embedding queries\n",
    "MAX_SEQUENCE_LEN = 512\n",
    "seed=0\n",
    "data_root = \"/tmp/data/mixed_soft\"\n",
    "split_alpha = 1.0  # moderate label heterogeneity of alpha=1.0\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Extract meta data and split\n",
    "data = []\n",
    "for i, x in enumerate(proteins):\n",
    "        if len(str(x.seq)) > MAX_SEQUENCE_LEN:\n",
    "            continue\n",
    "            \n",
    "        entry = {key: value for key, value in re.findall(r\"([A-Z_]+)=(-?[A-z0-9]+[.0-9]*)\", x.description)}\n",
    "        entry[\"sequences\"] = str(x.seq)\n",
    "        entry[\"id\"] = str(i)\n",
    "        entry[\"labels\"] = entry[\"TARGET\"]\n",
    "       \n",
    "        data.append(entry)\n",
    "print(f\"Read {len(data)} valid sequences.\")\n",
    "               \n",
    "# Split the data and save for each client\n",
    "# Note, test_data is kept the same on each client and is not split\n",
    "# `concat=False` is used for SCL experiments (see ../downstream/scl)\n",
    "split(proteins=data, num_sites=n_clients, split_dir=data_root, alpha=split_alpha, concat=False)  \n",
    "# `concat=True` is used for separate inference + MLP classifier in this notebook\n",
    "split(proteins=data, num_sites=n_clients, split_dir=data_root, alpha=split_alpha, concat=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc9110",
   "metadata": {},
   "source": [
    "### Federated embedding extraction\n",
    "Here, we run inference on each client sequentially using one thread to preserve GPU memory. Running inference of the ESM-1nv model to extract embeddings with a micro-batch size of 64 takes about 30 GB GPU memory. You can reduce the micro-batch size if you have less GPU memory available. For more inference requirements, see the [model overview](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara/models/esm2nv650m). Note that this can take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8073e435",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_path = \"/tmp/data/mixed_soft/results\"\n",
    "\n",
    "micro_bs = 64\n",
    "\n",
    "if not os.path.isdir(results_path):\n",
    "    os.makedirs(results_path)\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src\"))\n",
    "\n",
    "from bionemo_inference import BioNeMoInference\n",
    "from bionemo_launcher import BioNeMoLauncher\n",
    "\n",
    "from nvflare.app_opt.pt.job_config.base_fed_job import BaseFedJob\n",
    "from nvflare.app_common.launchers.subprocess_launcher import SubprocessLauncher\n",
    "\n",
    "# Create BaseFedJob\n",
    "job = BaseFedJob(\n",
    "  name=\"esm2_embeddings\"\n",
    ")\n",
    "\n",
    "# Define the controller and send to server\n",
    "controller = BioNeMoInference(\n",
    "    min_responses_required=n_clients\n",
    ")\n",
    "job.to_server(controller)\n",
    "\n",
    "# Add clients\n",
    "for i in range(n_clients):\n",
    "    # build inference command for each client\n",
    "    client_name = f\"site-{i+1}\"\n",
    "    command = f\"infer_esm2 --checkpoint-path {checkpoint_path} --data-path {data_root}/data_{client_name}.csv --results-path {results_path}/inference_results_{client_name} --precision bf16-mixed --include-embeddings --include-logits --include-input-ids --micro-batch-size {micro_bs} --num-gpus 1\"\n",
    "    \n",
    "    launcher = BioNeMoLauncher(\n",
    "        launcher_id=job.as_id(SubprocessLauncher(script=command, launch_once=True))\n",
    "    )\n",
    "    job.to(launcher, client_name)\n",
    "\n",
    "job.export_job(f\"./exported_jobs/embeddings\")  # optional export of job configurations. Can be used to submit the job to a real-world NVFlare deployment.\n",
    "job.simulator_run(\"/tmp/nvflare/bionemo/embeddings\", gpu=\"0\", threads=1)  # due to memory constraints, we run the client's inference sequentially in one thread and gpu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6448e76d",
   "metadata": {},
   "source": [
    "### Inspecting the embeddings and labels\n",
    "Embeddings returned from the BioNeMo model are vectors of fixed size for each input sequence. In other words, if we input 10 sequences, we will obtain a matrix `10xD`, where `D` is the size of the embedding (in the case of ESM2-8m, `D=320` & for ESM2-650m, `D=1280`). At a glance, these real-valued vector embeddings don't show any obvious features (see the printout in the next cell). But these vectors do contain information that can be used in downstream models to reveal properties of the protein, for example the subcellular location as we'll explore below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings from site-1\n",
    "import torch\n",
    "\n",
    "client_name = \"site-1\"\n",
    "results = torch.load(f\"{results_path}/inference_results_{client_name}/predictions__rank_0.pt\")\n",
    "\n",
    "for key, val in results.items():\n",
    "    if val is not None:\n",
    "        print(f'{key}\\t{val.shape}')\n",
    "\n",
    "protein_embeddings = results['embeddings']\n",
    "print(f\"Loaded {len(protein_embeddings)} embeddings from site-1.\")\n",
    "\n",
    "for i in range(4):\n",
    "    x = protein_embeddings[i]\n",
    "    print(f\"{i+1}. embedding: range {x.min():.2f}-{x.max():.2f}, mean={x.mean():.2f}, shape={x.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1bb7dc",
   "metadata": {},
   "source": [
    "Let's enumerate the labels corresponding to potential subcellular locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also print all the labels\n",
    "\n",
    "labels = set([entry['labels'] for entry in data])\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"{i+1}. {label.replace('_', ' ')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9eefb",
   "metadata": {},
   "source": [
    "### Training a MLP to predict subcellular location\n",
    "To be able to classify proteins for their subcellular location, we train a simple scikit-learn Multi-layer Perceptron (MPL) classifier using Federated Averaging ([FedAvg](https://arxiv.org/abs/1602.05629)). The MLP model uses a network of hidden layers to fit the input embedding vectors to the model classes (the cellular locations above). In the simulation below, we define the MLP to use the Adam optimizer with a network of (512, 256, 128) hidden layers, defining a random state (or seed) for reproducibility, and trained for 30 rounds of FedAvg (see [config_fed_server.json](./jobs/fedavg/app/config/config_fed_server.json)). \n",
    "\n",
    "We can use the same configuration also to simulate local training where each client is only training with their own data by setting `os.environ[\"SIM_LOCAL\"] = \"True\"`. Our [BioNeMoMLPLearner](./jobs/fedavg/app/custom/bionemo_mlp_learner.py) will then ignore the global weights coming from the server.\n",
    "\n",
    "### Local training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72a9dad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SIM_LOCAL\"] = \"True\"\n",
    "\n",
    "from bionemo_mlp_learner import BioNeMoMLPLearner\n",
    "from bionemo_mlp_job import BioNeMoMLPJob\n",
    "\n",
    "from nvflare.app_common.workflows.fedavg import FedAvg\n",
    "from nvflare.app_common.executors.model_learner_executor import ModelLearnerExecutor\n",
    "\n",
    "embedding_dimensions=320  # embedding dimensions of ESM2-8m\n",
    "\n",
    "# Create BioNeMoMLPJob which uses a custom model persistor for the sklearn MLP model\n",
    "job = BioNeMoMLPJob(\n",
    "  name=\"mlp_local\",\n",
    "  embedding_dimensions=embedding_dimensions\n",
    ")\n",
    "\n",
    "# Define the controller and send it to server\n",
    "controller = FedAvg(\n",
    "    num_clients=n_clients,\n",
    "    num_rounds=100,\n",
    ")\n",
    "job.to_server(controller)\n",
    "\n",
    "# Add clients\n",
    "for i in range(n_clients):\n",
    "    client_name = f\"site-{i+1}\"\n",
    "    executor = ModelLearnerExecutor(learner_id=job.as_id(\n",
    "            BioNeMoMLPLearner(\n",
    "                data_path=os.path.join(data_root, f\"data_{client_name}.csv\"),\n",
    "                inference_result=f\"{results_path}/inference_results_{client_name}/predictions__rank_0.pt\",\n",
    "                aggregation_epochs=4,\n",
    "                lr=1e-5,\n",
    "                embedding_dimensions=embedding_dimensions\n",
    "            )\n",
    "    ))\n",
    "    job.to(executor, client_name)    \n",
    "\n",
    "job.export_job(f\"./exported_jobs/local_alpha{split_alpha}\")  # optional export of job configurations. Can be used to submit the job to a real-world NVFlare deployment.\n",
    "job.simulator_run(f\"/tmp/nvflare/bionemo/local_alpha{split_alpha}\", threads=n_clients)  # As the MLP is very lightweight, we can run all clients in parallel on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58f1f5",
   "metadata": {},
   "source": [
    "### Federated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67275cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"SIM_LOCAL\"] = \"False\"\n",
    "\n",
    "from bionemo_mlp_learner import BioNeMoMLPLearner\n",
    "from bionemo_mlp_job import BioNeMoMLPJob\n",
    "\n",
    "from nvflare.app_common.workflows.fedavg import FedAvg\n",
    "from nvflare.app_common.executors.model_learner_executor import ModelLearnerExecutor\n",
    "\n",
    "embedding_dimensions=320  # embedding dimensions of ESM2-8m\n",
    "\n",
    "# Create BioNeMoMLPJob which uses a custom model persistor for the sklearn MLP model\n",
    "job = BioNeMoMLPJob(\n",
    "  name=\"mlp_fedavg\",\n",
    "  embedding_dimensions=embedding_dimensions\n",
    ")\n",
    "\n",
    "# Define the controller and send it to server\n",
    "controller = FedAvg(\n",
    "    num_clients=n_clients,\n",
    "    num_rounds=100,\n",
    ")\n",
    "job.to_server(controller)\n",
    "\n",
    "# Add clients\n",
    "for i in range(n_clients):\n",
    "    client_name = f\"site-{i+1}\"\n",
    "    executor = ModelLearnerExecutor(learner_id=job.as_id(\n",
    "            BioNeMoMLPLearner(\n",
    "                data_path=os.path.join(data_root, f\"data_{client_name}.csv\"),\n",
    "                inference_result=f\"{results_path}/inference_results_{client_name}/predictions__rank_0.pt\",\n",
    "                aggregation_epochs=4,\n",
    "                lr=1e-5,\n",
    "                embedding_dimensions=embedding_dimensions\n",
    "            )\n",
    "    ))\n",
    "    job.to(executor, client_name)    \n",
    "\n",
    "job.export_job(f\"./exported_jobs/fedavg_alpha{split_alpha}\")  # optional export of job configurations. Can be used to submit the job to a real-world NVFlare deployment.\n",
    "job.simulator_run(f\"/tmp/nvflare/bionemo/fedavg_alpha{split_alpha}\", threads=n_clients)  # As the MLP is very lightweight, we can run all clients in parallel on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d6304",
   "metadata": {},
   "source": [
    "### TensorBoard Visualization\n",
    "You can visualize the training progress using TensorBoard\n",
    "```\n",
    "tensorboard --logdir /tmp/nvflare/bionemo\n",
    "```\n",
    "\n",
    "Note, as the test set is shared between the clients in this example, we will see the same performance metrics for the global model computed on each client.\n",
    "\n",
    "An example of local (green, blue, red) vs. federated (orange) training is shown below using the ESM2-650m model for inference.\n",
    "\n",
    "![TensorBoard training curves](tb_curve.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0cb26-cf05-4ff1-833f-65126c17098e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
