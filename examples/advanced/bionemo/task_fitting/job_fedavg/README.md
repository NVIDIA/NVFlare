# Federated MLP Training on Protein Embeddings

This example demonstrates how to use NVIDIA FLARE to train a PyTorch Multi-Layer Perceptron (MLP) classifier on protein embeddings using federated averaging (FedAvg). The MLP predicts subcellular protein location from embeddings generated by the ESM2 model.

## Overview

This job trains a PyTorch-based MLP classifier in a federated learning setting. Each client:
1. Loads pre-computed protein embeddings from ESM2 inference
2. Trains a shared MLP model on their local embeddings
3. Sends model updates to the server for aggregation

Supports both federated and local training modes for comparison.

## Code Structure

```bash
job_fedavg/
├── model.py     # PyTorch MLP model definition
├── client.py    # Client script for PyTorch training
├── job.py       # Job configuration using FedAvgRecipe
└── README.md    # This file
```

## Prerequisites

Install required packages:

```bash
pip install nvflare torch pandas
```

## Data Preparation

Before running this job, you must first run the inference job to generate embeddings:

```bash
cd ../job_inference
python job.py
```

This will create embedding files in `/tmp/data/mixed_soft/results/` for each client.

## Configuration

Key parameters in `job.py`:
- `n_clients`: Number of federated clients (default: 3)
- `num_rounds`: Number of federated training rounds (default: 50)
- `aggregation_epochs`: Local epochs per round (default: 20)
- `lr`: Learning rate (default: 1e-5)
- `batch_size`: Training batch size (default: 128)
- `embedding_dimensions`: ESM2 embedding size (320 for 8m, 1280 for 650m)

Training modes:
- **Federated** (`SIM_LOCAL=False`): Standard FedAvg with model aggregation
- **Local** (`SIM_LOCAL=True`): Each client trains independently (for comparison)

## Client Code

The client code ([client.py](./client.py)) implements the PyTorch training workflow:

1. Initialize NVFlare client API with `flare.init()`
2. Load pre-computed embeddings and labels
3. Create PyTorch DataLoaders and initialize MLP model
4. Training loop: receive global model, train locally, send updates

## Model Architecture

PyTorch MLP with:
- **Input**: 1280-dim embeddings (ESM2-650m) or 320-dim (ESM2-8m)
- **Hidden layers**: 512 → 256 → 128 (with ReLU)
- **Output**: 10 classes (subcellular locations)

## Job Recipe

The job uses the PyTorch `FedAvgRecipe`:

```python
from nvflare.app_opt.pt.recipes.fedavg import FedAvgRecipe
from model import ProteinMLP

recipe = FedAvgRecipe(
    name=job_name,
    min_clients=n_clients,
    num_rounds=50,
    initial_model=ProteinMLP(input_dim=1280, num_classes=10),
    train_script="client.py",
    train_args=script_args,
)
```

## Run Job

From the `job_fedavg` directory:

```bash
python job.py
# Select 'fedavg' for federated or 'local' for local training
```

The job will:
1. Initialize a PyTorch MLP model with reproducible weights
2. Train for 50 rounds of federated averaging
3. Log metrics to TensorBoard
4. Save results to `/tmp/nvflare/bionemo/{job_name}_alpha1.0/`

## View Results

Monitor training progress with TensorBoard:

```bash
tensorboard --logdir /tmp/nvflare/bionemo/
```

Metrics tracked: `train_accuracy`, `test_accuracy`, `test_loss`

## Output

The training produces:
- Global model checkpoints at each round
- TensorBoard logs with accuracy and loss metrics
- Final trained model for subcellular location prediction

## Subcellular Location Classes

The model predicts one of 10 locations:
Cell_membrane, Cytoplasm, Endoplasmic_reticulum, Extracellular, Golgi_apparatus, Lysosome, Mitochondrion, Nucleus, Peroxisome, Plastid

## Notes

- This job runs clients **in parallel** (threads=n_clients)
- GPU acceleration is used if available on the client machines
- Each client uses a shared test set for comparable metrics
- The `SIM_LOCAL` environment variable controls training mode
- Full model weights are sent using PyTorch state_dict
