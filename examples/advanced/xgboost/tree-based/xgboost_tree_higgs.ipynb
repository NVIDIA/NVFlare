{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cada310b-e776-4b9a-aabe-f111c31efcc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Tree-based Federated Learning for XGBoost on HIGGS Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653cbf2-92f2-4a22-8317-69cfb0266e92",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Tree-based Collaboration\n",
    "Under tree-based collaboration, individual trees are independently trained on each client's local data without aggregating the global sample gradient histogram information.\n",
    "Trained trees are collected and passed to the server / other clients for aggregation. \n",
    "\n",
    "### Bagging Aggregation\n",
    "\n",
    "\"Bagging XGBoost\" is one way of performing tree-based federated boosting with multiple sites: at each round of tree boosting, all sites start from the same \"global model\", and boost a number of trees (in current example, 1 tree) based on their local data. The resulting trees are then send to server. A bagging aggregation scheme is applied to all the submitted trees to update the global model, which is further distributed to all clients for next round's boosting. \n",
    "\n",
    "This scheme bears certain similarity to the [Random Forest mode](https://xgboost.readthedocs.io/en/stable/tutorials/rf.html) of XGBoost, where a `num_parallel_tree` is boosted based on random row/col splits, rather than a single tree. Under federated learning setting, such split is fixed to clients rather than random and without column subsampling. \n",
    "\n",
    "In addition to basic uniform shrinkage setting where all clients have the same learning rate, based on our research, we enabled scaled shrinkage across clients for weighted aggregation according to each client's data size, which is shown to significantly improve the model's performance on non-uniform quantity splits over HIGGS data.\n",
    "\n",
    "Below we listed steps to run this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0292c-78b6-4bde-96d6-699dae996173",
   "metadata": {},
   "source": [
    "## 1. Setup NVFLARE\n",
    "\n",
    "Follow the [Getting_Started](https://nvflare.readthedocs.io/en/main/getting_started.html) to setup virtual environment and install NVFLARE\n",
    "\n",
    "We also provide a [Notebook](../../nvflare_setup.ipynb) for this setup process. \n",
    "\n",
    "Assume you have already setup the venv, lets first install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4130b15-09e6-456f-a3c7-87c8ee9e07f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d872d8a-9e44-49dd-94b1-7862b3815ffe",
   "metadata": {},
   "source": [
    "## 2. Data and job configs preparation \n",
    "Follow this [Notebook](../data_job_setup.ipynb) for this setup process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0713e2-e393-41c0-9da0-392535cf8a54",
   "metadata": {},
   "source": [
    "## 3. Run simulated xgboost experiment\n",
    "Now that we have the data and job configs ready, we run the experiment using Simulator.\n",
    "\n",
    "Here we simulate 5 clients under uniform data split.\n",
    "\n",
    "**warning: We suggest you only run this notebook when your machine has more than 25 GB RAM. Otherwise, please try a smaller dataset other than HIGGS.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde19b89-3aef-49ac-99d9-800d65829830",
   "metadata": {},
   "source": [
    "Simulator can either be used with CLI command (please run the CLI command in a terminal):\n",
    "\n",
    "```shell\n",
    "nvflare simulator ./jobs/higgs_5_bagging_uniform_split_uniform_lr -w /tmp/nvflare/workspaces/xgboost_workspace_5_bagging_uniform_split_uniform_lr -n 5 -t 5\n",
    "```\n",
    "\n",
    "or via Simulator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c75dcb-014d-40c4-8a4a-7a53847c486b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nvflare.private.fed.app.simulator.simulator_runner import SimulatorRunner  \n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"./jobs/higgs_5_bagging_uniform_split_uniform_lr\",\n",
    "    workspace=\"/tmp/nvflare/workspaces/xgboost_workspace_5_bagging_uniform_split_uniform_lr\",\n",
    "    n_clients=5,\n",
    "    threads=5\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e9ee2-e993-442d-a525-d2baf92af539",
   "metadata": {},
   "source": [
    "## 4. Result visualization\n",
    "Model accuracy can be visualized in tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6814434-4e6d-4460-b480-709cb3e77cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/nvflare/workspaces/xgboost_workspace_5_bagging_uniform_split_uniform_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef36a5-acc5-47a7-bac1-f94eae57a590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
