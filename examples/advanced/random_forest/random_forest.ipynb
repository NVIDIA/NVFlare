{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cada310b-e776-4b9a-aabe-f111c31efcc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Federated Random Forest on HIGGS Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653cbf2-92f2-4a22-8317-69cfb0266e92",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Libraries\n",
    "This example show how to use [NVIDIA FLARE](https://nvflare.readthedocs.io/en/main/index.html) on tabular data applications.\n",
    "It illustrates the [Random Forest](https://xgboost.readthedocs.io/en/stable/tutorials/rf.html) functionality using [XGBoost](https://github.com/dmlc/xgboost) library,\n",
    "which is an optimized distributed gradient boosting library, also covering random forest. In this example, we illustrate the use of NVFlare to carry out *horizontal* federated learning with tree-based collaboration - forming a random forest.\n",
    "\n",
    "### Dataset\n",
    "This example illustrate a binary classification task based on [HIGGS dataset](https://mlphysics.ics.uci.edu/data/higgs/).\n",
    "This dataset contains 11 million instances, each with 28 attributes.\n",
    "\n",
    "### Horizontal Federated Learning\n",
    "Under horizontal setting, each participant / client joining the federated learning will have part of the whole data / instances / examples/ records, while each instance has all the features.\n",
    "This is in contrast to vertical federated learning, where each client has part of the feature values for each instance.\n",
    "\n",
    "### Tree-based Collaboration\n",
    "Under tree-based collaboration, individual trees are independently trained on each client's local data without aggregating the global sample gradient histogram information.\n",
    "Trained trees are collected and passed to the server / other clients for aggregation. Note that under Random Forest setting, only one round of training will be performed.\n",
    "\n",
    "### Local Training and Aggregation\n",
    "Random forest training with multiple clients can be achieved in two steps:\n",
    "\n",
    "- Local training: each site train a local sub-forest consisting of a number of trees based on their local data by utilizing the `subsample` and `num_parallel_tree` functionalities from XGBoost. \n",
    "- Global aggregation: server collects all sub-forests from clients, and a bagging aggregation scheme is applied to generate the global forest model.\n",
    "\n",
    "No further training will be performed, `num_boost_round` should be 1 to align with the basic setting of random forest.\n",
    "\n",
    "Below we listed steps to run this example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a0292c-78b6-4bde-96d6-699dae996173",
   "metadata": {},
   "source": [
    "## 1. Setup NVFLARE\n",
    "\n",
    "Follow the [Getting_Started](https://nvflare.readthedocs.io/en/main/getting_started.html) to setup virtual environment and install NVFLARE\n",
    "\n",
    "We also provide a [Notebook](../../nvflare_setup.ipynb) for this setup process. \n",
    "\n",
    "Assume you have already setup the venv, lets first install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4130b15-09e6-456f-a3c7-87c8ee9e07f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d872d8a-9e44-49dd-94b1-7862b3815ffe",
   "metadata": {},
   "source": [
    "## 2. Data preparation \n",
    "\n",
    "### Download and Store Data\n",
    "To run the examples, we first download the dataset from the HIGGS link above.\n",
    "By default, we assume the dataset is downloaded, uncompressed, and stored in `DATASET_ROOT/HIGGS.csv`.\n",
    "Please note that the UCI's website may experience occasional downtime.\n",
    "\n",
    "### Generate Data Split\n",
    "Since HIGGS dataset is already randomly recorded,\n",
    "data split will be specified by the continuous index ranges for each client,\n",
    "rather than a vector of random instance indices.\n",
    "In this example, we choose uniform data split with 5 clients.\n",
    "\n",
    "**Please change the DATASET_ROOT to the correct local path.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f906a1c9-dce0-476c-be65-79ebd8ad5da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# please change this DATASET_ROOT to the correct path containing HIGGS dataset\n",
    "%env DATASET_ROOT=/data\n",
    "!python3 utils/prepare_data_split.py \\\n",
    "        --data_path \"${DATASET_ROOT}/HIGGS.csv\" \\\n",
    "        --site_num 5 \\\n",
    "        --size_total 11000000 \\\n",
    "        --size_valid 1000000 \\\n",
    "        --split_method uniform \\\n",
    "        --out_path \"/tmp/nvflare/random_forest/HIGGS/data_splits/5_uniform\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af257e69-2bb7-49b6-ac6c-f007b0e6618e",
   "metadata": {},
   "source": [
    "## 3. Prepare job configs\n",
    "We are using NVFlare's FL simulator to run the following experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8807d89-49f5-4fd0-bf8d-b9fe01047796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env DATA_SPLIT_ROOT=/tmp/nvflare/random_forest/HIGGS/data_splits\n",
    "!python3 utils/prepare_job_config.py \\\n",
    "        --site_num 5 \\\n",
    "        --num_local_parallel_tree 20 \\\n",
    "        --local_subsample 0.05 \\\n",
    "        --split_method uniform \\\n",
    "        --lr_mode uniform \\\n",
    "        --nthread 16 \\\n",
    "        --tree_method \"hist\" \\\n",
    "        --data_split_root \"${DATA_SPLIT_ROOT}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0713e2-e393-41c0-9da0-392535cf8a54",
   "metadata": {},
   "source": [
    "## 4. Run simulated random forest experiment\n",
    "Now that we have the job configs ready, we run the experiment using Simulator.\n",
    "\n",
    "**warning: We suggest you only run this notebook when your machine has more than 25 GB RAM. Otherwise, please try a smaller dataset other than HIGGS.**\n",
    "\n",
    "Simulator can either be used with CLI command (please run CLI commands in terminal):\n",
    "\n",
    "```shell\n",
    "nvflare simulator \"./jobs/higgs_5_0.05_uniform_split_uniform_lr\" -w \"/tmp/nvflare/random_forest/workspace_5_0.05_uniform_split_uniform_lr\" -n 5 -t 5\n",
    "```\n",
    "\n",
    "or via Simulator API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c75dcb-014d-40c4-8a4a-7a53847c486b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nvflare.private.fed.app.simulator.simulator_runner import SimulatorRunner  \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"./jobs/higgs_5_0.05_uniform_split_uniform_lr\",\n",
    "    workspace=\"/tmp/nvflare/random_forest/workspace_5_0.05_uniform_split_uniform_lr\",\n",
    "    n_clients=5,\n",
    "    threads=5\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e9ee2-e993-442d-a525-d2baf92af539",
   "metadata": {},
   "source": [
    "## 5. Validate the trained model\n",
    "The trained global random forest model can further be validated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6814434-4e6d-4460-b480-709cb3e77cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 utils/model_validation.py \\\n",
    "        --data_path \"${DATASET_ROOT}/HIGGS.csv\" \\\n",
    "        --model_path \"/tmp/nvflare/random_forest/workspace_5_0.05_uniform_split_uniform_lr/server/simulate_job/app_server/xgboost_model.json\" \\\n",
    "        --size_valid 1000000 --num_trees 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34027e0b-8bb0-4787-b6f6-b08a860b7e1d",
   "metadata": {},
   "source": [
    "The expected AUC is 0.7810306437097397"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
