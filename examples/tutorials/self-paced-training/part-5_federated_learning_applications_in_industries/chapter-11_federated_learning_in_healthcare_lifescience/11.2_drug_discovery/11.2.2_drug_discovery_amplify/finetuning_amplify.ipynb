{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65303e3a",
   "metadata": {},
   "source": [
    "# Federated Fine-tuning of an AMPLIFY Model\n",
    "\n",
    "This example demonstrates how to use the AMPLIFY protein language model from [chandar-lab/AMPLIFY](https://github.com/chandar-lab/AMPLIFY) for fine-tuning on multiple downstream tasks. AMPLIFY is a powerful protein language model that can be adapted for various protein-related tasks. In this example, we'll show how to fine-tune AMPLIFY for the prediction of several protein properties using antibody sequence data. For more details, please refer this [paper](https://www.biorxiv.org/content/10.1101/2024.09.23.614603v1).\n",
    "\n",
    "Note, this script assumes a regular Python environment and doesn't rely on running Docker as in the previous BioNemo example. For running AMPLIFY within the BioNeMo Framework, please see [here](https://docs.nvidia.com/bionemo-framework/latest/models/amplify/).\n",
    "\n",
    "**We recommend creating a fresh virtual environment before starting this notebook and installing the dependencies to avoid conflicts.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdca8fce",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "First, let's download the data and install the required dependencies.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "Before running the data preparation script, you need to clone the FLAb repository (at commit from Jan 13, 2025) to obtain the required data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b440836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/Graylab/FLAb.git\n",
    "%cd FLAb\n",
    "!git reset --hard 22f412bcee39e2bae3a51dc9f3c26265a2192542\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80187909",
   "metadata": {},
   "source": [
    "The FLAb repository contains experimental data for six properties of therapeutic antibodies: Expression, thermostability, immunogenicity, aggregation, polyreactivity, and binding affinity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1833ad2",
   "metadata": {},
   "source": [
    "First, we clone the AMPLIFY code and install it as a local pip package following the instructions [here](https://github.com/chandar-lab/AMPLIFY?tab=readme-ov-file#installation-as-a-local-pip-package). \n",
    "\n",
    "Note, we recommend creating a new virtual enviornment to run this JupyterLab Python kernel before installing the dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f645f8ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/chandar-lab/AMPLIFY\n",
    "!pip install --upgrade pip\n",
    "!pip install --editable AMPLIFY[dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177cf5c9",
   "metadata": {},
   "source": [
    "Furthermore, we install the required dependencies for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9b878",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd184c",
   "metadata": {},
   "source": [
    "## Federated Multi-task Fine-tuning\n",
    "\n",
    "In this scenario, each client trains a different downstream task from the [FLAb](https://github.com/Graylab/FLAb.git) antibody fitness datasets using a custom regression head. At the same time, they jointly fine-tune the AMPLIFY pretrained model trunk to benefit from each other using **Federated Learning (FL)**.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; margin: 20px 0;\">\n",
    "<img src=\"./figs/amplify_multi_task.svg\" alt=\"AMPLIFY model for multi-task fine-tuning\" style=\"width: 400px;\"/>\n",
    "</div>\n",
    "\n",
    "The process involves:\n",
    "1. Obtaining antibody sequence data from [FLAb](https://github.com/Graylab/FLAb.git)\n",
    "2. Preparing the data for fine-tuning combining \"light\" and \"heavy\" antibody sequences with a \"|\" separator and splitting the data into clients.\n",
    "3. Fine-tuning the AMPLIFY model for binding affinity prediction in two scenarios:\n",
    "    - Local training: Each data owner/client trains only on their local data.\n",
    "    - Federated learning: We use the federated averaging algorithm to jointly train a global model on all the clients' data.\n",
    "\n",
    "To allow clients to keep their regressor model local, we simply add a NVFlare [filter](https://nvflare.readthedocs.io/en/main/programming_guide/filters.html#filters) that removes the local regression layers before returning the updated AMPLIFY trunk to the server for aggregation. See the [run_fl_multitask.py](run_fl_multitask.py) where we add the [ExcludeParamsFilter](src/filters.py) filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1318a08",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "The [combine_data.py](src/combine_data.py) script is used to prepare data for sequence classification. It processes CSV files containing 'heavy' and 'light' feature columns, combines them, and splits the data into training and test sets for each task.\n",
    "\n",
    "**Combine the CSV Datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d6995c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for task in [\"aggregation\", \"binding\", \"expression\", \"immunogenicity\", \"polyreactivity\", \"tm\"]:\n",
    "    print(\"Combing $task CSV data\")\n",
    "    !python src/combine_data.py --input_dir ./FLAb/data/$task --output_dir ./FLAb/data_fl/$task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78895bad",
   "metadata": {},
   "source": [
    "This will:\n",
    "1. Read all CSV files from the `data` directory for each of the six antibody properties (aggregation, binding, expression, immunogenicity, polyreactivity, and thermostability)\n",
    "2. Combine the 'heavy' and 'light' columns with a '|' separator into a 'combined' column\n",
    "3. Split the data into training (80%) and test (20%) sets\n",
    "5. Save the processed data to the specified output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326be7f",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "The following experiments use the [120M AMPLIFY](https://huggingface.co/chandar-lab/AMPLIFY_120M) pretrained model from HuggingFace. This tutorial set up for running on one NVIDIA A100 GPU with 80 GB memory.\n",
    "For full training, we recommoned using three A100 GPUs. Then, with the 120M AMPLIFY model, we can run two clients on each GPU as specified by the ``--sim_gpus`` argument to `run_fl_*.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a359f7",
   "metadata": {},
   "source": [
    "#### Local Training\n",
    "First we run the local training. Here, each data owner/client trains only on their local data. As we only run 1 round, the clients will never get the benefit of the updated global model and can only learn from their own data.\n",
    "\n",
    "This command will:\n",
    "1. Run federated learning with 6 clients (one for each task)\n",
    "2. Perform one round of training with NVFlare\n",
    "3. Each client will train for 10 local epochs per round\n",
    "4. Use the 120M parameter AMPLIFY model by default\n",
    "5. Configure the regression MLP with layer sizes [128, 64, 32]\n",
    "\n",
    "Note, you can monitor the training progress with TensorBoard by running `tensorboard --logdir /tmp/nvflare/AMPLIFY` in a separate terminal.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE:</b> To speed up the results, we only run for a few local epochs. However, can see the resulting plots below when running for `local_epochs=600` and `num_rounds=600` in the local and federated, respectively. For running this notebook, we assume one GPU but reduce the batch size so that the six clients can fit into memory. For full training, we recommend using at least three A100 GPUs and distribute the six clients among them using the `--sim_gpus` option as shown in the comment using a batch size of 64 by changing the default value in `finetune_seqclassification_fl.py`.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d77d7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python run_fl_multitask.py \\\n",
    "    --num_rounds 1 \\\n",
    "    --local_epochs 3 \\\n",
    "    --pretrained_model \"chandar-lab/AMPLIFY_120M\" \\\n",
    "    --layer_sizes \"128,64,32\" \\\n",
    "    --exp_name \"local_singletask\" \\\n",
    "    --sim_gpus \"0\" # \"0,1,2,0,1,2\" for full training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a54e1d",
   "metadata": {},
   "source": [
    "### Federated Learning\n",
    "Next, we run the same data setting but using the federated averaging ([FedAvg](https://arxiv.org/abs/1602.05629)) algorithm.\n",
    "\n",
    "This command will:\n",
    "1. Run federated learning with 6 clients (one for each task)\n",
    "2. Perform 3 rounds of federated averaging\n",
    "3. Each client will train for 1 local epoch per round\n",
    "4. Use the 120M parameter AMPLIFY model by default\n",
    "5. Configure the regression MLP with layer sizes [128, 64, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1303414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python run_fl_multitask.py \\\n",
    "    --num_rounds 3 \\\n",
    "    --local_epochs 1 \\\n",
    "    --pretrained_model \"chandar-lab/AMPLIFY_120M\" \\\n",
    "    --layer_sizes \"128,64,32\" \\\n",
    "    --exp_name \"fedavg_multitask\" \\\n",
    "    --sim_gpus \"0\" # \"0,1,2,0,1,2\" for full training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ab815",
   "metadata": {},
   "source": [
    "### 1.3 Visualize the results\n",
    "\n",
    "Apart from monitoring the progress with TensorBoard, you can also use the plotting code in [figs/plot_training_curves.py](./figs/plot_training_curves.py) to load the generated TensorBoard event files and compare the performance \"local\" vs. \"fedavg\" experiments for each task. Here's an example of how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12327c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE metrics for all tasks\n",
    "!python figs/plot_training_curves.py \\\n",
    "    --log_dir /tmp/nvflare/AMPLIFY/multitask \\\n",
    "    --output_dir ./figs/tb_figs_rmse \\\n",
    "    --tag \"RMSE/local_test\" \\\n",
    "    --out_metric \"RMSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training Pearson coefficients for all tasks\n",
    "!python figs/plot_training_curves.py \\\n",
    "    --log_dir /tmp/nvflare/AMPLIFY/multitask \\\n",
    "    --output_dir ./figs/tb_figs_pearson \\\n",
    "    --tag \"Pearson/local_test\" \\\n",
    "    --out_metric \"Pearson\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291e2677",
   "metadata": {},
   "source": [
    "This will generate plots for each task comparing the local and federated training performance, saving them as both PNG and SVG files in the specified output directory. The plots will show the progression of the specified metric (RMSE or Pearson coefficients) over training steps for both local and federated training approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10896f47",
   "metadata": {},
   "source": [
    "**120M AMPLIFY Multi-task Fine-tuning Results**\n",
    "\n",
    "We plot the RMSE and Pearson Coefficients for different downstream tasks (lower is better): \"aggregation\", \"binding\", \"expression\", \"immunogenicity\", \"polyreactivity\", and \"Thermostability (tm)\". As can be observed, the models trained using FedAvg can achieve lower RMSE values for several downstream tasks compared to the locally only trained counterparts on the test set. \n",
    "\n",
    "Pearson Coefficients closer to 1.0 would indicate a direct positive correlation between the ground truth and predicted values. It can be observed that several downstream tasks are challenging for the 120M and only achieve low correlation scores. See the [FLAb paper](https://www.biorxiv.org/content/10.1101/2024.01.13.575504v1) for comparison. However, the FedAvg experiment shows benefits for several downstream tasks.\n",
    "\n",
    "> Note, by default, we smooth the training curves with a smoothing window of 30 (controlled by the `smoothing_window` argument).\n",
    "\n",
    "### Root Mean Squared Error\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px; flex-wrap: nowrap;\">\n",
    "<img src=\"./figs/tb_figs_rmse/aggregation.svg\" alt=\"Aggregation\" style=\"width: 300px; flex-shrink: 0;\"/>\n",
    "<img src=\"./figs/tb_figs_rmse/binding.svg\" alt=\"Binding\" style=\"width: 300px; flex-shrink: 0;\"/>\n",
    "<img src=\"./figs/tb_figs_rmse/expression.svg\" alt=\"Expression\" style=\"width: 300px; flex-shrink: 0;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px; flex-wrap: nowrap;\">\n",
    "<img src=\"./figs/tb_figs_rmse/immunogenicity.svg\" alt=\"Immunogenicity\"  style=\"width: 300px; flex-shrink: 0;\">\n",
    "<img src=\"./figs/tb_figs_rmse/polyreactivity.svg\" alt=\"Polyreactivity\"  style=\"width: 300px; flex-shrink: 0;\">\n",
    "<img src=\"./figs/tb_figs_rmse/tm.svg\" alt=\"Thermostability\"  style=\"width: 300px; flex-shrink: 0;\">\n",
    "</div>\n",
    "\n",
    "\n",
    "### Pearson Coefficient\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px; flex-wrap: nowrap;\">\n",
    "<img src=\"./figs/tb_figs_pearson/aggregation.svg\" alt=\"Aggregation\" style=\"width: 300px; flex-shrink: 0;\"/>\n",
    "<img src=\"./figs/tb_figs_pearson/binding.svg\" alt=\"Binding\" style=\"width: 300px; flex-shrink: 0;\"/>\n",
    "<img src=\"./figs/tb_figs_pearson/expression.svg\" alt=\"Expression\" style=\"width: 300px; flex-shrink: 0;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px; flex-wrap: nowrap;\">\n",
    "<img src=\"./figs/tb_figs_pearson/immunogenicity.svg\" alt=\"Immunogenicity\"  style=\"width: 300px; flex-shrink: 0;\">\n",
    "<img src=\"./figs/tb_figs_pearson/polyreactivity.svg\" alt=\"Polyreactivity\"  style=\"width: 300px; flex-shrink: 0;\">\n",
    "<img src=\"./figs/tb_figs_pearson/tm.svg\" alt=\"Thermostability\"  style=\"width: 300px; flex-shrink: 0;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e677aa",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates federated fine-tuning of the AMPLIFY protein language model for drug discovery applications. Here are the key components and steps covered:\n",
    "\n",
    "1. **Setup and Dependencies**\n",
    "   - Installation of AMPLIFY and required dependencies\n",
    "   - Setup of the FLAb repository containing experimental data for therapeutic antibodies\n",
    "\n",
    "2. **Data Preparation**\n",
    "   - Processing of six antibody properties: aggregation, binding, expression, immunogenicity, polyreactivity, and thermostability\n",
    "   - Data splitting into training (80%) and test (20%) sets\n",
    "   - Combination of heavy and light chain sequences\n",
    "\n",
    "3. **Model Architecture**\n",
    "   - Based on the 120M AMPLIFY pretrained model\n",
    "   - Transformer-based architecture with 24 encoder blocks\n",
    "   - Custom regression head with layer sizes [128, 64, 32]\n",
    "   - Total parameters: ~118M\n",
    "\n",
    "4. **Training Process**\n",
    "   - Federated learning setup with multiple clients\n",
    "   - Learning rates: Trunk (0.0001) and Regressor (0.01)\n",
    "   - Training metrics tracked: MSE loss, RMSE loss, and Pearson correlation\n",
    "   - Model evaluation on test sets for each property\n",
    "\n",
    "5. **Results**\n",
    "   - Performance metrics tracked across different antibody properties\n",
    "   - Visualization of training progress and model predictions\n",
    "   - Comparison of federated vs. centralized training approaches\n",
    "\n",
    "This example showcases how federated learning can be applied to drug discovery tasks while maintaining data privacy across different research institutions.\n",
    "\n",
    "Let's recap, what we learned in this [chapter](../../11.3_recap/recap.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
