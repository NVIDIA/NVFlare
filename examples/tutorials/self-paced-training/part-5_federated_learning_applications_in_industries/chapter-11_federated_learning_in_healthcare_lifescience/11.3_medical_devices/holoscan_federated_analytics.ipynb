{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82807dd-8490-4f47-bc0c-5a71e1cf37b3",
   "metadata": {},
   "source": [
    "# Holoscan Federated Analytics For Medical Devices\n",
    "\n",
    "This example shows NVIDIA FLARE's integration with NVIDIA Holoscan platform, to enable distributed data analytics for AI applications running on medical devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0738a99b-6ef9-4d30-b82f-634b6df302cd",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "[NVIDIA Holoscan](https://developer.nvidia.com/holoscan-sdk) is an AI sensor processing platform that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core microservices to run streaming, imaging, and other applications, from embedded to edge to cloud. It can be used to build streaming AI pipelines for a variety of domains, including Medical Devices, High Performance Computing at the Edge, Industrial Inspection and more. Holoscan platform offers:\n",
    "- [Holoscan SDK](https://github.com/nvidia-holoscan/holoscan-sdk): a comprehensive software development kit designed to enable real-time AI processing for streaming data applications. It's an open-source SDK, offering Python and C++ APIs.\n",
    "- [NVIDIA IGX](https://www.nvidia.com/en-us/edge-computing/products/igx/): an industrial-grade edge AI hardware platform, designed to deliver high performance, advanced functional safety, and robust security for enterprise applications. It is the ideal hardware to run applications developed using Holoscan SDK, delivering optimized performance for time-critical applications, for instance for medical devices.\n",
    "- [Holoscan Sensor Bridge](https://docs.nvidia.com/holoscan/sensor-bridge/1.0.0/index.html): an FPGA based interface designed to enable real-time, low-latency streaming of data from a wide range of sensors—including cameras, LiDAR, radar, and RF devices, to GPUs.\n",
    "- [Holohub](https://github.com/nvidia-holoscan/holohub): a central hub for open-source applications developed using Holoscan SDK for different industrial use cases.\n",
    "\n",
    "![Holoscan](image/holoscan.png)\n",
    "\n",
    "In this tutorial, we demonstrate how to leverage NVIDIA FLARE to enable federated data analytics for medical devices running on Holoscan. We use the AI [endoscopy out-of-body detection](https://github.com/nvidia-holoscan/holohub/tree/main/applications/endoscopy_out_of_body_detection) Holoscan application as an example, and showcase how to compute federated statistics of the Holoscan application running on a fleet of IGX devices.\n",
    "\n",
    "![Architecture](image/architecture.png)\n",
    "\n",
    "- We will start with setting up the Holohub repository, building the endoscopy out-of-body detection Holoscan application and configuring it to output analytic data in a standardized format.\n",
    "- Then, we will run the application in separate sessions under a simulated environment to generate different analytic data, as if the same application is running on a fleet of IGX devices.\n",
    "- Next, we will set up NVIDIA FLARE server, admin and clients to perform secure, distributed aggregation of analytic data from the Holoscan application, and compute federated statistics.\n",
    "- Finally, we will visualize the aggregated statistics from different runs of the Holoscan application.\n",
    "\n",
    "This tutorial is based on the [official repository for Holoscan Federated Analytics](https://github.com/nvidia-holoscan/holoscan-federated-analytics): refer to this repository to learn more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b2dff-972b-4baa-8d66-6a7a64deaff2",
   "metadata": {},
   "source": [
    "## Setting Up the Holoscan Application\n",
    "\n",
    "Let's first clone Holohub and build the endoscopy out-of-body detection application. Out-of-body detection is an important application in robot-assisted endoscopic surgeries. It helps identify which frames from the endoscope camera are out of the patient body, which might capture personal information about the operating room staff. Storing or broadcasting endoscope camera data without any deidentification might introduce a breach of data privacy regulations. With automatic detection of out-of-body endoscopic frames, a subsequent anonymization of these frames (using, for instance, Gaussian blur or mosaic pixelation) can be applied, achieving automatic deidentification of endoscope camera data. \n",
    "\n",
    "The Holoscan endoscopy out-of-body detection application that we will build in this tutorial applies a SEResNet50 deep-learning model, trained on endoscopic data, to perform binary in- and out-of-body classification on every frame of an endoscopic video.\n",
    "\n",
    "![Model](image/model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bade321-8f7a-48b9-b100-125da521f5c7",
   "metadata": {},
   "source": [
    "To set up the application, let's first clone Holohub with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63956a3d-a8e0-4c81-a901-38f7138e4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nvidia-holoscan/holohub.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e43f25-973d-4d9b-b906-6a6b98503a6c",
   "metadata": {},
   "source": [
    "After cloning it, let's build an image with basic dependencies to run Holoscan sample applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978fddf-8d76-4900-a3a1-305ef5e6c5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd holohub && ./dev_container build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8faa40f-6c4a-4e59-b000-c7ca4eb708b4",
   "metadata": {},
   "source": [
    "> **Note**\n",
    "> - To successfully build Holohub image, you need a PC with Ubuntu operating system and an NVIDIA GPU, or a supported NVIDIA developer kit. Check out the prerequisites [here](https://docs.nvidia.com/holoscan/sdk-user-guide/sdk_installation.html#prerequisites).\n",
    "> - Depending on your platform, the container build time could take more than 10 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67107d91-d2ec-4041-8f59-2630ea5cbf58",
   "metadata": {},
   "source": [
    "The command above should build an image named `holohub` with certain tag depending on the current version of Holoscan SDK release, and your platform. For example: `holohub:ngc-v3.3.0-dgpu`. Let's now execute the following command to start a container from the built image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a218ea7-25a4-479d-aff9-42bdb12403c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd holohub && ./dev_container launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7297755-811b-4fb9-9655-a1630e94e790",
   "metadata": {},
   "source": [
    "Once inside the container, we can use the `run` script to build the `endoscopy_out_of_body_detection` application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a215b8-fdf8-4e2d-aacb-87e7d64af428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside the holohub container\n",
    "!./run build endoscopy_out_of_body_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07173b0d-f9e4-4e9c-883b-7345c6848010",
   "metadata": {},
   "source": [
    "When building the application for the first time, the `run` script downloads the following from [NVIDIA GPU Cloud](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/clara-holoscan/resources/endoscopy_out_of_body_detection) (NGC) into a `data/endoscopy_out_of_body_detection` folder under `holohub` directory:\n",
    "- The trained in- and out-of-body detection model in `.onnx` format: `out_of_body_detection.onnx`\n",
    "- A sample endoscopy video clip in `.mp4` format: `sample_clip_out_of_body_detection.mp4`\n",
    "\n",
    "The 2 images below show a sample in-body frame (first) and out-of-body frame (second) from the sample video clip.\n",
    "\n",
    "![In Body](image/in_body_frame.png) ![Out of Body](image/out_body_frame.png)\n",
    "\n",
    "Once the build is finished, let's try to run this application. The `run` script can be used to run either the C++ or Python version of the application, if they both exist. For instance, execute the following command to run the Python version of the application:\n",
    "\n",
    "> **Note:** many sample applications from Holohub can be run using either C++ or Python, by specifying `cpp` or `python` flag for the `run` program. In this example, we will run the `endoscopy_out_of_body_detection` in Python. You should get the same result when running it in C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786325aa-6767-4914-a637-86e403ffd3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside the holohub container\n",
    "!./run launch endoscopy_out_of_body_detection python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65098b60-199f-4468-b849-8ff19a8fa0d6",
   "metadata": {},
   "source": [
    "Upon a successful run, you should see console output as follows, printing the in-/out-of-body classification label and confidence score for each frame of the sample endoscopy video:\n",
    "```bash\n",
    " ...\n",
    " Likely in-body. Confidence: 0.780183\n",
    " Likely in-body. Confidence: 0.850593\n",
    " Likely in-body. Confidence: 0.867589\n",
    " Likely in-body. Confidence: 0.715327\n",
    " Likely out-of-body. Confidence: 0.628363\n",
    " Likely out-of-body. Confidence: 0.814279\n",
    " Likely out-of-body. Confidence: 0.873904\n",
    " Likely out-of-body. Confidence: 0.922412\n",
    "...\n",
    "```\n",
    "\n",
    "> **Note**: it takes some time to start the application when you run it for the first time. This is because the deep learning model will be converted to optimized TensorRT format based on your platform, which can be time-consuming. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97e5554-0b5b-49da-9a23-3f163f340354",
   "metadata": {},
   "source": [
    "## Generating Analytics From Holoscan Application\n",
    "\n",
    "To enable federated analytics for all Holoscan applications, Holoscan SDK provides a `DataExporter` API, allowing applications to export data such as model inference output to a standardized format, for instance, a CSV file.\n",
    "\n",
    "The `endoscopy_out_of_body_detection` application was configured to leverage `DataExporter` API to output the classification result of each frame to a CSV file marked with the timestamp of the application run. An example output CSV file would have content like the following:\n",
    "```bash\n",
    "In-body,Out-of-body,Confidence Score\n",
    "1,0,0.972435\n",
    "1,0,0.90207\n",
    "1,0,0.897973\n",
    "0,1,0.939281\n",
    "0,1,0.948691\n",
    "0,1,0.94994\n",
    "```\n",
    "\n",
    "Refer to the [`DataExporter` documentation](https://docs.nvidia.com/holoscan/sdk-user-guide/components/analytics.html#data-exporter-api) to learn more.\n",
    "\n",
    "Let's now run this application twice, and configure it to generate two distinct output CSV files, simulating a distributed run of the application on 2 distinct IGX devices.\n",
    "\n",
    "### Running `endoscopy_out_of_body_detection` application on IGX 1\n",
    "\n",
    "First, let's start the holohub container that we just built, and mount the directory `/tmp/output-igx-1` to generate the application output later. Holoscan applications use the `HOLOSCAN_ANALYTICS_DATA_DIRECTORY` environment variable to determine the analytics output directory. Therefore we need to assign `/tmp/output-igx-1` to `HOLOSCAN_ANALYTICS_DATA_DIRECTORY` variable.\n",
    "\n",
    "```bash\n",
    "    cd holohub\n",
    "    mkdir /tmp/output-igx-1\n",
    "    ./dev_container launch --docker_opts \"-v /tmp/output-igx-1:/workspace/holoscan_data\"\n",
    "    \n",
    "    # Set the `HOLOSCAN_ANALYTICS_DATA_DIRECTORY` to `/workspace/holoscan_data`\n",
    "    export HOLOSCAN_ANALYTICS_DATA_DIRECTORY=/workspace/holoscan_data\n",
    "```\n",
    "\n",
    "Then, we can run the application with an extra argument `--analytics`. This tells the application to generate a CSV output file inside the previously defined `HOLOSCAN_ANALYTICS_DATA_DIRECTORY` directory.\n",
    "```bash\n",
    "    # Run endoscopy out of body detection application.\n",
    "    ./run launch endoscopy_out_of_body_detection python --extra_args \"--analytics\"\n",
    "```\n",
    "\n",
    "After the run finishes, you should find an output CSV file under `/tmp/output-igx-1/out_of_body_detection/[TIMESTAMP]/data.csv`. `TIMESTAMP` would be the time where the application was executed.\n",
    "\n",
    "### Running `endoscopy_out_of_body_detection` application on IGX 2\n",
    "\n",
    "Similar to the previous run, let's simulate running the application on a second IGX, by generating output inside directory `/tmp/output-igx-2`:\n",
    "\n",
    "```bash\n",
    "    cd holohub\n",
    "    mkdir /tmp/output-igx-2\n",
    "    ./dev_container launch --docker_opts \"-v /tmp/output-igx-2:/workspace/holoscan_data\"\n",
    "    \n",
    "    # Set the `HOLOSCAN_ANALYTICS_DATA_DIRECTORY` to `/workspace/holoscan_data`\n",
    "    export HOLOSCAN_ANALYTICS_DATA_DIRECTORY=/workspace/holoscan_data\n",
    "```\n",
    "\n",
    "Then, we can run the application the same way as before:\n",
    "```bash\n",
    "    # Run endoscopy out of body detection application.\n",
    "    ./run launch endoscopy_out_of_body_detection python --extra_args \"--analytics\"\n",
    "```\n",
    "\n",
    "After the run finishes, you should find an output CSV file under `/tmp/output-igx-2/out_of_body_detection/[TIMESTAMP]/data.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436ad492",
   "metadata": {},
   "source": [
    "## Set Up Federated Server and Clients\n",
    "\n",
    "In this section, we will perform the following to set up the environment to compute federated analytics for the `endoscopy_out_of_body_detection` application using NVIDIA FLARE:\n",
    "- Build an NVIDIA FLARE image\n",
    "- Provision a federated system\n",
    "- Start the server and clients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581a286",
   "metadata": {},
   "source": [
    "### Build an NVIDIA FLARE image\n",
    "\n",
    "To build a docker image with NVIDIA FLARE and dependencies for this tutorial, we use the `dev_container` script provided by the [official repository for Holoscan Federated Analytics](https://github.com/nvidia-holoscan/holoscan-federated-analytics), which we copied under [`code/dev_container`](code/dev_container).\n",
    "\n",
    "> **Note**: this is not the same `dev_container` script from Holohub repo.\n",
    "\n",
    "Let's go ahead and execute the following commands in a terminal:\n",
    "\n",
    "```bash\n",
    "cd code\n",
    "./dev_container build\n",
    "```\n",
    "\n",
    "This will build an image tagged `holoscan-nvflare-service`, using [code/docker/Dockerfile](code/docker/Dockerfile) as Dockerfile and [code/docker/requirements.txt](code/docker/requirements.txt) for dependencies installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2534054e",
   "metadata": {},
   "source": [
    "### Provision a federated system\n",
    "\n",
    "Let's first start a container from the image that we just built:\n",
    "```bash\n",
    "cd code\n",
    "docker run -it -v $(pwd):/workspace -u $(id -u):$(id -g) -w /workspace holoscan-nvflare-service \n",
    "```\n",
    "\n",
    "Once inside the container, let's provision a federated system using the configuration file in [code/project.yml](code/project.yml):\n",
    "\n",
    "```bash\n",
    "nvflare provision -p project.yml \n",
    "```\n",
    "This will create a `workspace` folder, with a provisioned system for the following participants, as indicated in the configuration file:\n",
    "- One server with name: `holoscan.nvflare.server`\n",
    "- Two clients: `Holoscan-Device-1` and `Holoscan-Device-2`. These clients correspond to the two IGX devices that we simulated previously to run the `endoscopy_out_of_body_detection` application separately.\n",
    "- One project admin with name: `holoscan_admin@nvidia.com`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db1f06",
   "metadata": {},
   "source": [
    "\n",
    "### Start the server\n",
    "\n",
    "> **Note**: you can start the server from any machine, as long as NVIDIA FLARE is installed and the machine's IP address is reachable by the clients and the admin. For the sake of simplicity, here we start the server on the same computer as the clients and the admin.\n",
    "\n",
    "First, let's create a directory to store server-side aggregated output:\n",
    "```bash\n",
    "mkdir -p /tmp/output-server/endoscopy_out_of_body_detection\n",
    "```\n",
    "\n",
    "Then, we can spin up a container from the `holoscan-nvflare-service` image to start the server. We can do this using `docker run` command with manually specified arguments, but the `dev_container` script provided by the [official repository for Holoscan Federated Analytics](https://github.com/nvidia-holoscan/holoscan-federated-analytics) offers an easier way. Let's go ahead and execute the following command in a terminal:\n",
    "\n",
    "> **Note**: we are using the `dev_container` script under [`code/dev_container`](code/dev_container). It is not the same `dev_container` script from Holohub repo.\n",
    "\n",
    "```bash\n",
    "cd code\n",
    "\n",
    "# Run NVFLARE server container in a terminal\n",
    "./dev_container run --server holoscan.nvflare.server --data /tmp/output-server/endoscopy_out_of_body_detection\n",
    "```\n",
    "Notice that here we specify `holoscan.nvflare.server` as the server name, as we previously provisioned.\n",
    "\n",
    "Once inside the container, we can start the NVFLARE server with:\n",
    "```bash\n",
    "./start.sh\n",
    "```\n",
    "When the server has successfully started, you should see console output similar to:\n",
    "```bash\n",
    "...\n",
    "2025-06-26 09:55:49,392 - root - INFO - Server started\n",
    "2025-06-26 09:55:54,389 - ServerState - INFO - Got the primary sp: holoscan.nvflare.server fl_port: 8002 SSID: ebc6125d-0a56-4688-9b08-355fe9e4d61a. Turning to hot.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea8cdea",
   "metadata": {},
   "source": [
    "### Start the clients\n",
    "\n",
    "Before starting the client, let's first make sure that the server name `holoscan.nvflare.server` has a reachable IP address inside the `/etc/hosts` of the client machine. For this, we need to add the following to `/etc/hosts` of the clients:\n",
    "```bash\n",
    "SERVER_IP_ADDRESS \tholoscan.nvflare.server\n",
    "```\n",
    "\n",
    "Since we are simulating everything on a single machine locally, we can just go ahead and add the following to the `/etc/hosts` of the machine\n",
    "```bash\n",
    "127.0.0.1 \tholoscan.nvflare.server\n",
    "```\n",
    "\n",
    "Next, we can leverage the same `dev_container` script to spin up two containers for the two clients, each in a separate terminal.\n",
    "\n",
    "> **Note**: we are using the `dev_container` script under [`code/dev_container`](code/dev_container). It is not the same `dev_container` script from Holohub repo.\n",
    "\n",
    "Start a **separate terminal** for client 1:\n",
    "```bash\n",
    "cd code\n",
    "./dev_container run --client Holoscan-Device-1 --data /tmp/output-igx-1\n",
    "```\n",
    "\n",
    "Notice that here we specify `Holoscan-Device-1` as the name for client 1, as we previously provisioned. The flag `--data` is used to specify the output directory of the `endoscopy_out_of_body_detection` application, which is `/tmp/output-igx-1` for client 1.\n",
    "\n",
    "Once inside the container, start the client 1 with:\n",
    "```bash\n",
    "./start.sh\n",
    "```\n",
    "Upon successful connection to the server, you should see console logs like the following:\n",
    "```bash\n",
    "...\n",
    "2025-06-26 11:59:56,613 - FederatedClient - INFO - Successfully registered client:Holoscan-Device-1 for project holoscan_federated_analytics. Token:f4c2a8e8-65e9-43c1-ae99-1d7f8699ec5c SSID:ebc6125d-0a56-4688-9b08-355fe9e4d61a\n",
    "2025-06-26 11:59:56,616 - FederatedClient - INFO - Got engine after 0.42764997482299805 seconds\n",
    "2025-06-26 11:59:56,616 - FederatedClient - INFO - Got the new primary SP: grpc://holoscan.nvflare.server:8002\n",
    "```\n",
    "\n",
    "For client 2, the process is similar. Start a **new terminal**, and execute the following commands:\n",
    "```bash\n",
    "cd code\n",
    "./dev_container run --client Holoscan-Device-2 --data /tmp/output-igx-2\n",
    "```\n",
    "\n",
    "Once inside the container, start the client 2 with:\n",
    "```bash\n",
    "./start.sh\n",
    "```\n",
    "Upon successful connection to the server, you should see console logs like the following:\n",
    "```bash\n",
    "...\n",
    "2025-06-26 12:02:24,644 - FederatedClient - INFO - Successfully registered client:Holoscan-Device-2 for project holoscan_federated_analytics. Token:f3bc350d-5b39-4212-ae7d-6c55c8ad2306 SSID:ebc6125d-0a56-4688-9b08-355fe9e4d61a\n",
    "2025-06-26 12:02:24,649 - FederatedClient - INFO - Got engine after 0.43787550926208496 seconds\n",
    "2025-06-26 12:02:24,649 - FederatedClient - INFO - Got the new primary SP: grpc://holoscan.nvflare.server:8002\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00309c7f",
   "metadata": {},
   "source": [
    "## Compute Hierarchical Federated Statistics\n",
    "\n",
    "The [official repository for Holoscan Federated Analytics](https://github.com/nvidia-holoscan/holoscan-federated-analytics) provides a sample FLARE application `holoscan_fl_example` that computes several federated statistical values from different clients. We will run this sample FLARE application in this tutorial, which is copied under `code/application/holoscan_fl_example`.\n",
    "\n",
    "First, let's understand what this application is computing. From the [server's configuration](code/application/holoscan_fl_example/app/config/config_fed_server.json), we can see that this application is using the [HierarchicalStatisticsController](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/hierarchical_statistics_controller.py), to aggregate the following statistical values from the clients:\n",
    "- `count`\n",
    "- `sum`\n",
    "- `max`\n",
    "- `min`\n",
    "- `mean`\n",
    "- `var` \n",
    "- `stddev`\n",
    "- `histogram`\n",
    "\n",
    "Local statistics computation is implemented in [code/application/holoscan_fl_example/app/custom/holoscan_statistics.py](code/application/holoscan_fl_example/app/custom/holoscan_statistics.py), which will be used by FLARE's [StatisticsExecutor](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/executors/statistics/statistics_executor.py), as defined in [code/application/holoscan_fl_example/app/config/config_fed_client.json](code/application/holoscan_fl_example/app/config/config_fed_client.json).\n",
    "\n",
    "The [HierarchicalStatisticsController](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/hierarchical_statistics_controller.py) is **hierarchical** because it exposes a `hierarchy_config` argument to specify the **hierarchy configuration** among all clients. Once a client hierarchy specified, the aggregated statistics will be computed for each hierarchical level. In this sample app, the hierarchy configuration is located at [code/application/holoscan_fl_example/app/config/device_map_2.json](code/application/holoscan_fl_example/app/config/device_map_2.json), with the following content:\n",
    "```json\n",
    "{\n",
    "  \"Manufacturers\": [\n",
    "    {\n",
    "      \"Name\": \"Manufacturer-1\",\n",
    "      \"Devices\": [\"Holoscan-Device-1\"]\n",
    "    },\n",
    "    {\n",
    "      \"Name\": \"Manufacturer-2\",\n",
    "      \"Devices\": [\"Holoscan-Device-2\"]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "This configuration defines 3 hierarchical levels: device level, manufacturer level and global level. Therefore aggregated statistics will be computed at:\n",
    "- Each device individually (this is in fact the local statistics)\n",
    "- Each manufacturer's devices\n",
    "- Global level, for all devices from all manufacturers\n",
    "\n",
    "To learn more about hierarchical statistics, please refer to [this example](https://github.com/NVIDIA/NVFlare/tree/main/examples/advanced/federated-statistics/hierarchical_stats) in NVIDIA FLARE's tutorial catalog, which shows how to compute various statistics under a complex hierarchy.\n",
    "\n",
    "Now let's run this application. We need to start the admin user and submit a job to run this application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a44dceb",
   "metadata": {},
   "source": [
    "\n",
    "### Start the admin\n",
    "\n",
    "> **Note**: you can start the admin from any machine, as long as NVIDIA FLARE is installed and the machine can reach the server. For the sake of simplicity, here we start the admin on the same computer as the server and the clients.\n",
    "\n",
    "Run the following command in a new terminal to start a container for the admin:\n",
    "\n",
    "> **Note**: we are using the `dev_container` script under [`code/dev_container`](code/dev_container). It is not the same `dev_container` script from Holohub repo.\n",
    "\n",
    "```bash\n",
    "cd code\n",
    "./dev_container run --admin holoscan_admin@nvidia.com\n",
    "```\n",
    "\n",
    "Once inside the container, start the admin with command:\n",
    "```bash\n",
    "./startup/fl_admin.sh\n",
    "```\n",
    "You will be prompted to enter the `user Name:` of the admin. Enter: `holoscan_admin@nvidia.com`, you will enter the FLARE Console:\n",
    "```bash\n",
    "Trying to obtain server address\n",
    "Obtained server address: holoscan.nvflare.server:8003\n",
    "Trying to login, please wait ...\n",
    "Logged into server at holoscan.nvflare.server:8003 with SSID: ebc6125d-0a56-4688-9b08-355fe9e4d61a\n",
    "Type ? to list commands; type \"? cmdName\" to show usage of a command.\n",
    "> \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8fdbc3",
   "metadata": {},
   "source": [
    "\n",
    "### Submit a job to run the `holoscan_fl_example` application\n",
    "\n",
    "Let's open a new terminal, and copy the application folder under the `transfer` folder of the admin:\n",
    "```bash\n",
    "cp -r code/application/holoscan_fl_example code/workspace/holoscan_federated_analytics/prod_00/holoscan_admin@nvidia.com/transfer/.\n",
    "```\n",
    "\n",
    "> **Note:** you can submit the job in FLARE Console by typing the application folder's absolute path. Here for simplicity, we copy the application to admin's transfer folder, so job submission can be done by only typing the application folder's name.\n",
    "\n",
    "Now, submit a job to run the `holoscan_fl_example` application, by executing the following command inside admin's FLARE Console:\n",
    "\n",
    "```bash\n",
    "# Inside FLARE console\n",
    "submit_job holoscan_fl_example\n",
    "```\n",
    "\n",
    "You can use `list_jobs` to query the status of the job's runtime. Once the job is completed, you will get the following status:\n",
    "```bash\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "| JOB ID                               | NAME                      | STATUS             | SUBMIT TIME                      | RUN DURATION   |\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "| 443ac73c-261d-4559-adf7-6bbbc868dd38 | holoscan_fl_example_stats | FINISHED:COMPLETED | 2025-06-26T12:16:06.338911+00:00 | 0:00:08.706943 |\n",
    "---------------------------------------------------------------------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "The global statistics will be a json file `nvflare_output.json`, available under our predefined output directory `/tmp/output-server/endoscopy_out_of_body_detection/`, under a folder with the timestamp of the current FLARE job execution. \n",
    "\n",
    "Examine its content, you will find aggregated global statistics (`count`, `sum`, `max`, `min`, `mean`, `var`, `stddev`, `histogram`), for different hierarchical levels: device (local), manufacturer, and global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539d5a8d",
   "metadata": {},
   "source": [
    "### Visualization of hierarchical statistics\n",
    "\n",
    "The [official repository for Holoscan Federated Analytics](https://github.com/nvidia-holoscan/holoscan-federated-analytics) also provides [a guide](https://github.com/nvidia-holoscan/holoscan-federated-analytics/blob/main/tutorial/README.md#visualizing-federated-analytics-results) on setting up visualization backend and frontend to visualize federated statistics in a web application.\n",
    "\n",
    "Let's go ahead and set up the visualization components. The backend and frontend applications are copied under `code/visualization`.\n",
    "\n",
    "### Set up the backend\n",
    "\n",
    "First, let's open a terminal, and build and run the backend REST API service container:\n",
    "\n",
    "> **Note**: the following commands for the backend REST API service need to be run on the same machine as the NVIDIA FLARE server, as the backend container is configured to access our predefined data output directory for the server-side aggregation, i.e., `/tmp/output-server`.\n",
    "\n",
    "```bash\n",
    "cd code/visualization/backend\n",
    "docker compose build\n",
    "```\n",
    "\n",
    "This will build an image named `backend-web`. After the build finishes, run the following commands:\n",
    "```bash\n",
    "cd code/visualization/backend\n",
    "docker compose up\n",
    "```\n",
    "\n",
    "When the backend service is up-and-running, you should see console logs similar to the following:\n",
    "```bash\n",
    "...\n",
    "analytics-backend  | INFO:     Will watch for changes in these directories: ['/app']\n",
    "analytics-backend  | INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
    "analytics-backend  | INFO:     Started reloader process [1] using StatReload\n",
    "analytics-backend  | INFO:     Started server process [8]\n",
    "analytics-backend  | INFO:     Waiting for application startup.\n",
    "analytics-backend  | INFO:     Application startup complete.\n",
    "```\n",
    "\n",
    "The REST API service will be accessible on port `8000` by default. Currently the following APIs are supported:\n",
    "- Getting the list of existing applications (API: `/get_apps/`)\n",
    "- Getting the list of available statistics for a given application (API: `/get_stats_list/{app_name}/`)\n",
    "- Getting the statistics for a given application (API: `/get_stats/{app_name}/` and `/get_stats/{app_name}/?timestamp=<timestamp>`)\n",
    "- Getting the statistics for a date range (API: `/get_range_stats/{app_name}/{start}/{end}/`)\n",
    "\n",
    "To test the backend API service, let's create a test JWT token that can be used to authenticate REST API service. In a terminal, execute the following commands:\n",
    "```bash\n",
    "cd code/visualization/backend\n",
    "\n",
    "python3 -m venv .venv # You can use whatever python virtual env tool you want to create a virtual env\n",
    "source .venv/bin/activate\n",
    "pip3 install -r requirements.txt\n",
    "\n",
    "TOKEN=`python3 ./create_test_token.py`\n",
    "echo $TOKEN # We will also use this key later for the frontend.\n",
    "```\n",
    "Then, send the following `curl` request in the same terminal:\n",
    "```bash\n",
    "curl -H \"Authorization: Bearer $TOKEN\" http://127.0.0.1:8000/api/v1/get_apps/\n",
    "```\n",
    "You should get the console output: `[\"endoscopy_out_of_body_detection\"]`, which is the name of the application that we are running in this tutorial.\n",
    "\n",
    "### Set up the frontend\n",
    "\n",
    "Let's build and run the frontend application.\n",
    "\n",
    "> **Note**: the frontend application can be built and run on any device, as long as the IP address of the backend API service is properly set up (see below). Here for simplicity, we build and run the front end application on the same machine as the backend API service.\n",
    "\n",
    "First, open the [code/visualization/frontend/analytics-dashboard/.env](code/visualization/frontend/analytics-dashboard/.env) file, and edit the following:\n",
    "- Set `NEXT_PUBLIC_AUTHORIZATION_HEADER` to be the JWT key that you just created in the section above to test the backend API service. You can print this key with `echo $TOKEN` in the same terminal that you used to generate the JWT key.\n",
    "- Set `NEXT_PUBLIC_ROOT_URI` to be: `http://[BACKEND_IP_ADDRESS]]:8000/api/v1`, where `BACKEND_IP_ADDRESS`, as the name suggests, is the IP address of the machine running the backend API service. Since we are running everything locally, let's just set it to `http://127.0.0.1:8000/api/v1`.\n",
    "\n",
    "Open a terminal and run the following commands.\n",
    "\n",
    "```bash\n",
    "cd code/visualization/frontend/analytics-dashboard\n",
    "docker compose build\n",
    "```\n",
    "\n",
    "This will build an image named `analytics-dashboard-nextjs`. Now run the frontend application with the following commands:\n",
    "```bash\n",
    "cd code/visualization/frontend/analytics-dashboard\n",
    "docker compose up\n",
    "```\n",
    "\n",
    "Once the frontend application is up-and-running, you should see console logs similar to the following:\n",
    "```bash\n",
    "...\n",
    "analytics-dashboard  | > analytics-dashboard@0.1.0 start /app\n",
    "analytics-dashboard  | > next start -p $PORT\n",
    "analytics-dashboard  | \n",
    "analytics-dashboard  |   ▲ Next.js 14.2.15\n",
    "analytics-dashboard  |   - Local:        http://localhost:8888\n",
    "analytics-dashboard  | \n",
    "analytics-dashboard  |  ✓ Starting...\n",
    "analytics-dashboard  |  ✓ Ready in 184ms\n",
    "...\n",
    "```\n",
    "\n",
    "### Visualize the hierarchical statistics\n",
    "\n",
    "Open a browser and navigate to `http://[FRONTEND_IP_ADDRESS]:8000` (for instance, `http://localhost:8888` if you are using the same machine running the frontend application), you will be able to visualize different aggregated statistics, as illustrated below.\n",
    "\n",
    "![Visualization](image/visualization.png)\n",
    "\n",
    "Feel free to play with different settings in the visualization UI to explore more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e222b403",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "That's it, you've learned how to leverage NVIDIA FLARE to enable federated analytics for Holoscan applications running on medical devices. Feel free to refer to the [official repository for Holoscan Federated Analytics](https://github.com/nvidia-holoscan/holoscan-federated-analytics) to learn about more aspects to it, for instance, how to [dynamically add new devices to current federation](https://github.com/nvidia-holoscan/holoscan-federated-analytics/blob/main/tutorial/README.md#dynamic-provisioning-of-nvflare-clients-and-users) with FLARE's dynamic provisioning system.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
