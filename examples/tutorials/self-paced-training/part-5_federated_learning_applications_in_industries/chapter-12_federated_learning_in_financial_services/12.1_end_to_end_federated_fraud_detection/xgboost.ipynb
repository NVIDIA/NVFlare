{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66676f50-e23a-44bf-b75b-7fa917ab7055",
   "metadata": {},
   "source": [
    "# End-to-end credit card fraud detection with Federated XGBoost\n",
    "\n",
    "This notebook shows how to convert an existing tabular credit dataset, enrich and pre-process the data using a single site (like a centralized dataset), and then convert this centralized process into federated ETL steps easily. Then, construct a federated XGBoost; the only thing the user needs to define is the XGBoost data loader. \n",
    "\n",
    "## Install requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dd6a9-ca60-42c8-b85f-f4a7b11c3a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d994614-7d32-40c0-9645-ed62eed2654b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1: Data Preparation \n",
    "First, we prepare the data by adding random transactional information to the base creditcard dataset following the below script:\n",
    "\n",
    "* [prepare data](./notebooks/prepare_data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69c008a-b19b-4c1a-b3c4-c376eccf53ba",
   "metadata": {},
   "source": [
    "## Step 2: Feature Analysis\n",
    "\n",
    "For this stage, we would like to analyze the data, understand the features, and derive (and encode) secondary features that can be more useful for building the model.\n",
    "\n",
    "Towards this goal, there are two options:\n",
    "1. **Feature Enrichment**: This process involves adding new features based on the existing data. For example, we can calculate the average transaction amount for each currency and add this as a new feature. \n",
    "2. **Feature Encoding**: This process involves encoding the current features and transforming them to embedding space via machine learning models. This model can be either pre-trained, or trained with the candidate dataset.\n",
    "\n",
    "Considering the fact that the only two numerical features in the dataset are \"Amount\" and \"Time\", we will perform feature enrichment first. Optionally, we can also perform feature encoding. In this example, we use a graph neural network (GNN); we will train the GNN model in a federated, unsupervised fashion and then use the model to encode the features for all sites. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcf825-6e31-4d10-9968-2f353eaa4cea",
   "metadata": {},
   "source": [
    "### Step 2.1: Rule-based Feature Enrichment\n",
    "\n",
    "#### Single-site Enrichment and Additional Processing\n",
    "The detailed feature enrichment step is illustrated using one site as example: \n",
    "\n",
    "* [feature_enrichments with-one-site](./notebooks/feature_enrichment.ipynb)\n",
    "\n",
    "Similarly, we examine the additional pre-processing step using one site: \n",
    "\n",
    "* [pre-processing with one-site](./notebooks/pre_process.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc8bb99-a253-415e-8953-91af62ef22a2",
   "metadata": {},
   "source": [
    "#### Federated Job to Perform on All Sites\n",
    "In order to run feature enrichment and processing job on each site similar to above steps, we wrote federated ETL job scripts for client-side based on single-site implementations.\n",
    "\n",
    "* [enrichment script](./nvflare/enrich.py)\n",
    "* [pre-processing script](./nvflare/pre_process.py) \n",
    "\n",
    "Then we define job scripts on server-side to trigger and coordinate running client-side scripts on each site: \n",
    "\n",
    "* [enrich_job.py](./nvflare/enrich_job.py)\n",
    "* [pre-processing-job](./nvflare/pre_process_job.py)\n",
    "\n",
    "Example script as below:\n",
    "```\n",
    "# Define the enrich_ctrl workflow and send to server\n",
    "    enrich_ctrl = ETLController(task_name=\"enrich\")\n",
    "    job.to(enrich_ctrl, \"server\", id=\"enrich\")\n",
    "\n",
    "    # Add clients\n",
    "    for site_name in site_names:\n",
    "        executor = ScriptExecutor(task_script_path=task_script_path, task_script_args=task_script_args)\n",
    "        job.to(executor, site_name, tasks=[\"enrich\"], gpu=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068c808-fce8-4f27-b0cf-76b486a24903",
   "metadata": {},
   "source": [
    "### (Optional) Step 2.2: GNN-based Feature Encoding\n",
    "Based on raw features, or combining the derived features from **Step 2.1**, we can use machine learning models to encode the features. \n",
    "In this example, we use federated GNN to learn and generate the feature embeddings.\n",
    "\n",
    "First, we construct a graph based on the transaction data. Each node represents a transaction, and the edges represent the relationships between transactions. We then use the GNN to learn the embeddings of the nodes, which represent the transaction features.\n",
    "\n",
    "#### Single-site operation example: graph construction\n",
    "The detailed graph construction step is illustrated using one site as example:\n",
    "\n",
    "* [graph_construction with one-site](./notebooks/graph_construct.ipynb)\n",
    "\n",
    "The detailed GNN training and encoding step is illustrated using one site as example:\n",
    "\n",
    "* [gnn_training_encoding with one-site](./notebooks/gnn_train_encode.ipynb)\n",
    "\n",
    "#### Federated Job to Perform on All Sites\n",
    "In order to run feature graph construction job on each site similar to the enrichment and processing steps, we wrote federated ETL job scripts for client-side based on single-site implementations.\n",
    "\n",
    "* [graph_construction script](./nvflare/graph_construct.py)\n",
    "* [gnn_train_encode script](./nvflare/gnn_train_encode.py)\n",
    "\n",
    "Similarily, we define job scripts on server-side to trigger and coordinate running client-side scripts on each site: \n",
    "\n",
    "* [graph_construction_job.py](./nvflare/graph_construct_job.py)\n",
    "* [gnn_train_encode_job.py](./nvflare/gnn_train_encode_job.py)\n",
    "\n",
    "The resulting GNN encodings will be merged with the normalized data for enhancing the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a102a09-1424-4f24-bb37-f8c65040950d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3: Federated XGBoost \n",
    "\n",
    "Now that we have the data ready, either enriched and normalized features, or GNN feature embeddings, we can fit them with XGBoost. NVIDIA FLARE has already written XGBoost Controller and Executor for the job. All we need to provide is the data loader to fit into the XGBoost.\n",
    "\n",
    "To specify the controller and executor, we need to define a Job. You can find the job construction in\n",
    "\n",
    "* [xgb_job.py](./nvflare/xgb_job.py)\n",
    "* [xgb_job_embed.py](./nvflare/xgb_job_embed.py)\n",
    "\n",
    "Below is main part of the code\n",
    "\n",
    "```\n",
    "    controller = XGBFedController(\n",
    "        num_rounds=num_rounds,\n",
    "        training_mode=\"horizontal\",\n",
    "        xgb_params=xgb_params,\n",
    "        xgb_options={\"early_stopping_rounds\": early_stopping_rounds},\n",
    "    )\n",
    "    job.to(controller, \"server\")\n",
    "\n",
    "    # Add clients\n",
    "    for site_name in site_names:\n",
    "        executor = FedXGBHistogramExecutor(data_loader_id=\"data_loader\")\n",
    "        job.to(executor, site_name, gpu=0)\n",
    "        data_loader = CreditCardDataLoader(root_dir=root_dir, file_postfix=file_postfix)\n",
    "        job.to(data_loader, site_name, id=\"data_loader\")\n",
    "```\n",
    "> file_postfix\n",
    "  file_postfix is default to \"_normalized.csv\", we are loading the normalized csv files normalized by pre-processing step. \n",
    "  the files are \n",
    "  * train__normalized.csv\n",
    "  * test__normalized.csv\n",
    "  \n",
    "\n",
    "Notice we assign defined a [```CreditCardDataLoader```](./nvflare/xgb_data_loader.py), this a XGBLoader we defined to load the credit card dataset. \n",
    "\n",
    "```\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost.core import DataSplitMode\n",
    "\n",
    "from nvflare.app_opt.xgboost.data_loader import XGBDataLoader\n",
    "\n",
    "\n",
    "class CreditCardDataLoader(XGBDataLoader):\n",
    "    def __init__(self, root_dir: str, file_postfix: str):\n",
    "        self.dataset_names = [\"train\", \"test\"]\n",
    "        self.base_file_names = {}\n",
    "        self.root_dir = root_dir\n",
    "        self.file_postfix = file_postfix\n",
    "        for name in self.dataset_names:\n",
    "            self.base_file_names[name] = name + file_postfix\n",
    "        self.numerical_columns = [\n",
    "            \"Timestamp\",\n",
    "            \"Amount\",\n",
    "            \"trans_volume\",\n",
    "            \"total_amount\",\n",
    "            \"average_amount\",\n",
    "            \"hist_trans_volume\",\n",
    "            \"hist_total_amount\",\n",
    "            \"hist_average_amount\",\n",
    "            \"x2_y1\",\n",
    "            \"x3_y2\",\n",
    "        ]\n",
    "\n",
    "    def load_data(self, client_id: str, split_mode: int) -> Tuple[xgb.DMatrix, xgb.DMatrix]:\n",
    "        data = {}\n",
    "        for ds_name in self.dataset_names:\n",
    "            print(\"\\nloading for site = \", client_id, f\"{ds_name} dataset \\n\")\n",
    "            file_name = os.path.join(self.root_dir, client_id, self.base_file_names[ds_name])\n",
    "            df = pd.read_csv(file_name)\n",
    "            data_num = len(data)\n",
    "\n",
    "            # split to feature and label\n",
    "            y = df[\"Class\"]\n",
    "            x = df[self.numerical_columns]\n",
    "            data[ds_name] = (x, y, data_num)\n",
    "\n",
    "\n",
    "        # training\n",
    "        x_train, y_train, total_train_data_num = data[\"train\"]\n",
    "        data_split_mode = DataSplitMode(split_mode)\n",
    "        dmat_train = xgb.DMatrix(x_train, label=y_train, data_split_mode=data_split_mode)\n",
    "\n",
    "        # validation\n",
    "        x_valid, y_valid, total_valid_data_num = data[\"test\"]\n",
    "        dmat_valid = xgb.DMatrix(x_valid, label=y_valid, data_split_mode=data_split_mode)\n",
    "\n",
    "        return dmat_train, dmat_valid\n",
    "```\n",
    "\n",
    "We are now ready to run all the code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036417d1-ad58-4835-b59b-fae94aafded3",
   "metadata": {},
   "source": [
    "## Run All the Jobs End-to-end\n",
    "Here we are going to run each job in sequence. For real-world use case,\n",
    "\n",
    "* prepare data is not needed, as you already have the data\n",
    "* feature enrichment / encoding scripts need to be defined based on your own technique\n",
    "* for XGBoost Job, you will need to write your own data loader \n",
    "\n",
    "Note: All Sender SICs are considered clients: they are \n",
    "* 'ZHSZUS33_Bank_1'\n",
    "* 'SHSHKHH1_Bank_2'\n",
    "* 'YXRXGB22_Bank_3'\n",
    "* 'WPUWDEFF_Bank_4'\n",
    "* 'YMNYFRPP_Bank_5'\n",
    "* 'FBSFCHZH_Bank_6'\n",
    "* 'YSYCESMM_Bank_7'\n",
    "* 'ZNZZAU3M_Bank_8'\n",
    "* 'HCBHSGSG_Bank_9'\n",
    "* 'XITXUS33_Bank_10' \n",
    "\n",
    "Total 10 banks\n",
    "\n",
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e5788-8985-4c89-ba34-987bb407be9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python3 ./utils/prepare_data.py -i ./creditcard.csv -o /tmp/nvflare/xgb/credit_card"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aa1f2c-e3ca-4ced-b94e-6a68cf4b809e",
   "metadata": {},
   "source": [
    "### Enrich data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335c190-6db1-499b-b1e3-6667675a45a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd nvflare\n",
    "! python3 enrich_job.py -c 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'FBSFCHZH_Bank_6' 'YMNYFRPP_Bank_5' 'WPUWDEFF_Bank_4' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'YSYCESMM_Bank_7' 'ZHSZUS33_Bank_1' 'HCBHSGSG_Bank_9' -p enrich.py  -a \"-i /tmp/nvflare/xgb/credit_card/ -o /tmp/nvflare/xgb/credit_card/\"\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8cba2c-9018-410b-93c2-930816d65a33",
   "metadata": {},
   "source": [
    "### Pre-Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515a542-cb96-46a8-a3ac-4eb7cee5b46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd nvflare\n",
    "! python3 pre_process_job.py -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4' -p pre_process.py -a \"-i /tmp/nvflare/xgb/credit_card  -o /tmp/nvflare/xgb/credit_card/\"\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f95e5-d104-43d3-8320-dd077d885799",
   "metadata": {},
   "source": [
    "### Construct Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775b7a4-32de-4791-b17f-bc8291a5f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd nvflare\n",
    "! python graph_construct_job.py -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4' -p graph_construct.py -a \"-i /tmp/nvflare/xgb/credit_card  -o /tmp/nvflare/xgb/credit_card/\"\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c201d73d-d0eb-4691-b4f6-f4b930168ef2",
   "metadata": {},
   "source": [
    "### GNN Training and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f64e8-072e-4f6f-8698-f513ecb47f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd nvflare\n",
    "! python gnn_train_encode_job.py -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4' -p gnn_train_encode.py -a \"-i /tmp/nvflare/xgb/credit_card  -o /tmp/nvflare/xgb/credit_card/\"\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6484fc-e226-4b1b-bc79-afbae4d2b918",
   "metadata": {},
   "source": [
    "### GNN Encoding Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8adaf-0644-4534-85ac-d83454b9ac8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 ./utils/merge_feat.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae5236b-0f40-4b91-9fc2-4f2836b52537",
   "metadata": {},
   "source": [
    "### Run XGBoost Job\n",
    "#### Without GNN embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5151e2-fad7-4982-9007-db8531b1367e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd nvflare\n",
    "! python3 xgb_job.py -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4' -i /tmp/nvflare/xgb/credit_card  -w /tmp/nvflare/workspace/xgb/credit_card/\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc1faa-bc20-4c78-8bb2-19880e98723f",
   "metadata": {},
   "source": [
    "#### With GNN embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8530b8f8-5877-4c86-b72d-e8adacbad35a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd nvflare\n",
    "! python xgb_job_embed.py -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4' -i /tmp/nvflare/xgb/credit_card  -w /tmp/nvflare/workspace/xgb/credit_card_embed\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9090a-d50a-46d7-bc8f-388717d18f96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Job for POC and Production\n",
    "\n",
    "With job running well in simulator, we are ready to run in a POC mode, so we can simulate the deployment in localhost or simply deploy to production. \n",
    "\n",
    "All we need is the job definition; we can use the job.export_job() method to generate the job configuration and export it to a given directory. For example, in xgb_job.py, we have the following\n",
    "\n",
    "```\n",
    "    if work_dir:\n",
    "        print(\"work_dir=\", work_dir)\n",
    "        job.export_job(work_dir)\n",
    "\n",
    "    if not args.config_only:\n",
    "        job.simulator_run(work_dir)\n",
    "```\n",
    "\n",
    "let's try this out and then look at the directory. We use ```tree``` command if you have it. othewise, simply use ```ls -al ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ffde2-e7b8-4666-86cb-8f579e5818da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd nvflare\n",
    "! python xgb_job.py -co -w /tmp/nvflare/workspace/xgb/credit_card/config -c 'YSYCESMM_Bank_7' 'FBSFCHZH_Bank_6' 'YXRXGB22_Bank_3' 'XITXUS33_Bank_10' 'HCBHSGSG_Bank_9' 'YMNYFRPP_Bank_5' 'ZHSZUS33_Bank_1' 'ZNZZAU3M_Bank_8' 'SHSHKHH1_Bank_2' 'WPUWDEFF_Bank_4'  -i /tmp/nvflare/xgb/credit_card  \n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6922e7e-cb15-4842-8093-ef9b030621df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tree /tmp/nvflare/workspace/xgb/credit_card/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96a68d-f0d5-4cdb-b454-78384bb5cc72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/workspace/xgb/credit_card/config/xgb_job/meta.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1743b92d-3711-4197-bb05-7cb3618d4539",
   "metadata": {},
   "source": [
    "Now we have the job definition, you can either run it in POC mode or production setup. \n",
    "\n",
    "* setup POC\n",
    "``` \n",
    "    nvfalre poc prepare -c <list of clients>\n",
    "    nvflare poc start -ex admin@nvidia.com  \n",
    "```\n",
    "  \n",
    "* submit job using NVFLARE console \n",
    "        \n",
    "    from different terminal \n",
    "   \n",
    "   ```\n",
    "   nvflare poc start -p admin@nvidia.com\n",
    "   ```\n",
    "   using submit job command\n",
    "    \n",
    "* use nvflare job submit command  to submit job\n",
    "\n",
    "* use NVFLARE API to submit job\n",
    "\n",
    "The exact same process for production. Please look at this site for documentation or tuturial examples: https://nvidia.github.io/NVFlare/\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf10abe-e7c3-4121-b312-a59897d5742e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
