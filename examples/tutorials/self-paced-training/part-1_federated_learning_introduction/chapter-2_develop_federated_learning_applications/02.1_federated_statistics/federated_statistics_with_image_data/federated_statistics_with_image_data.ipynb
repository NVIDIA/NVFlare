{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cb3afa",
   "metadata": {},
   "source": [
    "# Federated Statistics with image data\n",
    "\n",
    "In this example, we will compute local and global image statistics with the consideration that data is private at each of the client sites.\n",
    "\n",
    "## Define target statistics configuration\n",
    "\n",
    "For Image statistics, we are only interested in histogram of the image intensity, so we ignore all other statistic measures. \n",
    "\n",
    "```python\n",
    "\n",
    "statistic_configs = {\"count\": {}, \"histogram\": {\"*\": {\"bins\": 20, \"range\": [0, 256]}}}\n",
    "```\n",
    "\n",
    "## Define the local statistics generator\n",
    "\n",
    "Based on the above target statistics configuration, we can define the local statistics generator. To do this, we need to write a class that implement \n",
    "\n",
    "```python\n",
    "\n",
    "class Statistics(InitFinalComponent, ABC):\n",
    "\n",
    "    def initialize(self, fl_ctx: FLContext):\n",
    "    def pre_run(self, statistics: List[str], num_of_bins: Optional[Dict[str, Optional[int]]],bin_ranges: Optional[Dict[str, Optional[List[float]]]]):\n",
    "    def features(self) -> Dict[str, List[Feature]]:\n",
    "    def count(self, dataset_name: str, feature_name: str) -> int:\n",
    "    def sum(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def mean(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def stddev(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def variance_with_mean(self, dataset_name: str, feature_name: str, global_mean: float, global_count: float) -> float:\n",
    "    def histogram(self, dataset_name: str, feature_name: str, num_of_bins: int, global_min_value: float, global_max_value: float) -> Histogram:\n",
    "    def max_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def min_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def failure_count(self, dataset_name: str, feature_name: str) -> int:\n",
    "    def quantiles(self, dataset_name: str, feature_name: str, percentiles: List) -> Dict:\n",
    "    def finalize(self, fl_ctx: FLContext):\n",
    "\n",
    "```\n",
    "\n",
    "But since we are only interested in two metrics : Count and Histogram, we can ignore other metrics implementation and only implements count and histogram. Here is the skeleton code for this generator\n",
    "\n",
    "```python\n",
    "\n",
    "class ImageStatistics(Statistics):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    " \n",
    "    def initialize(self, fl_ctx: FLContext):\n",
    "        self.fl_ctx = fl_ctx\n",
    "        self.client_name = fl_ctx.get_identity_name()\n",
    "        \n",
    "        # call load data function \n",
    "\n",
    "    def _load_data_list(self, client_name, fl_ctx: FLContext) -> bool:\n",
    "        pass\n",
    "\n",
    "\n",
    "    def pre_run(\n",
    "        self,\n",
    "        statistics: List[str],\n",
    "        num_of_bins: Optional[Dict[str, Optional[int]]],\n",
    "        bin_ranges: Optional[Dict[str, Optional[List[float]]]],\n",
    "    ):\n",
    "        return {}\n",
    "\n",
    "    def features(self) -> Dict[str, List[Feature]]:\n",
    "        return {\"train\": [Feature(\"intensity\", DataType.FLOAT)]}\n",
    "\n",
    "    def count(self, dataset_name: str, feature_name: str) -> int:\n",
    "\n",
    "        # return number of images loaded\n",
    "        pass\n",
    "            \n",
    "\n",
    "    def failure_count(self, dataset_name: str, feature_name: str) -> int:\n",
    "\n",
    "        return self.failure_images\n",
    "\n",
    "    def histogram(\n",
    "        self, dataset_name: str, feature_name: str, num_of_bins: int, global_min_value: float, global_max_value: float\n",
    "    ) -> Histogram:\n",
    "        # do histogram calculation: \n",
    "        return Histogram(HistogramType.STANDARD, histogram_bins)\n",
    "```\n",
    "\n",
    "Here ```FLContext``` is the context of the current Job workflow, \"identity\" referring to the site identity, therefore ```get_identity_name()``` will return the site name.\n",
    "\n",
    "You can take a look of the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31517cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat code/src/image_statistics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdda4a9",
   "metadata": {},
   "source": [
    "# Define Job Configuration\n",
    " \n",
    "\n",
    "```python\n",
    "\n",
    "    statistic_configs = {\"count\": {}, \"histogram\": {\"*\": {\"bins\": 20, \"range\": [0, 256]}}}\n",
    "    \n",
    "    # define local stats generator\n",
    "    stats_generator = ImageStatistics(data_root_dir)\n",
    "\n",
    "    job = StatsJob(\n",
    "        job_name=\"stats_image\",\n",
    "        statistic_configs=statistic_configs,\n",
    "        stats_generator=stats_generator,\n",
    "        output_path=output_path,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a17f22-5667-4f99-b4f6-d49116db74b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8969bf-d010-42b5-a807-0808922402d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065b351-baac-4f84-aa15-3d875f86cb93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download data\n",
    "\n",
    "As an example, we use the dataset from the [\"COVID-19 Radiography Database\"](https://www.kaggle.com/tawsifurrahman/covid19-radiography-database).\n",
    "it contains png image files in four different classes: `COVID`, `Lung_Opacity`, `Normal`, and `Viral Pneumonia`.\n",
    "First create a temp directory, then we download and extract to `/tmp/nvflare/image_stats/data/.`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0f0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e64769-17f1-4805-9399-1c141e050065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "# prepare the directory\n",
    "\n",
    "if [ ! -d /tmp/nvflare/image_stats/data ]; then\n",
    "  mkdir -p /tmp/nvflare/image_stats/data\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519789a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"tawsifurrahman/covid19-radiography-database\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562f713-5892-43c7-a3d6-d277c337b5ea",
   "metadata": {},
   "source": [
    "Download and unzip the data (you may need to log in to Kaggle or use an API key). Once you have extracted the data from the zip file, check the directory to make sure you have the COVID-19_Radiography_Dataset directory at the following location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc68ebf-6071-479d-8cc1-15439bedea02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mv {path} /tmp/nvflare/image_stats/data/\n",
    "\n",
    "! tree /tmp/nvflare/image_stats/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94faaa6b-08fd-485c-87d5-53b4520177fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Prepare data\n",
    "\n",
    "Next, create the data lists simulating different clients with varying amounts and types of images. \n",
    "The downloaded archive contains subfolders for four different classes: `COVID`, `Lung_Opacity`, `Normal`, and `Viral Pneumonia`.\n",
    "Here we assume each class of image corresponds to a different site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ba087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! code/data/prepare_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00de5e4-4360-4fc5-a819-4eb156e56341",
   "metadata": {},
   "source": [
    "## Run Job with FL Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e972070",
   "metadata": {},
   "source": [
    "The file [image_stats_job.py](code/image_stats_job.py) uses `StatsJob` to generate a job configuration in a Pythonic way. With the default arguments, the job will be exported to `/tmp/nvflare/jobs/image_stats` and then the job will be run with the FL simulator with the `simulator_run()` command with a work_dir of `/tmp/nvflare/workspace/image_stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd code\n",
    "\n",
    "! python3 image_stats_job.py\n",
    "\n",
    "%cd -\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09aed14-5011-4418-8840-5f7c16c97534",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Examine the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf6e9a-3265-4e45-8b06-c8e543605f21",
   "metadata": {},
   "source": [
    "\n",
    "The results are stored on the server in the workspace at \"/tmp/nvflare/image_stats\" and can be accessed with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a7dd0-45d9-42ea-98b2-f72a3bbccf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! ls -al /tmp/nvflare/workspace/image_stats/server/simulate_job/statistics/image_stats.json\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd042db-6ce0-4e37-bcbe-d96051e4d164",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization\n",
    "We can visualize the results easily via the visualization notebook. Before we do that, we need to copy the data to the notebook directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c89693-37b9-450c-85dd-8a2d78fee3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cp /tmp/nvflare/workspace/image_stats/server/simulate_job/statistics/image_stats.json code/image_stats/demo/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6f632-3326-4236-902e-8c0965688d85",
   "metadata": {},
   "source": [
    "now we can visualize via the [visualization notebook](code/image_stats/demo/visualization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda06c0b-798d-480d-9b4c-a62fab95bcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## We are done !\n",
    "Congratulations, you have just completed the federated stats image histogram calculation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_env",
   "language": "python",
   "name": "nvflare_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
