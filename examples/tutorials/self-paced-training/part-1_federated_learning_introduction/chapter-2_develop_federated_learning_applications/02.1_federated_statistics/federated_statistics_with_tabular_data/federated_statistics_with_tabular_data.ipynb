{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a17f22-5667-4f99-b4f6-d49116db74b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Federated Statistics with Tabular Data\n",
    "\n",
    "Tabular data is the most common data type in the world; it is easy to understand and manipulate. In this chapter, we will explore federated statistics with tabular data. We will leverage pandas dataframe to calculate the statistics of the local data and the global data.\n",
    "\n",
    "\n",
    "To illustrate with an example, we will prepare the dependencies and prepare the dataset. \n",
    "\n",
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d44fa",
   "metadata": {},
   "source": [
    "\n",
    "### Install dependencies\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8969bf-d010-42b5-a807-0808922402d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9890a",
   "metadata": {},
   "source": [
    "\n",
    "> Sidebar: \n",
    "> **Installing fastdigest**\n",
    ">\n",
    "> If you intend to calculate quantiles, you need to install fastdigest.  the fastdigest not included in the requirements.txt file. If you are not calculating quantiles, you can skip this step.\n",
    ">\n",
    "> ```bash\n",
    "> pip install fastdigest==0.4.0\n",
    "> ```\n",
    ">\n",
    "> On Ubuntu, you might get the following error:\n",
    ">\n",
    "> ```\n",
    "> Cargo, the Rust package manager, is not installed or is not on PATH.\n",
    "> This package requires Rust and Cargo to compile extensions. Install it through\n",
    "> the system's package manager or via https://rustup.rs/\n",
    ">     \n",
    "> Checking for Rust toolchain....\n",
    "> ```\n",
    ">\n",
    "> This is because fastdigest (or its dependencies) requires Rust and Cargo to build. \n",
    ">\n",
    "> You need to install Rust and Cargo on your Ubuntu system. Follow these steps:\n",
    ">\n",
    "> 1. Install Rust and Cargo by running:\n",
    ">    ```bash\n",
    ">    cd NVFlare/examples/advanced/federated-statistics/df_stats\n",
    ">    ./install_cargo.sh\n",
    ">    ```\n",
    ">\n",
    "> 2. Then install fastdigest again:\n",
    ">    ```bash\n",
    ">    pip install fastdigest==0.4.0\n",
    ">    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94faaa6b-08fd-485c-87d5-53b4520177fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Prepare data\n",
    "\n",
    "In this example, we are using the UCI (University of California, Irvine) [adult dataset](https://archive.ics.uci.edu/dataset/2/adult)\n",
    "The original dataset already contains \"training\" and \"test\" datasets. Here we simply assume that the \"training\" and \"test\" data set each belong to a client, so we assign the adult.train dataset to site-1 and the adult.test dataset to site-2.\n",
    "```\n",
    "site-1: adult.train\n",
    "site-2: adult.test\n",
    "```\n",
    "\n",
    "Now we use the data utility to download UCI datasets to separate client package directory to /tmp/nvflare/data/ directory.\n",
    "Please note that the UCI's website may experience occasional downtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea959f-7282-4e55-bb26-11524ec47e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd code/data\n",
    "\n",
    "from prepare_data import prepare_data\n",
    "\n",
    "prepare_data(data_root_dir = \"/tmp/nvflare/df_stats/data\")\n",
    "\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5444d8f-4938-4759-bd43-831013043c23",
   "metadata": {},
   "source": [
    "Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf37d0-7555-4818-9963-ca7342161a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path =\"/tmp/nvflare/df_stats/data/site-1/data.csv\"\n",
    "data_features = [\n",
    "            \"Age\",\n",
    "            \"Workclass\",\n",
    "            \"fnlwgt\",\n",
    "            \"Education\",\n",
    "            \"Education-Num\",\n",
    "            \"Marital Status\",\n",
    "            \"Occupation\",\n",
    "            \"Relationship\",\n",
    "            \"Race\",\n",
    "            \"Sex\",\n",
    "            \"Capital Gain\",\n",
    "            \"Capital Loss\",\n",
    "            \"Hours per week\",\n",
    "            \"Country\",\n",
    "            \"Target\",\n",
    "        ]\n",
    "\n",
    "        # the original dataset has no header,\n",
    "        # we will use the adult.train dataset for site-1, the adult.test dataset for site-2\n",
    "        # the adult.test dataset has incorrect formatted row at 1st line, we will skip it.\n",
    "skip_rows = {\n",
    "            \"site-1\": [],\n",
    "            \"site-2\": [0],\n",
    "        }\n",
    "\n",
    "df= pd.read_csv(data_path, names=data_features, sep=r\"\\s*,\\s*\", skiprows=skip_rows, engine=\"python\", na_values=\"?\")\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6d572-7dc0-4cec-8382-f25555f52af9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "> Note \n",
    "We will only calculate the statistics of numerical features; categorical features will be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bb5a4",
   "metadata": {},
   "source": [
    "## Define target statistics configuration\n",
    "\n",
    "\n",
    "Let's see what statistics we want to calculate, we can capture the statistics configuration in a dictionary, the key is the statistic name, the value is the statistic configuration.\n",
    "\n",
    "```python\n",
    "\n",
    "   statistic_configs = {\n",
    "        \"count\": {},\n",
    "        \"mean\": {},\n",
    "        \"sum\": {},\n",
    "        \"stddev\": {},\n",
    "        \"histogram\": {\"*\": {\"bins\": 20}, \"Age\": {\"bins\": 10, \"range\": [0, 100]}},\n",
    "        \"quantile\": {\"*\": [0.1, 0.5, 0.9], \"Age\": [0.5, 0.9]},\n",
    "    }\n",
    "```\n",
    "\n",
    "For each statistic, we can configure to give additional instructions for each feature. While count, mean, sum and stddev are defined in such a way that the calculation will be the same for all features, for histogram, we can define different bins for each feature. \"*\" is a wildcard for all features.\n",
    "```\n",
    "\"histogram\": {\"*\": {\"bins\": 20}, \"Age\": {\"bins\": 20, \"range\": [0, 10]}},\n",
    "```\n",
    "here, we define a histogram with a histogram with 20 bins for all features, the range is not defined, which means the range will be calculated from the data.\n",
    "We also defined 10 bins and range [0, 100] for the feature \"Age\".\n",
    "\n",
    "Similarly the quantile is defined for different features with different values.  This can be specified with Job API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d17a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat code/df_stats_job.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1857b89",
   "metadata": {},
   "source": [
    "If a user only wants to calculate statistics except for quantile and histogram, then the configuration can be simplified as:\n",
    "\n",
    "```python\n",
    "\n",
    " statistic_configs = {\n",
    "        \"count\": {},\n",
    "        \"mean\": {},\n",
    "        \"sum\": {},\n",
    "        \"stddev\": {},\n",
    "  }\n",
    "\n",
    "```\n",
    "\n",
    "Similarly, the code can be changed to \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "    skip previous code\n",
    "\n",
    "\n",
    "    statistic_configs = {\n",
    "        \"count\": {},\n",
    "        \"mean\": {},\n",
    "        \"sum\": {},\n",
    "        \"stddev\": {},\n",
    "\n",
    "    }\n",
    "    # define local stats generator\n",
    "    df_stats_generator = DFStatistics(data_root_dir=data_root_dir)\n",
    "\n",
    "    job = StatsJob(\n",
    "        job_name=\"stats_df\",\n",
    "        statistic_configs=statistic_configs,\n",
    "        stats_generator=df_stats_generator,\n",
    "        output_path=output_path,\n",
    "    )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f102f6b9",
   "metadata": {},
   "source": [
    "## Define the local statistics generator\n",
    "\n",
    "Based on the above target statistics configuration, we can define the local statistics generator. To do this, we need to write a class that implement \n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "class Statistics(InitFinalComponent, ABC):\n",
    "\n",
    "    def initialize(self, fl_ctx: FLContext):\n",
    "    def pre_run(self, statistics: List[str], num_of_bins: Optional[Dict[str, Optional[int]]],bin_ranges: Optional[Dict[str, Optional[List[float]]]]):\n",
    "    def features(self) -> Dict[str, List[Feature]]:\n",
    "    def count(self, dataset_name: str, feature_name: str) -> int:\n",
    "    def sum(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def mean(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def stddev(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def variance_with_mean(self, dataset_name: str, feature_name: str, global_mean: float, global_count: float) -> float:\n",
    "    def histogram(self, dataset_name: str, feature_name: str, num_of_bins: int, global_min_value: float, global_max_value: float) -> Histogram:\n",
    "    def max_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def min_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def failure_count(self, dataset_name: str, feature_name: str) -> int:\n",
    "    def quantiles(self, dataset_name: str, feature_name: str, percentiles: List) -> Dict:\n",
    "    def finalize(self, fl_ctx: FLContext):\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "NVIDIA FLARE has implemented ```DFStatisticsCore```, which is a core class for calculating the statistics of the data frame. We can inherit this class and override the methods to calculate the statistics. Here are a few assumptions:\n",
    "\n",
    "* data can be loaded and cached in the memory.\n",
    "* data has the proper column names and can be loaded into a pandas dataframe.\n",
    "* The feature names can be obtained from the dataframe.\n",
    "\n",
    "```\n",
    "def load_data(self, fl_ctx: FLContext) -> Dict[str, pd.DataFrame]:\n",
    "```\n",
    "which loads the data into a dictionary of pandas dataframe, the key is the dataset name, the value is the pandas dataframe.\n",
    "\n",
    "Let's take a look what the user needs to do to implement the local statistics generator. With ```DFStatisticsCore```, the user needs to implement the following methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad902da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat code/src/df_statistics.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb7c51",
   "metadata": {},
   "source": [
    "# Define Job Configuration\n",
    "\n",
    "Each FLARE job is defined by a job configuration, the configuration includes configurations for the clients and server. Optionally, the job configuration also contains the customized job code. You have seen this in [Job Structure and configuration](../../../chapter-1_running_federated_learning_applications/01.6_job_structure_and_configuration/understanding_fl_job.ipynb)\n",
    "\n",
    "For now, we can define a FebJob via FLARE's Job API, here is Statistrics Job we havea predefined for you\n",
    "\n",
    "```python\n",
    "\n",
    "    job = StatsJob(\n",
    "        job_name=\"<job_name>\",\n",
    "        statistic_configs=statistic_configs,\n",
    "        stats_generator=df_stats_generator,\n",
    "        output_path=output_path,\n",
    "    )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4da328",
   "metadata": {},
   "source": [
    "For this example, we simply hardcoded the column names, but in practice, users can get the column names from files such as CSV files, parquet files, etc.\n",
    "\n",
    "\n",
    "## Run Job with FL Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d013501",
   "metadata": {},
   "source": [
    "The file [df_stats_job.py](code/df_stats_job.py) uses `StatsJob` to generate a job configuration in a Pythonic way. With the default arguments, the job will be exported to `/tmp/nvflare/jobs/stats_df` and then the job will be run with the FL simulator with the `simulator_run()` command with a work_dir of `/tmp/nvflare/jobs/stats_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361a85e-4187-433c-976c-0dc4021908ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd code\n",
    "\n",
    "! python3 df_stats_job.py \n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be8a9a-b1b8-413c-abab-5cbd7e191a0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Examine the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf6e9a-3265-4e45-8b06-c8e543605f21",
   "metadata": {},
   "source": [
    "With the default parameters, the results are stored in workspace \"/tmp/nvflare/jobs/stats_df/work_dir\"\n",
    "```\n",
    "/tmp/nvflare/jobs/stats_df/work_dir/server/simulate_job/statistics/adults_stats.json \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a7dd0-45d9-42ea-98b2-f72a3bbccf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls -al  /tmp/nvflare/jobs/stats_df/work_dir/server/simulate_job/statistics/adults_stats.json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd042db-6ce0-4e37-bcbe-d96051e4d164",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization\n",
    "We can visualize the results easly via the visualizaiton notebook. Before we do that, we need to copy the data to the notebook directory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c89693-37b9-450c-85dd-8a2d78fee3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cp  /tmp/nvflare/jobs/stats_df/work_dir/server/simulate_job/statistics/adults_stats.json  code/demo/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6f632-3326-4236-902e-8c0965688d85",
   "metadata": {},
   "source": [
    "Now we can visualize the results with the [visualization notebook](code/demo/visualization.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef94e26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fda06c0b-798d-480d-9b4c-a62fab95bcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## We are done !\n",
    "Congratulations, you just completed the federated stats calculation with data represented by a data frame.\n",
    "\n",
    "Let's move on to [federated stats with Image Data](../federated_statistics_with_image_data/federated_statistics_with_image_data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70459712",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_env",
   "language": "python",
   "name": "nvflare_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
