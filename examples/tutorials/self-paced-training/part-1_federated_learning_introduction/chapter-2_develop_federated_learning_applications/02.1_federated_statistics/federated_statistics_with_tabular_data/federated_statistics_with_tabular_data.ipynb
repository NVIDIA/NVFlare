{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a17f22-5667-4f99-b4f6-d49116db74b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Federated Statistics with Tabular Data\n",
    "\n",
    "Tabular data is one of the most common data types in the world. In this chapter, we will explore federated statistics with tabular data. We will leverage `pandas` `dataframe` to calculate the statistics of the local data and the global data.\n",
    "\n",
    "To illustrate with an example, we will prepare the dependencies and prepare the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497d44fa",
   "metadata": {},
   "source": [
    "\n",
    "### Install dependencies\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8969bf-d010-42b5-a807-0808922402d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d9890a",
   "metadata": {},
   "source": [
    "\n",
    "> Sidebar: \n",
    "> **Installing fastdigest**\n",
    ">\n",
    "> If you intend to calculate quantiles, you need to install fastdigest.  the fastdigest not included in the requirements.txt file. If you are not calculating quantiles, you can skip this step.\n",
    ">\n",
    "> ```bash\n",
    "> pip install fastdigest==0.4.0\n",
    "> ```\n",
    ">\n",
    "> On Ubuntu, you might get the following error:\n",
    ">\n",
    "> ```\n",
    "> Cargo, the Rust package manager, is not installed or is not on PATH.\n",
    "> This package requires Rust and Cargo to compile extensions. Install it through\n",
    "> the system's package manager or via https://rustup.rs/\n",
    ">     \n",
    "> Checking for Rust toolchain....\n",
    "> ```\n",
    ">\n",
    "> This is because fastdigest (or its dependencies) requires Rust and Cargo to build. \n",
    ">\n",
    "> You need to install Rust and Cargo on your Ubuntu system. Follow these steps:\n",
    ">\n",
    "> 1. Install Rust and Cargo by running:\n",
    ">    ```bash\n",
    ">    cd NVFlare/examples/advanced/federated-statistics/df_stats\n",
    ">    ./install_cargo.sh\n",
    ">    ```\n",
    ">\n",
    "> 2. Then install fastdigest again:\n",
    ">    ```bash\n",
    ">    pip install fastdigest==0.4.0\n",
    ">    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94faaa6b-08fd-485c-87d5-53b4520177fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "### Prepare data\n",
    "\n",
    "In this example, we are using the UCI (University of California, Irvine) [adult dataset](https://archive.ics.uci.edu/dataset/2/adult).\n",
    "\n",
    "The original dataset already contains \"training\" and \"test\" datasets. Here we simply assume that the \"training\" and \"test\" data set each belong to a client, so we assign the adult.train dataset to site-1 and the adult.test dataset to site-2.\n",
    "```\n",
    "site-1: adult.train\n",
    "site-2: adult.test\n",
    "```\n",
    "\n",
    "Now we use the data utility to download UCI datasets to separate client package directory to /tmp/nvflare/data/ directory.\n",
    "Please note that the UCI's website may experience occasional downtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea959f-7282-4e55-bb26-11524ec47e99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd code/data\n",
    "\n",
    "from prepare_data import prepare_data\n",
    "\n",
    "prepare_data(data_root_dir = \"/tmp/nvflare/df_stats/data\")\n",
    "\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5444d8f-4938-4759-bd43-831013043c23",
   "metadata": {},
   "source": [
    "Let's take a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cf37d0-7555-4818-9963-ca7342161a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path =\"/tmp/nvflare/df_stats/data/site-1/data.csv\"\n",
    "data_features = [\n",
    "            \"Age\",\n",
    "            \"Workclass\",\n",
    "            \"fnlwgt\",\n",
    "            \"Education\",\n",
    "            \"Education-Num\",\n",
    "            \"Marital Status\",\n",
    "            \"Occupation\",\n",
    "            \"Relationship\",\n",
    "            \"Race\",\n",
    "            \"Sex\",\n",
    "            \"Capital Gain\",\n",
    "            \"Capital Loss\",\n",
    "            \"Hours per week\",\n",
    "            \"Country\",\n",
    "            \"Target\",\n",
    "        ]\n",
    "\n",
    "        # the original dataset has no header,\n",
    "        # we will use the adult.train dataset for site-1, the adult.test dataset for site-2\n",
    "        # the adult.test dataset has incorrect formatted row at 1st line, we will skip it.\n",
    "skip_rows = {\n",
    "            \"site-1\": [],\n",
    "            \"site-2\": [0],\n",
    "        }\n",
    "\n",
    "df= pd.read_csv(data_path, names=data_features, sep=r\"\\s*,\\s*\", skiprows=skip_rows, engine=\"python\", na_values=\"?\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6d572-7dc0-4cec-8382-f25555f52af9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "> Note \n",
    "We will only calculate the statistics of numerical features; categorical features will be skipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3bb5a4",
   "metadata": {},
   "source": [
    "## Define statistics configuration\n",
    "\n",
    "We can configure each statistics, using a dictionary, where the key is a statistic's name, and the value is a statistic's configuration.\n",
    "\n",
    "```python\n",
    "\n",
    "   statistic_configs = {\n",
    "        \"count\": {},\n",
    "        \"mean\": {},\n",
    "        \"sum\": {},\n",
    "        \"stddev\": {},\n",
    "        \"histogram\": {\"*\": {\"bins\": 20}, \"Age\": {\"bins\": 10, \"range\": [0, 100]}},\n",
    "        \"quantile\": {\"*\": [0.1, 0.5, 0.9], \"Age\": [0.5, 0.9]},\n",
    "    }\n",
    "```\n",
    "\n",
    "For each statistic, we can configure to give additional instructions for each feature. While count, mean, sum and stddev are defined in such a way that the calculation will be the same for all features, for histogram, we can define different bins for each feature. \"*\" is a wildcard for all features.\n",
    "\n",
    "For example, here:\n",
    "```\n",
    "\"histogram\": {\"*\": {\"bins\": 20}, \"Age\": {\"bins\": 20, \"range\": [0, 10]}},\n",
    "```\n",
    "We will compute histograms with 20 bins for all features, and the range is not defined, which means the range will be calculated from the data. We also defined 10 bins and range [0, 100] for the feature \"Age\".\n",
    "\n",
    "Similarly the quantile is defined for different features with different values.\n",
    "\n",
    "If a user only only needs to calculate statistics except for quantile and histogram, then the configuration can be simplified as:\n",
    "\n",
    "```python\n",
    " statistic_configs = {\n",
    "        \"count\": {},\n",
    "        \"mean\": {},\n",
    "        \"sum\": {},\n",
    "        \"stddev\": {},\n",
    "  }\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f102f6b9",
   "metadata": {},
   "source": [
    "## Define the local statistics generator\n",
    "\n",
    "Based on the above target statistics configuration, we can define the local statistics generator. To do this, we need to write a class that implement \n",
    "\n",
    "```python\n",
    "class Statistics(InitFinalComponent, ABC):\n",
    "    def initialize(self, fl_ctx: FLContext):\n",
    "    def pre_run(self, statistics: List[str], num_of_bins: Optional[Dict[str, Optional[int]]],bin_ranges: Optional[Dict[str, Optional[List[float]]]]):\n",
    "    def features(self) -> Dict[str, List[Feature]]:\n",
    "    def count(self, dataset_name: str, feature_name: str) -> int:\n",
    "    def sum(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def mean(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def stddev(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def variance_with_mean(self, dataset_name: str, feature_name: str, global_mean: float, global_count: float) -> float:\n",
    "    def histogram(self, dataset_name: str, feature_name: str, num_of_bins: int, global_min_value: float, global_max_value: float) -> Histogram:\n",
    "    def max_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def min_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "    def failure_count(self, dataset_name: str, feature_name: str) -> int:\n",
    "    def quantiles(self, dataset_name: str, feature_name: str, percentiles: List) -> Dict:\n",
    "    def finalize(self, fl_ctx: FLContext):\n",
    "```\n",
    "\n",
    "NVIDIA FLARE provides a base [`DFStatisticsCore`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_opt/statistics/df/df_core_statistics.py#L28) class, which is a core class for calculating the statistics of the data frame. We can inherit this class and override the methods to calculate the statistics. Here are a few assumptions:\n",
    "\n",
    "* data can be loaded and cached in the memory.\n",
    "* data has the proper column names and can be loaded into a pandas dataframe.\n",
    "* The feature names can be obtained from the dataframe.\n",
    "\n",
    "Let's take a look our example in [code/src/df_statistics.py](code/src/df_statistics.py). We can see that, with `DFStatisticsCore`, we only need to implement the `initialize()` function, which internally calls the `load_data()` function that loads the dataset as a `pandas` `dataframe`:\n",
    "```python\n",
    "def load_data(self, fl_ctx: FLContext) -> Dict[str, pd.DataFrame]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad902da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat code/src/df_statistics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb7c51",
   "metadata": {},
   "source": [
    "# Define Job Configuration\n",
    "\n",
    "Each FLARE job is defined by a job configuration, the configuration includes configurations for the clients and server. Optionally, the job configuration also contains the customized job code. You have seen this in [Job Structure and configuration](../../../chapter-1_running_federated_learning_applications/01.6_job_structure_and_configuration/understanding_fl_job.ipynb)\n",
    "\n",
    "Similar to other examples, we can use FLARE's Job API to define and configure the job for statistics computation. FLARE provides a built-in `StatsJob` class, which inherits from the `FedJob` class.\n",
    "\n",
    "```python\n",
    "job = StatsJob(\n",
    "    job_name=\"<job_name>\",\n",
    "    statistic_configs=statistic_configs,\n",
    "    stats_generator=df_stats_generator,\n",
    "    output_path=output_path,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4da328",
   "metadata": {},
   "source": [
    "Let's take a look at our job defined in [code/df_stats_job.py](code/df_stats_job.py). \n",
    "\n",
    "Notice that we hardcoded the column names in this example for simplicity, but in practice, users can get the column names from files such as CSV files, parquet files, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d013501",
   "metadata": {},
   "source": [
    "## Run Job with FL Simulator\n",
    "\n",
    "Let's run the job.\n",
    "\n",
    "With the default arguments, the job will be exported to `/tmp/nvflare/jobs/stats_df` and then the job will be run with the FL simulator with the `simulator_run()` command with a work_dir of `/tmp/nvflare/jobs/stats_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361a85e-4187-433c-976c-0dc4021908ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd code && python3 df_stats_job.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51be8a9a-b1b8-413c-abab-5cbd7e191a0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Examine the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf6e9a-3265-4e45-8b06-c8e543605f21",
   "metadata": {},
   "source": [
    "With the default parameters, the results are stored in workspace \"/tmp/nvflare/jobs/stats_df/work_dir\"\n",
    "```\n",
    "/tmp/nvflare/jobs/stats_df/work_dir/server/simulate_job/statistics/adults_stats.json \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a7dd0-45d9-42ea-98b2-f72a3bbccf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls -al  /tmp/nvflare/jobs/stats_df/work_dir/server/simulate_job/statistics/adults_stats.json "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd042db-6ce0-4e37-bcbe-d96051e4d164",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization\n",
    "We can visualize the results easly via the visualizaiton notebook. Before we do that, we need to copy the data to the notebook directory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c89693-37b9-450c-85dd-8a2d78fee3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cp  /tmp/nvflare/jobs/stats_df/work_dir/server/simulate_job/statistics/adults_stats.json  code/demo/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6f632-3326-4236-902e-8c0965688d85",
   "metadata": {},
   "source": [
    "Now we can visualize the results with the [visualization notebook](code/demo/visualization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda06c0b-798d-480d-9b4c-a62fab95bcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## We are done !\n",
    "Congratulations, you just completed the federated stats calculation with data represented by a data frame.\n",
    "\n",
    "Let's move on to [federated stats with Image Data](../federated_statistics_with_image_data/federated_statistics_with_image_data.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
