{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c19632",
   "metadata": {},
   "source": [
    "# Converting Logistic Regression to FL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d96ed",
   "metadata": {},
   "source": [
    "## Federated Logistic Regression with Second-Order Newton-Raphson optimization\n",
    "This example shows how to implement a federated binary classification via logistic regression with second-order Newton-Raphson optimization.\n",
    "\n",
    "The [UCI Heart Disease dataset](https://archive.ics.uci.edu/dataset/45/heart+disease) is\n",
    "used in this example. Scripts are provided to download and process the\n",
    "dataset as described\n",
    "[here](https://github.com/owkin/FLamby/tree/main/flamby/datasets/fed_heart_disease).\n",
    "\n",
    "This dataset contains samples from 4 sites, splitted into training and\n",
    "testing sets as described below:\n",
    "|site         | sample split                          |\n",
    "|-------------|---------------------------------------|\n",
    "|Cleveland    | train: 199 samples, test: 104 samples |\n",
    "|Hungary      | train: 172 samples, test: 89 samples  |\n",
    "|Switzerland  | train: 30 samples, test: 16 samples   |\n",
    "|Long Beach V | train: 85 samples, test: 45 samples   |\n",
    "\n",
    "The number of features in each sample is 13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f0dcc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The [Newton-Raphson\n",
    "optimization](https://en.wikipedia.org/wiki/Newton%27s_method) problem\n",
    "can be described as follows.\n",
    "\n",
    "In a binary classification task with logistic regression, the\n",
    "probability of a data sample $x$ classified as positive is formulated\n",
    "as:\n",
    "$$p(x) = \\sigma(\\beta \\cdot x + \\beta_{0})$$\n",
    "where $\\sigma(.)$ denotes the sigmoid function. We can incorporate\n",
    "$\\beta_{0}$ and $\\beta$ into a single parameter vector $\\theta =\n",
    "( \\beta_{0},  \\beta)$. Let $d$ be the number\n",
    "of features for each data sample $x$ and let $N$ be the number of data\n",
    "samples. We then have the matrix version of the above probability\n",
    "equation:\n",
    "$$p(X) = \\sigma( X \\theta )$$\n",
    "Here $X$ is the matrix of all samples, with shape $N \\times (d+1)$,\n",
    "having it's first column filled with value 1 to account for the\n",
    "intercept $\\theta_{0}$.\n",
    "\n",
    "The goal is to compute parameter vector $\\theta$ that maximizes the\n",
    "below likelihood function:\n",
    "$$L_{\\theta} = \\prod_{i=1}^{N} p(x_i)^{y_i} (1 - p(x_i)^{1-y_i})$$\n",
    "\n",
    "The Newton-Raphson method optimizes the likelihood function via\n",
    "quadratic approximation. Omitting the maths, the theoretical update\n",
    "formula for parameter vector $\\theta$ is:\n",
    "$$\\theta^{n+1} = \\theta^{n} - H_{\\theta^{n}}^{-1} \\nabla L_{\\theta^{n}}$$\n",
    "where\n",
    "$$\\nabla L_{\\theta^{n}} = X^{T}(y - p(X))$$\n",
    "is the gradient of the likelihood function, with $y$ being the vector\n",
    "of ground truth for sample data matrix $X$,  and\n",
    "$$H_{\\theta^{n}} = -X^{T} D X$$\n",
    "is the Hessian of the likelihood function, with $D$ a diagonal matrix\n",
    "where diagonal value at $(i,i)$ is $D(i,i) = p(x_i) (1 - p(x_i))$.\n",
    "\n",
    "In federated Newton-Raphson optimization, each client will compute its\n",
    "own gradient $\\nabla L_{\\theta^{n}}$ and Hessian $H_{\\theta^{n}}$\n",
    "based on local training samples. A server will aggregate the gradients\n",
    "and Hessians computed from all clients, and perform the update of\n",
    "parameter $\\theta$ based on the theoretical update formula described\n",
    "above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32003ba9",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Using `nvflare`, The federated logistic regression with Newton-Raphson\n",
    "optimization is implemented as follows.\n",
    "\n",
    "On the server side, all workflow logics are implemented in\n",
    "class `FedAvgNewtonRaphson`, which can be found\n",
    "[here](code/newton_raphson/app/custom/newton_raphson_workflow.py). The\n",
    "`FedAvgNewtonRaphson` class inherits from the\n",
    "[`BaseFedAvg`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/base_fedavg.py)\n",
    "class, which itself inherits from the **ModelController**\n",
    "([`ModelController`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/model_controller.py))\n",
    "class. This is the preferrable approach to implement a custom\n",
    "workflow, since `ModelController` decouples communication logic from\n",
    "actual workflow (training & validation) logic. The mandatory\n",
    "method to override in `ModelController` is the\n",
    "[`run()`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/model_controller.py#L37)\n",
    "method, where the orchestration of server-side workflow actually\n",
    "happens. The implementation of `run()` method in\n",
    "[`FedAvgNewtonRaphson`](code/newton_raphson/app/custom/newton_raphson_workflow.py)\n",
    "is similar to the classic\n",
    "[`FedAvg`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/fedavg.py#L44):\n",
    "- Initialize the global model, this is acheived through method `load_model()`\n",
    "  from base class\n",
    "  [`ModelController`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/model_controller.py#L292),\n",
    "  which relies on the\n",
    "  [`ModelPersistor`](https://nvflare.readthedocs.io/en/main/glossary.html#persistor). A\n",
    "  custom\n",
    "  [`NewtonRaphsonModelPersistor`](code/newton_raphson/app/custom/newton_raphson_persistor.py)\n",
    "  is implemented in this example, which is based on the\n",
    "  [`NPModelPersistor`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/np/np_model_persistor.py)\n",
    "  for numpy data, since the _model_ in the case of logistic regression\n",
    "  is just the parameter vector $\\theta$ that can be represented by a\n",
    "  numpy array. Only the `__init__` method needs to be re-implemented\n",
    "  to provide a proper initialization for the global parameter vector\n",
    "  $\\theta$.\n",
    "- During each training round, the global model will be sent to the\n",
    "  list of participating clients to perform a training task. This is\n",
    "  done using the\n",
    "  [`send_model_and_wait()`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/model_controller.py#L41)\n",
    "  method. Once\n",
    "  the clients finish their local training, results will be collected\n",
    "  and sent back to server as\n",
    "  [`FLModel`](https://nvflare.readthedocs.io/en/main/programming_guide/fl_model.html#flmodel)s.\n",
    "- Results sent by clients contain their locally computed gradient and\n",
    "  Hessian. A [custom aggregation\n",
    "  function](code/newton_raphson/app/custom/newton_raphson_workflow.py)\n",
    "  is implemented to get the averaged gradient and Hessian, and compute\n",
    "  the Newton-Raphson update for the global parameter vector $\\theta$,\n",
    "  based on the theoretical formula shown above. The averaging of\n",
    "  gradient and Hessian is based on the\n",
    "  [`WeightedAggregationHelper`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/aggregators/weighted_aggregation_helper.py#L20),\n",
    "  which weighs the contribution from each client based on the number\n",
    "  of local training samples. The aggregated Newton-Raphson update is\n",
    "  returned as an `FLModel`.\n",
    "- After getting the aggregated Newton-Raphson update, an\n",
    "  [`update_model()`](code/newton_raphson/app/custom/newton_raphson_workflow.py#L172)\n",
    "  method is implemented to actually apply the Newton-Raphson update to\n",
    "  the global model.\n",
    "- The last step is to save the updated global model, again through\n",
    "  the `NewtonRaphsonModelPersistor` using `save_model()`.\n",
    "\n",
    "\n",
    "On the client side, the local training logic is implemented\n",
    "[here](code/newton_raphson/app/custom/newton_raphson_train.py). The\n",
    "implementation is based on the [`Client\n",
    "API`](https://nvflare.readthedocs.io/en/main/programming_guide/execution_api_type.html#client-api). This\n",
    "allows user to add minimum `nvflare`-specific code to turn a typical\n",
    "centralized training script into a federated client side local training\n",
    "script.\n",
    "- During local training, each client receives a copy of the global\n",
    "  model, sent by the server, using `flare.receive()` from the Client API.\n",
    "  The received global model is an instance of `FLModel`.\n",
    "- A local validation is first performed, where validation metrics\n",
    "  (accuracy and precision) are streamed to server using the\n",
    "  [`SummaryWriter`](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.client.tracking.html#nvflare.client.tracking.SummaryWriter). The\n",
    "  streamed metrics can be loaded and visualized using tensorboard.\n",
    "- Then each client computes it's gradient and Hessian based on local\n",
    "  training data, using their respective theoretical formula described\n",
    "  above. This is implemented in the\n",
    "  [`train_newton_raphson()`](code/newton_raphson/app/custom/newton_raphson_train.py#L82)\n",
    "  method. Each client then sends the computed results (always in\n",
    "  `FLModel` format) to server for aggregation, using the Client API call\n",
    "  `flare.send()`.\n",
    "\n",
    "Each client site corresponds to a site listed in the data table above.\n",
    "\n",
    "A [centralized training script](code/train_centralized.py) is also\n",
    "provided, which allows for comparing the federated Newton-Raphson\n",
    "optimization versus the centralized version. In the centralized\n",
    "version, training data samples from all 4 sites were concatenated into\n",
    "a single matrix, used to optimize the model parameters. The\n",
    "optimized model was then tested separately on testing data samples of\n",
    "the 4 sites, using accuracy and precision as metrics.\n",
    "\n",
    "Comparing the federated [client-side training\n",
    "code](code/newton_raphson/app/custom/newton_raphson_train.py) with the\n",
    "centralized [training code](code/train_centralized.py), we can see that\n",
    "the training logic remains similar: load data, perform training\n",
    "(Newton-Raphson updates), and valid trained model. The only added\n",
    "differences in the federated code are related to interaction with the\n",
    "FL system, such as receiving and send `FLModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc55e0",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04911ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea8504",
   "metadata": {},
   "source": [
    "## Download and prepare data\n",
    "\n",
    "Execute the following script\n",
    "```\n",
    "bash ./code/data/prepare_heart_disease_data.sh\n",
    "```\n",
    "This will download the heart disease dataset under\n",
    "`/tmp/flare/dataset/heart_disease_data/`\n",
    "\n",
    "Please note that you may need to accept the data terms in order to complete the download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548b466",
   "metadata": {},
   "source": [
    "## Centralized Logistic Regression\n",
    "\n",
    "Launch the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 code/train_centralized.py --solver custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa666b79",
   "metadata": {},
   "source": [
    "Two implementations of logistic regression are provided in the\n",
    "centralized training script, which can be specified by the `--solver`\n",
    "argument:\n",
    "- One is using `sklearn.LogisticRegression` with the `newton-cholesky`\n",
    "  solver\n",
    "- The other one is manually implemented using the theoretical update\n",
    "  formulas described above.\n",
    "\n",
    "Both implementations were tested to converge in 4 iterations and to\n",
    "give the same result.\n",
    "\n",
    "Example output:\n",
    "```\n",
    "using solver: custom\n",
    "loading training data.\n",
    "training data X loaded. shape: (486, 13)\n",
    "training data y loaded. shape: (486, 1)\n",
    "\n",
    "site - 1\n",
    "validation set n_samples:  104\n",
    "accuracy: 0.75\n",
    "precision: 0.7115384615384616\n",
    "\n",
    "site - 2\n",
    "validation set n_samples:  89\n",
    "accuracy: 0.7528089887640449\n",
    "precision: 0.6122448979591837\n",
    "\n",
    "site - 3\n",
    "validation set n_samples:  16\n",
    "accuracy: 0.75\n",
    "precision: 1.0\n",
    "\n",
    "site - 4\n",
    "validation set n_samples:  45\n",
    "accuracy: 0.6\n",
    "precision: 0.9047619047619048\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72ef2b",
   "metadata": {},
   "source": [
    "## Federated Logistic Regression\n",
    "\n",
    "Execute the following command to launch federated logistic\n",
    "regression. This will run in `nvflare`'s simulator mode.\n",
    "```\n",
    "nvflare simulator -w ./workspace -n 4 -t 4 job/newton_raphson/\n",
    "```\n",
    "\n",
    "Accuracy and precision for each site can be viewed in Tensorboard:\n",
    "```\n",
    "tensorboard --logdir=./workspace/server/simulate_job/tb_events\n",
    "```\n",
    "As can be seen from the figure below, per-site evaluation metrics in\n",
    "federated logistic regression are on-par with the centralized version.\n",
    "\n",
    "<img src=\"./code/figs/tb-metrics.png\" alt=\"Tensorboard metrics server\"/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
