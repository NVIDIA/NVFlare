{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58149c32",
   "metadata": {},
   "source": [
    "# Transform Existing Code to FL Easily with the FLARE Client API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06203527",
   "metadata": {},
   "source": [
    "The FLARE Client API provides an easy way to convert centralized, local training code into federated learning code with just a few lines of code changes.\n",
    "\n",
    "Most of the previous examples up this point have already been using the Client API, but in this section we focus on the core concepts of the Client API and explain some of the ways it can be configured to help you use the Client API more effectively.\n",
    "\n",
    "You can see the detailed examples with actual integration with deep learing platforms including PyTorch and TensorFlow here: https://github.com/NVIDIA/NVFlare/tree/main/examples/hello-world/ml-to-fl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7efa36",
   "metadata": {},
   "source": [
    "## Core Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76102eac",
   "metadata": {},
   "source": [
    "The general structure of the popular federated learning (FL) workflow, \"FedAvg\" is as follows:\n",
    "\n",
    "1. **FL server initializes an initial model**\n",
    "2. **For each round (global iteration):**\n",
    "    1. FL server sends the global model to clients\n",
    "    2. Each FL client starts with this global model and trains on their own data\n",
    "    3. Each FL client sends back their trained model\n",
    "    4. FL server aggregates all the models and produces a new global model\n",
    "\n",
    "On the client side, the training workflow is as follows:\n",
    "\n",
    "1. Receive the model from the FL server\n",
    "2. Perform local training on the received global model and/or evaluate the received global model for model selection\n",
    "3. Send the new model back to the FL server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e2b7dd",
   "metadata": {},
   "source": [
    "To convert a centralized training code to federated learning, we need to\n",
    "adapt the code to do the following steps:\n",
    "\n",
    "1. Obtain the required information from the received `fl_model`\n",
    "2. Run local training\n",
    "3. Put the results in a new `fl_model` to be sent back\n",
    "\n",
    "For a general use case, there are three essential methods for the Client API:\n",
    "\n",
    "* ``init()``: Initializes NVFlare Client API environment.\n",
    "* ``receive()``: Receives model from NVFlare side.\n",
    "* ``send()``: Sends the model to NVFlare side.\n",
    "\n",
    "You can use the Client API to change centralized training code to\n",
    "federated learning, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f21ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvflare.client as flare\n",
    "\n",
    "flare.init() # 1. Initializes NVFlare Client API environment.\n",
    "input_model = flare.receive() # 2. Receives model from NVFlare side.\n",
    "params = input_model.params # 3. Obtain the required information from received FLModel\n",
    "\n",
    "# original local training code begins\n",
    "new_params = local_train(params)\n",
    "# original local training code ends\n",
    "\n",
    "output_model = flare.FLModel(params=new_params) # 4. Put the results in a new FLModel\n",
    "flare.send(output_model) # 5. Sends the model to NVFlare side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e4079",
   "metadata": {},
   "source": [
    "With 5 lines of code changes, we convert the centralized training code to work in a\n",
    "federated learning setting.\n",
    "\n",
    "After this, we can use the job templates and the Job CLI\n",
    "to generate a job and export it to run on a deployed NVFlare system or directly run the job using FL Simulator.\n",
    "\n",
    "To see a table of the key Client APIs, see the [Client API documentation in the programming guide](https://nvflare.readthedocs.io/en/main/programming_guide/execution_api_type/client_api.html#id2).\n",
    "\n",
    "Please consult the [Client API Module](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.client.api.html) for more in-depth information about all of the Client API functions.\n",
    "\n",
    "If you are using PyTorch Lightning in your training code, you can check the [Lightning API Module](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.app_opt.lightning.api.html). Also, be sure to look through the [Convert Torch Lightning to FL notebook](../02.2_convert_torch_lightning_to_federated_learning/convert_torch_lightning_to_fl.ipynb) and related code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09d80e",
   "metadata": {},
   "source": [
    "## Advanced User Options: Client API with Different Implementations\n",
    "\n",
    "Within the Client API, we offer multiple implementations tailored to diverse requirements:\n",
    "\n",
    "* In-process Client API: In this setup, the client training script operates within the same process as the NVFlare Client job.\n",
    "This configuration, utilizing the ```InProcessClientAPIExecutor```, offers shared memory usage and is efficient with simple configuration. \n",
    "This is the default for `ScriptRunner` since by default `launch_external_process=False`. Use this configuration for development or single GPU training.\n",
    "\n",
    "* Sub-process Client API: Here, the client training script runs in a separate subprocess.\n",
    "Utilizing the ```ClientAPILauncherExecutor```, this option offers flexibility in communication mechanisms:\n",
    "  * Communication via CellPipe (default)\n",
    "  * Communication via FilePipe (no capability to stream metrics for experiment tracking) \n",
    "This configuration is ideal for scenarios requiring multi-GPU or distributed PyTorch training.\n",
    "\n",
    "Choose the option best suited to your specific requirements and workflow preferences.\n",
    "\n",
    "These implementations can be easily configured using the JobAPI's `ScriptRunner`.\n",
    "By default, the ```InProcessClientAPIExecutor``` is used, however setting `launch_external_process=True` uses the ```ClientAPILauncherExecutor```\n",
    "with pre-configured CellPipes for communication and metrics streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac92dd",
   "metadata": {},
   "source": [
    "## NVFlare Client API Job with NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a4b34",
   "metadata": {},
   "source": [
    "In this example we use simple NumPy scripts to showcase the Client API with the `ScriptRunner` for both in-process and sub-process settings. With NumPy, only nvflare is needed so you do not have to install any additional dependencies.\n",
    "\n",
    "The default mode of the `ScriptRunner` uses `InProcessClientAPIExecutor` with the client training script operating within the same process as the NVFlare Client job. Below, we show a script that sends back full model parameters and then one that sends back model parameters differences before explaining metrics streaming and then showing how to launch those same scripts with the Sub-process Client API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af7da6",
   "metadata": {},
   "source": [
    "### Send model parameters back to the NVFlare server\n",
    "\n",
    "We use the mock training script in [train_full.py](code/src/train_full.py)\n",
    "and send back the FLModel with `params_type=\"FULL\"`.\n",
    "\n",
    "After we modify our training script, we can create a job using the ScriptRunner: [np_client_api_job.py](code/np_client_api_job.py).\n",
    "\n",
    "The script will run the job using the simulator with the Job API by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952cab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 code/np_client_api_job.py --script code/src/train_full.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076b478",
   "metadata": {},
   "source": [
    "To instead export the job configuration to use in other modes, run the script with the flag `--export_config`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdceecd",
   "metadata": {},
   "source": [
    "### Send model parameters differences back to the NVFlare server\n",
    "\n",
    "We can send model parameter differences back to the NVFlare server by calculating the parameters differences and sending it back: [train_diff.py](code/src/train_diff.py)\n",
    "\n",
    "Note that we set the `params_type` to `DIFF` when creating `flare.FLModel`.\n",
    "\n",
    "Then we can run it using the NVFlare Simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737d8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 code/np_client_api_job.py --script code/src/train_diff.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550479f2",
   "metadata": {},
   "source": [
    "### Metrics streaming\n",
    "\n",
    "We already showed an example with metrics streaming in section 01.5 of Chapter 1 in Part 1, but this is a simple example with the Client API for streaming the training progress to the server with `MLflowWriter`.\n",
    "\n",
    "NVFlare supports the following writers:\n",
    "\n",
    "  - `SummaryWriter` mimics Tensorboard `SummaryWriter`'s `add_scalar`, `add_scalars` method\n",
    "  - `WandBWriter` mimics Weights And Biases's `log` method\n",
    "  - `MLflowWriter` mimics MLflow's tracking api\n",
    "\n",
    "In this example we use `MLflowWriter` in [train_metrics.py](code/src/train_metrics.py) and configure a corresponding `MLflowReceiver` in the job script [np_client_api_job.py](code/np_client_api_job.py)\n",
    "\n",
    "Then we can run it using the NVFlare Simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 code/np_client_api_job.py --script code/src/train_metrics.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3774d943",
   "metadata": {},
   "source": [
    "After the experiment is finished, you can view the results by running the the mlflow command: `mlflow ui --port 5000` inside the directory `/tmp/nvflare/jobs/workdir/server/simulate_job/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadee3dd",
   "metadata": {},
   "source": [
    "## Sub-process Client API\n",
    "\n",
    "The `ScriptRunner` with `launch_external_process=True` uses the `ClientAPILauncherExecutor` for external process script execution.\n",
    "This configuration is ideal for scenarios requiring third-party integrations, multi-GPU or distributed PyTorch training, or if additional processes are needed for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade375cd",
   "metadata": {},
   "source": [
    "### Launching the script\n",
    "\n",
    "When launching a script in an external process, it is launched once for the entire job.\n",
    "We must ensure our training script [train_full.py](code/src/train_full.py) is in a loop to support this.\n",
    "\n",
    "Then we can run it using the NVFlare Simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ee641",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 code/np_client_api_job.py --script code/src/train_full.py --launch_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5006b87a",
   "metadata": {},
   "source": [
    "### Metrics streaming\n",
    "\n",
    "In this example we use `MLflowWriter` in [train_metrics.py](code/src/train_metrics.py) and configure a corresponding `MLflowReceiver` in the job script [np_client_api_job.py](code/np_client_api_job.py)\n",
    "\n",
    "Then we can run it using the NVFlare Simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071834d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 np_client_api_job.py --script src/train_metrics.py --launch_process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95504253",
   "metadata": {},
   "source": [
    "If you want to see example code with actual integration with PyTorch and TensorFlow, you can find it in the [Hello World ML to FL](https://github.com/NVIDIA/NVFlare/tree/main/examples/hello-world/ml-to-fl) section of the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e633e",
   "metadata": {},
   "source": [
    "With this, we are at the end of Chapter 2. The [next notebook](../02.5_recap/recap.ipynb) is a reacap of this chapter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
