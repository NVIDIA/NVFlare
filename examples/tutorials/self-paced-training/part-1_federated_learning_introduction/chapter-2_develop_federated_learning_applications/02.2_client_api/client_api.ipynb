{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58149c32",
   "metadata": {},
   "source": [
    "# Transform Existing Code to FL Easily with the FLARE Client API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06203527",
   "metadata": {},
   "source": [
    "The FLARE Client API offers a straightforward path to transform your existing machine learning or deep learning code into federated learning applications. With just a few lines of code changes, you can adapt your training logic without restructuring your codebase or moving code into different class methods. This flexibility applies to both traditional machine learning and deep learning frameworks. For PyTorch Lightning users, the process is even more streamlined with dedicated Lightning API support.\n",
    "\n",
    "You can see detailed examples with actual integration across different platforms including PyTorch and TensorFlow [here:](https://github.com/NVIDIA/NVFlare/tree/main/examples/hello-world/ml-to-fl)\n",
    "\n",
    "In Chapter 1, you have already seen the Client API in action with pytorch. In this section, we will focus on the core concepts of the Client API and explain some of the ways it can be configured to help you use the Client API more effectively.\n",
    "\n",
    "Then we will see how to use the Client API with PyTorch Lightning, and traditional machine learning algorithms such as Logistic Regression, KMeans and survival analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7efa36",
   "metadata": {},
   "source": [
    "## Core Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76102eac",
   "metadata": {},
   "source": [
    "The general workflow of the popular federated learning (FL) follows the following steps:\n",
    "\n",
    "1. **FL server initializes an initial model**\n",
    "2. **For each round (global iteration):**\n",
    "    * FL server broadcasts the global model to clients\n",
    "    * Each FL client starts with this global model and perform the local training on their own data\n",
    "    * Each FL client, then sends back their newly trained model to the FL server\n",
    "    * FL server aggregates all the local models and produces a new global model\n",
    "\n",
    "On the client side, the training workflow is as follows:\n",
    "\n",
    "1. Receive the model from the FL server\n",
    "2. Perform local training on the received global model and/or evaluate the received global model for model selection\n",
    "3. Send the new model back to the FL server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e2b7dd",
   "metadata": {},
   "source": [
    "To convert a centralized training code to federated learning, we need to\n",
    "adapt the code to do the following steps:\n",
    "\n",
    "\n",
    "1. Obtain the required information from the received `FLModel`\n",
    "2. Run local training\n",
    "3. Put the results in a new `FLModel` to be sent back\n",
    "\n",
    "For a general use case, there are three essential methods for the Client API:\n",
    "\n",
    "* ``init()``: Initializes NVFlare Client API environment.\n",
    "* ``receive()``: Receives model from NVFlare side.\n",
    "* ``send()``: Sends the model to NVFlare side.\n",
    "\n",
    "Where `FLModel` is a data structure like this:\n",
    "\n",
    "```python\n",
    "\n",
    "class FLModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        params_type: Union[None, str, ParamsType] = None,\n",
    "        params: Any = None,\n",
    "        optimizer_params: Any = None,\n",
    "        metrics: Optional[Dict] = None,\n",
    "        start_round: Optional[int] = 0,\n",
    "        current_round: Optional[int] = None,\n",
    "        total_rounds: Optional[int] = None,\n",
    "        meta: Optional[Dict] = None,\n",
    "    ):\n",
    "        \"\"\"FLModel is a standardize data structure for NVFlare to communicate with external systems.\n",
    "\n",
    "        Args:\n",
    "            params_type: type of the parameters. It only describes the \"params\".\n",
    "                If params_type is None, params need to be None.\n",
    "                If params is provided but params_type is not provided, then it will be treated as FULL.\n",
    "            params: model parameters, for example: model weights for deep learning.\n",
    "            optimizer_params: optimizer parameters.\n",
    "                For many cases, the optimizer parameters don't need to be transferred during FL training.\n",
    "            metrics: evaluation metrics such as loss and scores.\n",
    "            current_round: the current FL rounds. A round means round trip between client/server during training.\n",
    "                None for inference.\n",
    "            total_rounds: total number of FL rounds. A round means round trip between client/server during training.\n",
    "                None for inference.\n",
    "            meta: metadata dictionary used to contain any key-value pairs to facilitate the process.\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "You can use the Client API to change centralized training code to\n",
    "federated learning, for example:\n",
    "\n",
    "```python\n",
    "\n",
    "import nvflare.client as flare\n",
    "\n",
    "flare.init() # 1. Initializes NVFlare Client API environment.\n",
    "input_model = flare.receive() # 2. Receives model from NVFlare side.\n",
    "params = input_model.params # 3. Obtain the required information from received FLModel\n",
    "\n",
    "# original local training code begins\n",
    "\n",
    "new_params = trainer.fit(params)\n",
    "\n",
    "# original local training code ends\n",
    "\n",
    "output_model = flare.FLModel(params=new_params) # 4. Put the results in a new FLModel\n",
    "flare.send(output_model) # 5. Sends the model to NVFlare side.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494e4079",
   "metadata": {},
   "source": [
    "With 5 lines of code changes, we convert the centralized training code to work in a\n",
    "federated learning setting.\n",
    "\n",
    "After this, we can use the job templates and the Job CLI\n",
    "to generate a job and export it to run on a deployed NVFlare system or directly run the job using FL Simulator.\n",
    "\n",
    "To see a table of the key Client APIs, see the [Client API documentation in the programming guide](https://nvflare.readthedocs.io/en/main/programming_guide/execution_api_type/client_api.html#id2).\n",
    "\n",
    "Please consult the [Client API Module](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.client.api.html) for more in-depth information about all of the Client API functions.\n",
    "\n",
    "If you are using PyTorch Lightning in your training code, you can check the [Lightning API Module](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.app_opt.lightning.api.html). Also, be sure to look through the [Convert Torch Lightning to FL notebook](../02.2_client_api/convert_torch_lightning_to_federated_learning/convert_torch_lightning_to_fl.ipynb) and related code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a09d80e",
   "metadata": {},
   "source": [
    "## Client API with Different Implementations\n",
    "\n",
    "Within the Client API, we offer multiple implementations tailored to diverse requirements:\n",
    "\n",
    "* In-process Client API: efficient for single GPU training\n",
    "* Sub-process Client API: flexible for multi-GPU or distributed PyTorch training\n",
    "\n",
    "\n",
    "\n",
    "### In-process Client API\n",
    "\n",
    "In this setup, the client training script operates within the same process as the NVFlare Client job. This configuration, utilizing the ```InProcessClientAPIExecutor```, offers shared memory usage and is efficient with simple configuration. \n",
    "This is the default for `ScriptRunner` since by default `launch_external_process=False`. Use this configuration for development or single GPU training.\n",
    "\n",
    "### Sub-process Client API: \n",
    "\n",
    "Here, the client training script runs in a separate subprocess.\n",
    "\n",
    "Utilizing the ```ClientAPILauncherExecutor```, this option offers flexibility in communication mechanisms:\n",
    "  * Communication via CellPipe (default)\n",
    "  * Communication via FilePipe (no capability to stream metrics for experiment tracking) \n",
    "\n",
    "This configuration is ideal for scenarios requiring multi-GPU or distributed PyTorch training.\n",
    "\n",
    "Choose the option best suited to your specific requirements and workflow preferences.\n",
    "\n",
    "\n",
    "## Client API communication patterns\n",
    "\n",
    "We have two different implementations of the Client API tailored to different scenarios, each linked with distinct communication patterns.\n",
    "\n",
    "Broadly, we present in-process and sub-process executors. The in-process executor entails both training scripts and client executor operating within the same process.\n",
    "\n",
    "\n",
    "On the other hand, the LauncherExecutor employs a sub-process to execute training scripts, leading to the client executor and training scripts residing in separate processes. Communication between them is facilitated by either CellPipe (default) or FilePipe.\n",
    "\n",
    "<img src=\"./client_api_communication_pattern.png\" alt=\"Client API communication patterns\" width=\"80%\">\n",
    "\n",
    "\n",
    "\n",
    "### Choice of different Pipes\n",
    "We suggest using the default setting with CellPipe for most users.\n",
    "\n",
    "CellPipe facilitates TCP-based cell-to-cell connections between the Executor and training script processes on the local host. The term cell represents logical endpoints. This communication enables the exchange of models, metrics, and metadata between the two processes.\n",
    "\n",
    "In contrast, FilePipe offers file-based communication between the Executor and training script processes, utilizing a job-specific file directory for exchanging models and metadata via files. While FilePipe is easier to set up than CellPipe, it’s not suitable for high-frequency metrics exchange. On the other hand, FilePipe might be a better choice for scenarios where the training script is running on a remote machine and the client executor is running on the local machine.\n",
    "\n",
    "\n",
    "## Client API Examples\n",
    "\n",
    "All implementations can be easily configured using the JobAPI's `ScriptRunner`. By default, the in-process is used, however setting `launch_external_process=True` uses the sub-process with pre-configured CellPipes for communication and metrics streaming.\n",
    "\n",
    "To find out more about the Client API, and its pipe configurations, please refer to the [Client API](https://nvflare.readthedocs.io/en/2.4/programming_guide/execution_api_type/client_api.html). In this tutorial, we will only use the in-process for simplicity.  You can follow the examples in [ML-To-FL](https://github.com/NVIDIA/NVFlare/tree/main/examples/hello-world/ml-to-fl) for sub-processtraining. with different pipe training. \n",
    "\n",
    "In the following sections, we will see how to use the Client API with PyTorch Lightning and machine learning algorithms.\n",
    "\n",
    "\n",
    "* [Convert PyTorch lightning to federated learning](../02.3_convert_torch_lightning_to_federated_learning/convert_torch_lightning_to_fl.ipynb)\n",
    "\n",
    "* [Convert logistic regression to federated learning](../02.4_convert_machine_learning_to_federated_learning/02.3.1_convert_logistic_regression_to_federated_learning/convert_logistic_regression_to_fl.ipynb)\n",
    "\n",
    "* [Convert Kmeans to federated learning](../02.4_convert_machine_learning_to_federated_learning/02.3.2_convert_kmeans_to_federated_learning/convert_kmeans_to_fl.ipynb)\n",
    "\n",
    "* [Convert survival analysis to federated learning](../02.4_convert_machine_learning_to_federated_learning/02.3.3_convert_survival_analysis_to_federated_learning/convert_survival_analysis_to_fl.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffcd761",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
