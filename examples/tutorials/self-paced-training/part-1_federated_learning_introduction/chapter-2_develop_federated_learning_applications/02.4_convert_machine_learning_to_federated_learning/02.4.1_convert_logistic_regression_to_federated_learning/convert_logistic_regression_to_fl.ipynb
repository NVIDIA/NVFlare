{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e8c19632",
      "metadata": {},
      "source": [
        "# Converting Logistic Regression to Federated Learning\n",
        "\n",
        "\n",
        "Logistic regression is a fundamental classification algorithm that models the probability of a binary outcome. Despite its name, it's used for classification rather than regression. The model uses the logistic (sigmoid) function to transform a linear combination of features into a probability between 0 and 1.\n",
        "\n",
        "The Newton-Raphson method is a powerful second-order optimization technique that uses both first-order (gradient) and second-order (Hessian) information to find the optimal model parameters. Unlike first-order methods like gradient descent, Newton's method incorporates curvature information through the Hessian matrix, often leading to faster convergence, especially near the optimum.\n",
        "\n",
        "In this section, we will convert logistics regression with the 2nd order Newton-Raphson optimization to Federated Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f9d96ed",
      "metadata": {},
      "source": [
        "## Federated Logistic Regression with Second-Order Newton-Raphson optimization\n",
        "This example shows how to implement a federated binary classification via logistic regression with second-order Newton-Raphson optimization.\n",
        "\n",
        "The [UCI Heart Disease dataset](https://archive.ics.uci.edu/dataset/45/heart+disease) is\n",
        "used in this example. Scripts are provided to download and process the\n",
        "dataset as described\n",
        "[here](https://github.com/owkin/FLamby/tree/main/flamby/datasets/fed_heart_disease).\n",
        "\n",
        "This dataset contains samples from 4 sites, splitted into training and\n",
        "testing sets as described below:\n",
        "|site         | sample split                          |\n",
        "|-------------|---------------------------------------|\n",
        "|Cleveland    | train: 199 samples, test: 104 samples |\n",
        "|Hungary      | train: 172 samples, test: 89 samples  |\n",
        "|Switzerland  | train: 30 samples, test: 16 samples   |\n",
        "|Long Beach V | train: 85 samples, test: 45 samples   |\n",
        "\n",
        "The number of features in each sample is 13."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e54f0dcc",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "The [Newton-Raphson\n",
        "optimization](https://en.wikipedia.org/wiki/Newton%27s_method) problem\n",
        "can be described as follows.\n",
        "\n",
        "In a binary classification task with logistic regression, the\n",
        "probability of a data sample $x$ classified as positive is formulated\n",
        "as:\n",
        "$$p(x) = \\sigma(\\beta \\cdot x + \\beta_{0})$$\n",
        "where $\\sigma(.)$ denotes the sigmoid function. We can incorporate\n",
        "$\\beta_{0}$ and $\\beta$ into a single parameter vector $\\theta =\n",
        "( \\beta_{0},  \\beta)$. Let $d$ be the number\n",
        "of features for each data sample $x$ and let $N$ be the number of data\n",
        "samples. We then have the matrix version of the above probability\n",
        "equation:\n",
        "$$p(X) = \\sigma( X \\theta )$$\n",
        "Here $X$ is the matrix of all samples, with shape $N \\times (d+1)$,\n",
        "having it's first column filled with value 1 to account for the\n",
        "intercept $\\theta_{0}$.\n",
        "\n",
        "The goal is to compute parameter vector $\\theta$ that maximizes the\n",
        "below likelihood function:\n",
        "$$L_{\\theta} = \\prod_{i=1}^{N} p(x_i)^{y_i} (1 - p(x_i)^{1-y_i})$$\n",
        "\n",
        "The Newton-Raphson method optimizes the likelihood function via\n",
        "quadratic approximation. Omitting the maths, the theoretical update\n",
        "formula for parameter vector $\\theta$ is:\n",
        "$$\\theta^{n+1} = \\theta^{n} - H_{\\theta^{n}}^{-1} \\nabla L_{\\theta^{n}}$$\n",
        "where\n",
        "$$\\nabla L_{\\theta^{n}} = X^{T}(y - p(X))$$\n",
        "is the gradient of the likelihood function, with $y$ being the vector\n",
        "of ground truth for sample data matrix $X$,  and\n",
        "$$H_{\\theta^{n}} = -X^{T} D X$$\n",
        "is the Hessian of the likelihood function, with $D$ a diagonal matrix\n",
        "where diagonal value at $(i,i)$ is $D(i,i) = p(x_i) (1 - p(x_i))$.\n",
        "\n",
        "In federated Newton-Raphson optimization, each client will compute its\n",
        "own gradient $\\nabla L_{\\theta^{n}}$ and Hessian $H_{\\theta^{n}}$\n",
        "based on local training samples. A server will aggregate the gradients\n",
        "and Hessians computed from all clients, and perform the update of\n",
        "parameter $\\theta$ based on the theoretical update formula described\n",
        "above."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3fc55e0",
      "metadata": {},
      "source": [
        "## Install requirements\n",
        "First, install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04911ca3",
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install -r code/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33ea8504",
      "metadata": {},
      "source": [
        "## Download and prepare data\n",
        "\n",
        "Execute the following script\n",
        "```\n",
        "bash ./code/data/prepare_heart_disease_data.sh\n",
        "```\n",
        "This will download the heart disease dataset under\n",
        "`/tmp/flare/dataset/heart_disease_data/`\n",
        "\n",
        "Please note that you may need to accept the data terms in order to complete the download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b395c0d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note: the download site remembers your download history and aborts the 2nd download attempt. \n",
        "! echo y | bash ./code/data/prepare_heart_disease_data.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61e13343",
      "metadata": {},
      "outputs": [],
      "source": [
        "! ls -al /tmp/flare/dataset/heart_disease_data/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d548b466",
      "metadata": {},
      "source": [
        "## Centralized Logistic Regression\n",
        "\n",
        "Two implementations of logistic regression are provided in the\n",
        "centralized training script, which can be specified by the `--solver`\n",
        "argument:\n",
        "- One is using `sklearn.LogisticRegression` with the `newton-cholesky`\n",
        "  solver\n",
        "- The other one is manually implemented using the theoretical update\n",
        "  formulas described above.\n",
        "\n",
        "Both implementations were tested to converge in 4 iterations and to\n",
        "give the same result.\n",
        "\n",
        "Launch the following script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c68fe1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd code\n",
        "! python3 train_centralized.py --solver custom\n",
        "%cd -"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c8d278",
      "metadata": {},
      "source": [
        "## Federated Logistic Regression\n",
        "\n",
        "\n",
        "To convert the centralized logistic regression to federated learning, we need to do the following:\n",
        "\n",
        "1. Decide what model parameters will be transmitted between the server and clients\n",
        "2. Define the workflow that orchestrates the federated learning process\n",
        "3. Define how to load the initial model on the server side\n",
        "4. Modify the client-side training logic to handle models received from the server\n",
        "5. Implement the aggregation logic for the gradients and Hessians computed by the clients\n",
        "6. Configure the job via FLARE's Job API\n",
        "\n",
        "Let's examine each step.\n",
        "\n",
        "### Model Parameters\n",
        "\n",
        "We decided to simply capture the model parameters in the FLModel:\n",
        "\n",
        "```python\n",
        "\n",
        "model = FLModel(params={\"gradient\": gradient, \"hessian\": hessian})\n",
        "```\n",
        "\n",
        "We could optionally use FLModel.optimizer_params to store the Hessian, but either approach works.\n",
        "\n",
        "We add a few metadata fields to help with the training process. We use the training sample size as the weight, storing this information in the metadata:\n",
        "\n",
        "```python\n",
        "\n",
        "model = FLModel(params=result_dict, params_type=ParamsType.FULL)\n",
        "model.meta[\"sample_size\"] = data[\"train_X\"].shape[0]\n",
        "```\n",
        "\n",
        "### Server-Side Workflow\n",
        "\n",
        "NVFlare now provides a standardized workflow for Federated Logistic Regression with Newton-Raphson optimization.\n",
        "We use the `FedAvgLR` class from `nvflare.app_common.workflows.lr.fedavg`, which implements the FedAvg pattern\n",
        "specifically for logistic regression.\n",
        "\n",
        "### Model Loader & Initializer\n",
        "\n",
        "NVFlare provides a standardized persistor for Logistic Regression models. We use the `LRModelPersistor` class from\n",
        "`nvflare.app_common.workflows.lr.np_persistor`, which handles model initialization, loading, and saving.\n",
        "\n",
        "```python\n",
        "from nvflare.app_common.workflows.lr.np_persistor import LRModelPersistor\n",
        "\n",
        "class LRModelPersistor(NPModelPersistor):\n",
        "    \"\"\"\n",
        "    This class defines the persistor for Logistic Regression model.\n",
        "\n",
        "    A persistor controls the logic behind initializing, loading\n",
        "    and saving of the model / parameters for each round of a\n",
        "    federated learning process.\n",
        "\n",
        "    In the Logistic Regression with Newton Raphson, a model is just a\n",
        "    1-D numpy vector containing the parameters for logistic\n",
        "    regression. The length of the parameter vector is defined\n",
        "    by the number of features in the dataset.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_dir=\"models\", model_name=\"weights.npy\", n_features=13):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model_dir = model_dir\n",
        "        self.model_name = model_name\n",
        "        self.n_features = n_features\n",
        "\n",
        "        # A default model is loaded when no local model is available.\n",
        "        # This happens when training starts.\n",
        "        #\n",
        "        # A `model` for a binary logistic regression is just a matrix,\n",
        "        # with shape (n_features + 1, 1).\n",
        "        # For the UCI ML Heart Disease dataset, the n_features = 13.\n",
        "        #\n",
        "        # A default matrix with value 0s is created.\n",
        "        #\n",
        "        self.model = np.zeros((self.n_features + 1, 1), dtype=np.float32)\n",
        "    \n",
        "    def _get_initial_model_as_numpy(self) -> np.ndarray:\n",
        "        \"\"\"Fallback initializer used by NPModelPersistor when no saved model exists.\"\"\"\n",
        "        return self.model.copy()\n",
        "```\n",
        "\n",
        "### Aggregation Logic\n",
        "\n",
        "Besides the `run()` method, we also need to implement custom aggregation and update functions.\n",
        "\n",
        "```python\n",
        "    def newton_raphson_aggregator_fn(self, results: List[FLModel]):\n",
        "        \"\"\"\n",
        "        This uses the default thread-safe WeightedAggregationHelper,\n",
        "        which implement a weighted average of all values received from\n",
        "        a `result` dictionary.\n",
        "\n",
        "        Args:\n",
        "            results: a list of `FLModel`s. Each `FLModel` is received\n",
        "                from a client. The field `params` is a dictionary that\n",
        "                contains values to be aggregated: the gradient and hessian.\n",
        "        \"\"\"\n",
        "        \n",
        "        # On client side the `sample_size` key is used to track the number of samples for each client.\n",
        "        for curr_result in results:\n",
        "            self.aggregator.add(\n",
        "                data=curr_result.params,\n",
        "                weight=curr_result.meta.get(\"sample_size\", 1.0),\n",
        "                contributor_name=curr_result.meta.get(\"client_name\", AppConstants.CLIENT_UNKNOWN),\n",
        "                contribution_round=curr_result.current_round,\n",
        "            )\n",
        "\n",
        "        aggregated_dict = self.aggregator.get_result()\n",
        "        \n",
        "        # Compute global model update:\n",
        "        # update = - damping_factor * Hessian^{-1} . Gradient\n",
        "        # A regularization is added to avoid empty hessian.\n",
        "        #\n",
        "        reg = self.epsilon * np.eye(aggregated_dict[\"hessian\"].shape[0])\n",
        "\n",
        "        newton_raphson_updates = self.damping_factor * np.linalg.solve(\n",
        "            aggregated_dict[\"hessian\"] + reg, aggregated_dict[\"gradient\"]\n",
        "        )\n",
        "        \n",
        "        # Convert the aggregated result to `FLModel`, this `FLModel`\n",
        "        # will then be used by `update_model` method from the base class,\n",
        "        # to update the global model weights.\n",
        "        #\n",
        "        aggr_result = FLModel(\n",
        "            params={\"newton_raphson_updates\": newton_raphson_updates},\n",
        "            params_type=results[0].params_type,\n",
        "            meta={\n",
        "                \"nr_aggregated\": len(results),\n",
        "                AppConstants.CURRENT_ROUND: results[0].current_round,\n",
        "                AppConstants.NUM_ROUNDS: self.num_rounds,\n",
        "            },\n",
        "        )\n",
        "        return aggr_result\n",
        "\n",
        "    def update_model(self, model, model_update, replace_meta=True) -> FLModel:\n",
        "        \"\"\"\n",
        "        Update logistic regression parameters based on\n",
        "        aggregated gradient and hessian.\n",
        "\n",
        "        \"\"\"\n",
        "        if replace_meta:\n",
        "            model.meta = model_update.meta\n",
        "        else:\n",
        "            model.meta.update(model_update.meta)\n",
        "\n",
        "        model.metrics = model_update.metrics\n",
        "        model.params[NPConstants.NUMPY_KEY] += model_update.params[\"newton_raphson_updates\"]\n",
        "\n",
        "```\n",
        "\n",
        "Again, we just need to use FLModel to store the result and update the model. \n",
        "\n",
        "### Client Training Logic \n",
        "\n",
        "Now, we need to convert the centralized training logic to the federated training logic with Client API.\n",
        "\n",
        "The complete client-side code can be found in [code/src/newton_raphson_train.py](code/src/newton_raphson_train.py).\n",
        "\n",
        "```python\n",
        "def main():\n",
        " \n",
        "    args = parse_arguments()\n",
        "\n",
        "    flare.init()\n",
        "\n",
        "    site_name = flare.get_site_name()\n",
        "    \n",
        "    # Load client site data.\n",
        "    data = load_data(args.data_root, site_name)\n",
        "\n",
        "\n",
        "    # keep running until the job is terminated or end of training round\n",
        "    while flare.is_running():\n",
        "\n",
        "        # Receive global model (FLModel) from server.\n",
        "        global_model = flare.receive()\n",
        "\n",
        "        # Get the weights, aka parameter theta for logistic regression.\n",
        "        global_weights = global_model.params[\"weights\"]\n",
        "\n",
        "        # Local validation before training\n",
        "        validation_scores = validate(data, global_weights)\n",
        "\n",
        "        # Local training\n",
        "        result_dict = train_newton_raphson(data, theta=global_weights)\n",
        "\n",
        "        # Send result to server for aggregation.\n",
        "        local_model = FLModel(params=result_dict, params_type=ParamsType.FULL)\n",
        "        local_model.meta[\"sample_size\"] = data[\"train_X\"].shape[0]\n",
        "\n",
        "        flare.send(local_model)\n",
        "```\n",
        "\n",
        "This is pretty straight forward. We receive the global model, perform the local training and send the result to the server. The code structure is the same to the centralized training with additional loop for the federated training. \n",
        "\n",
        "We added the sample size to the meta data so we can use it in weighted aggregation as the aggregation weight.\n",
        "\n",
        "### Job Configuration with Recipe API\n",
        "\n",
        "With the above steps, we have converted the centralized training to the federated training. \n",
        "\n",
        "Now, let's connect the pieces together using NVFlare's **Recipe API**, which provides a simple, declarative way to configure federated learning jobs.\n",
        "\n",
        "NVFlare provides a pre-built recipe for Federated Logistic Regression with Newton-Raphson: `FedAvgLrRecipe`. \n",
        "This recipe encapsulates all the complexity of setting up the workflow, persistor, and client runners.\n",
        "\n",
        "```python\n",
        "from nvflare.app_common.np.recipes.lr.fedavg import FedAvgLrRecipe\n",
        "from nvflare.recipe import SimEnv\n",
        "\n",
        "    n_clients = 4\n",
        "    num_rounds = 5\n",
        "    data_root = \"/tmp/flare/dataset/heart_disease_data\"\n",
        "\n",
        "    # Create FedAvgLrRecipe for Logistic Regression with Newton-Raphson\n",
        "    recipe = FedAvgLrRecipe(\n",
        "        min_clients=n_clients,\n",
        "        name=\"newton_raphson_fedavg\",\n",
        "        num_rounds=num_rounds,\n",
        "        damping_factor=0.8,\n",
        "        num_features=13,\n",
        "        train_script=\"src/newton_raphson_train.py\",\n",
        "        train_args=f\"--data_root {data_root}\",\n",
        "        launch_external_process=True,\n",
        "    )\n",
        "\n",
        "    # Execute the recipe in simulation environment\n",
        "    env = SimEnv(num_clients=n_clients, num_threads=n_clients, workspace_root=\"/tmp/nvflare/jobs/workdir\")\n",
        "    run = recipe.execute(env)\n",
        "    result_location = run.get_result()\n",
        "    print(f\"Result location: {result_location}\")\n",
        "```\n",
        "\n",
        "The recipe approach is much simpler than using the Job API directly:\n",
        "- **No manual component wiring**: The recipe automatically sets up the `FedAvgLR` controller, `LRModelPersistor`, and client runners\n",
        "- **Fewer lines of code**: Just specify the essential parameters\n",
        "- **Built-in best practices**: The recipe uses the standardized implementations from `nvflare.app_common.workflows.lr`\n",
        "- **Flexible execution**: Can easily switch between `SimEnv` (simulator) and `PocEnv` (proof of concept) environments\n",
        "\n",
        "Under the hood, the recipe uses:\n",
        "- `FedAvgLR` workflow from `nvflare.app_common.workflows.lr.fedavg`\n",
        "- `LRModelPersistor` from `nvflare.app_common.workflows.lr.np_persistor`\n",
        "- Proper configuration for Newton-Raphson optimization with the specified parameters "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a3fe4de",
      "metadata": {},
      "source": [
        "That's it, we have converted a logistic regression example to a federated job! "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b72ef2b",
      "metadata": {},
      "source": [
        "## Running Federated Logistic Regression Job\n",
        "\n",
        "Execute the following command to launch federated logistic\n",
        "regression. This will run in `nvflare`'s simulator mode.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2faea343",
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "! cd code && python lr_fl_job.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d19b9a27",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "Accuracy and precision for each site can be viewed in Tensorboard:\n",
        "```\n",
        "tensorboard --logdir=/tmp/nvflare/jobs/workdir/server/simulate_job/tb_events/\n",
        "```\n",
        "As can be seen from the figure below, per-site evaluation metrics in federated logistic regression are on-par with the centralized version.\n",
        "\n",
        "<img src=\"./code/figs/tb-metrics.png\" alt=\"Tensorboard metrics server\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ef6dff-5cd6-4113-9da0-064277b8b30b",
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0858459",
      "metadata": {},
      "outputs": [],
      "source": [
        "%tensorboard --logdir=/tmp/nvflare/jobs/workdir/server/simulate_job/tb_events/ --bind_all"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8383681",
      "metadata": {},
      "source": [
        "Now that we have converted the centralized logistic regression to federated learning, let's move on to [federated K-Means](../02.4.2_convert_kmeans_to_federated_learning/convert_kmeans_to_fl.ipynb)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
