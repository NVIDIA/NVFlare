{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c19632",
   "metadata": {},
   "source": [
    "# Converting Logistic Regression to Federated Learning\n",
    "\n",
    "\n",
    "Logistic regression is a fundamental classification algorithm that models the probability of a binary outcome. Despite its name, it's used for classification rather than regression. The model uses the logistic (sigmoid) function to transform a linear combination of features into a probability between 0 and 1.\n",
    "\n",
    "The Newton-Raphson method is a powerful second-order optimization technique that uses both first-order (gradient) and second-order (Hessian) information to find the optimal model parameters. Unlike first-order methods like gradient descent, Newton's method incorporates curvature information through the Hessian matrix, often leading to faster convergence, especially near the optimum.\n",
    "\n",
    "In this section, we will convert logistics regression with the 2nd order Newton-Raphson optimization to Federated Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9d96ed",
   "metadata": {},
   "source": [
    "## Federated Logistic Regression with Second-Order Newton-Raphson optimization\n",
    "This example shows how to implement a federated binary classification via logistic regression with second-order Newton-Raphson optimization.\n",
    "\n",
    "The [UCI Heart Disease dataset](https://archive.ics.uci.edu/dataset/45/heart+disease) is\n",
    "used in this example. Scripts are provided to download and process the\n",
    "dataset as described\n",
    "[here](https://github.com/owkin/FLamby/tree/main/flamby/datasets/fed_heart_disease).\n",
    "\n",
    "This dataset contains samples from 4 sites, splitted into training and\n",
    "testing sets as described below:\n",
    "|site         | sample split                          |\n",
    "|-------------|---------------------------------------|\n",
    "|Cleveland    | train: 199 samples, test: 104 samples |\n",
    "|Hungary      | train: 172 samples, test: 89 samples  |\n",
    "|Switzerland  | train: 30 samples, test: 16 samples   |\n",
    "|Long Beach V | train: 85 samples, test: 45 samples   |\n",
    "\n",
    "The number of features in each sample is 13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f0dcc",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The [Newton-Raphson\n",
    "optimization](https://en.wikipedia.org/wiki/Newton%27s_method) problem\n",
    "can be described as follows.\n",
    "\n",
    "In a binary classification task with logistic regression, the\n",
    "probability of a data sample $x$ classified as positive is formulated\n",
    "as:\n",
    "$$p(x) = \\sigma(\\beta \\cdot x + \\beta_{0})$$\n",
    "where $\\sigma(.)$ denotes the sigmoid function. We can incorporate\n",
    "$\\beta_{0}$ and $\\beta$ into a single parameter vector $\\theta =\n",
    "( \\beta_{0},  \\beta)$. Let $d$ be the number\n",
    "of features for each data sample $x$ and let $N$ be the number of data\n",
    "samples. We then have the matrix version of the above probability\n",
    "equation:\n",
    "$$p(X) = \\sigma( X \\theta )$$\n",
    "Here $X$ is the matrix of all samples, with shape $N \\times (d+1)$,\n",
    "having it's first column filled with value 1 to account for the\n",
    "intercept $\\theta_{0}$.\n",
    "\n",
    "The goal is to compute parameter vector $\\theta$ that maximizes the\n",
    "below likelihood function:\n",
    "$$L_{\\theta} = \\prod_{i=1}^{N} p(x_i)^{y_i} (1 - p(x_i)^{1-y_i})$$\n",
    "\n",
    "The Newton-Raphson method optimizes the likelihood function via\n",
    "quadratic approximation. Omitting the maths, the theoretical update\n",
    "formula for parameter vector $\\theta$ is:\n",
    "$$\\theta^{n+1} = \\theta^{n} - H_{\\theta^{n}}^{-1} \\nabla L_{\\theta^{n}}$$\n",
    "where\n",
    "$$\\nabla L_{\\theta^{n}} = X^{T}(y - p(X))$$\n",
    "is the gradient of the likelihood function, with $y$ being the vector\n",
    "of ground truth for sample data matrix $X$,  and\n",
    "$$H_{\\theta^{n}} = -X^{T} D X$$\n",
    "is the Hessian of the likelihood function, with $D$ a diagonal matrix\n",
    "where diagonal value at $(i,i)$ is $D(i,i) = p(x_i) (1 - p(x_i))$.\n",
    "\n",
    "In federated Newton-Raphson optimization, each client will compute its\n",
    "own gradient $\\nabla L_{\\theta^{n}}$ and Hessian $H_{\\theta^{n}}$\n",
    "based on local training samples. A server will aggregate the gradients\n",
    "and Hessians computed from all clients, and perform the update of\n",
    "parameter $\\theta$ based on the theoretical update formula described\n",
    "above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32003ba9",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Using `nvflare`, The federated logistic regression with Newton-Raphson\n",
    "optimization is implemented as follows.\n",
    "\n",
    "On the server side, all workflow logics are implemented in\n",
    "class `FedAvgNewtonRaphson`, which can be found\n",
    "[here](code/newton_raphson/app/custom/newton_raphson_workflow.py). The\n",
    "`FedAvgNewtonRaphson` class inherits from the\n",
    "[`BaseFedAvg`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/base_fedavg.py)\n",
    "class, which itself inherits from the **ModelController**\n",
    "([`ModelController`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/model_controller.py))\n",
    "class. This is the preferrable approach to implement a custom\n",
    "workflow, since `ModelController` decouples communication logic from\n",
    "actual workflow (training & validation) logic. The mandatory\n",
    "method to override in `ModelController` is the\n",
    "[`run()`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/model_controller.py#L37)\n",
    "method, where the orchestration of server-side workflow actually\n",
    "happens. The implementation of `run()` method in\n",
    "[`FedAvgNewtonRaphson`](code/newton_raphson/app/custom/newton_raphson_workflow.py)\n",
    "is similar to the classic\n",
    "[`FedAvg`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/fedavg.py#L44):\n",
    "- Initialize the global model, this is acheived through method `load_model()`\n",
    "  from base class\n",
    "  [`ModelController`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/model_controller.py#L292),\n",
    "  which relies on the\n",
    "  [`ModelPersistor`](https://nvflare.readthedocs.io/en/main/glossary.html#persistor). A\n",
    "  custom\n",
    "  [`NewtonRaphsonModelPersistor`](code/newton_raphson/app/custom/newton_raphson_persistor.py)\n",
    "  is implemented in this example, which is based on the\n",
    "  [`NPModelPersistor`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/np/np_model_persistor.py)\n",
    "  for numpy data, since the _model_ in the case of logistic regression\n",
    "  is just the parameter vector $\\theta$ that can be represented by a\n",
    "  numpy array. Only the `__init__` method needs to be re-implemented\n",
    "  to provide a proper initialization for the global parameter vector\n",
    "  $\\theta$.\n",
    "- During each training round, the global model will be sent to the\n",
    "  list of participating clients to perform a training task. This is\n",
    "  done using the\n",
    "  [`send_model_and_wait()`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/workflows/model_controller.py#L41)\n",
    "  method. Once\n",
    "  the clients finish their local training, results will be collected\n",
    "  and sent back to server as\n",
    "  [`FLModel`](https://nvflare.readthedocs.io/en/main/programming_guide/fl_model.html#flmodel)s.\n",
    "- Results sent by clients contain their locally computed gradient and\n",
    "  Hessian. A [custom aggregation\n",
    "  function](code/newton_raphson/app/custom/newton_raphson_workflow.py)\n",
    "  is implemented to get the averaged gradient and Hessian, and compute\n",
    "  the Newton-Raphson update for the global parameter vector $\\theta$,\n",
    "  based on the theoretical formula shown above. The averaging of\n",
    "  gradient and Hessian is based on the\n",
    "  [`WeightedAggregationHelper`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/aggregators/weighted_aggregation_helper.py#L20),\n",
    "  which weighs the contribution from each client based on the number\n",
    "  of local training samples. The aggregated Newton-Raphson update is\n",
    "  returned as an `FLModel`.\n",
    "- After getting the aggregated Newton-Raphson update, an\n",
    "  [`update_model()`](code/newton_raphson/app/custom/newton_raphson_workflow.py#L172)\n",
    "  method is implemented to actually apply the Newton-Raphson update to\n",
    "  the global model.\n",
    "- The last step is to save the updated global model, again through\n",
    "  the `NewtonRaphsonModelPersistor` using `save_model()`.\n",
    "\n",
    "\n",
    "On the client side, the local training logic is implemented\n",
    "[here](code/newton_raphson/app/custom/newton_raphson_train.py). The\n",
    "implementation is based on the [`Client\n",
    "API`](https://nvflare.readthedocs.io/en/main/programming_guide/execution_api_type.html#client-api). This\n",
    "allows user to add minimum `nvflare`-specific code to turn a typical\n",
    "centralized training script into a federated client side local training\n",
    "script.\n",
    "- During local training, each client receives a copy of the global\n",
    "  model, sent by the server, using `flare.receive()` from the Client API.\n",
    "  The received global model is an instance of `FLModel`.\n",
    "- A local validation is first performed, where validation metrics\n",
    "  (accuracy and precision) are streamed to server using the\n",
    "  [`SummaryWriter`](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.client.tracking.html#nvflare.client.tracking.SummaryWriter). The\n",
    "  streamed metrics can be loaded and visualized using tensorboard.\n",
    "- Then each client computes it's gradient and Hessian based on local\n",
    "  training data, using their respective theoretical formula described\n",
    "  above. This is implemented in the\n",
    "  [`train_newton_raphson()`](code/newton_raphson/app/custom/newton_raphson_train.py#L82)\n",
    "  method. Each client then sends the computed results (always in\n",
    "  `FLModel` format) to server for aggregation, using the Client API call\n",
    "  `flare.send()`.\n",
    "\n",
    "Each client site corresponds to a site listed in the data table above.\n",
    "\n",
    "A [centralized training script](code/train_centralized.py) is also\n",
    "provided, which allows for comparing the federated Newton-Raphson\n",
    "optimization versus the centralized version. In the centralized\n",
    "version, training data samples from all 4 sites were concatenated into\n",
    "a single matrix, used to optimize the model parameters. The\n",
    "optimized model was then tested separately on testing data samples of\n",
    "the 4 sites, using accuracy and precision as metrics.\n",
    "\n",
    "Comparing the federated [client-side training\n",
    "code](code/newton_raphson/app/custom/newton_raphson_train.py) with the\n",
    "centralized [training code](code/train_centralized.py), we can see that\n",
    "the training logic remains similar: load data, perform training\n",
    "(Newton-Raphson updates), and valid trained model. The only added\n",
    "differences in the federated code are related to interaction with the\n",
    "FL system, such as receiving and send `FLModel`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fc55e0",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04911ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ea8504",
   "metadata": {},
   "source": [
    "## Download and prepare data\n",
    "\n",
    "Execute the following script\n",
    "```\n",
    "bash ./code/data/prepare_heart_disease_data.sh\n",
    "```\n",
    "This will download the heart disease dataset under\n",
    "`/tmp/flare/dataset/heart_disease_data/`\n",
    "\n",
    "Please note that you may need to accept the data terms in order to complete the download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b395c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the the download site remember your download history and abort the 2nd download attempt. \n",
    "\n",
    "! echo y | bash ./code/data/prepare_heart_disease_data.sh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e13343",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -al /tmp/flare/dataset/heart_disease_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548b466",
   "metadata": {},
   "source": [
    "## Centralized Logistic Regression\n",
    "\n",
    "Two implementations of logistic regression are provided in the\n",
    "centralized training script, which can be specified by the `--solver`\n",
    "argument:\n",
    "- One is using `sklearn.LogisticRegression` with the `newton-cholesky`\n",
    "  solver\n",
    "- The other one is manually implemented using the theoretical update\n",
    "  formulas described above.\n",
    "\n",
    "Both implementations were tested to converge in 4 iterations and to\n",
    "give the same result.\n",
    "\n",
    "Launch the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c68fe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd code\n",
    "! python3 train_centralized.py --solver custom\n",
    "\n",
    "%cd -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c8d278",
   "metadata": {},
   "source": [
    "## Federated Logistic Regression\n",
    "\n",
    "\n",
    "To convert the centralized logistic regression to federated learning, we need to do the following:\n",
    "\n",
    "1. Decide what model parameters will be transmitted between the server and clients\n",
    "2. Define the workflow that orchestrates the federated learning process\n",
    "3. Define how to load the initial model on the server side\n",
    "4. Modify the client-side training logic to handle models received from the server\n",
    "5. Implement the aggregation logic for the gradients and Hessians computed by the clients\n",
    "6. Configure the job via FLARE job API\n",
    "\n",
    "Let's examine each step.\n",
    "\n",
    "### Model Parameters\n",
    "\n",
    "We decided to simply capture the model parameters in the FLModel:\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "model = FLModel(params={\"gradient\": gradient, \"hessian\": hessian})\n",
    "```\n",
    "\n",
    "We could optionally use FLModel.optimizer_params to store the Hessian, but either approach works.\n",
    "\n",
    "We add a few metadata fields to help with the training process. We use the training sample size as the weight, storing this information in the metadata:\n",
    "\n",
    "```python\n",
    "\n",
    "model = FLModel(params=result_dict, params_type=ParamsType.FULL)\n",
    "model.meta[\"sample_size\"] = data[\"train_X\"].shape[0]\n",
    "```\n",
    "\n",
    "### Workflow\n",
    "\n",
    "We decided to choose the FedAvg type of scatter and gather workflow. So we can based the class using the `BaseFedAvg` class. \n",
    "\n",
    "```python\n",
    "\n",
    "class FedAvgNewtonRaphson(BaseFedAvg):\n",
    "\n",
    "    def __init__(self, damping_factor, epsilon=1.0, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \"\"\"\n",
    "    Args:\n",
    "        damping_factor: damping factor for Newton Raphson updates.\n",
    "        epsilon: a regularization factor to avoid empty hessian for\n",
    "            matrix inversion\n",
    "    \"\"\"\n",
    "        self.damping_factor = damping_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.aggregator = WeightedAggregationHelper()\n",
    "\n",
    "    def run(self) -> None:\n",
    "        \n",
    "        # First load the model and set up some training params.\n",
    "        # A `persisitor` (NewtonRaphsonModelPersistor) will load\n",
    "        # the model in `ModelLearnable` format, then will be\n",
    "        # converted `FLModel` by `ModelController`.\n",
    "        #\n",
    "        model = self.load_model()\n",
    "\n",
    "        model.start_round = self.start_round\n",
    "        model.total_rounds = self.num_rounds\n",
    "\n",
    "       \n",
    "        for self.current_round in range(self.start_round, self.start_round + self.num_rounds):\n",
    "\n",
    "            # Get the list of clients.\n",
    "            clients = self.sample_clients(self.num_clients)\n",
    "\n",
    "            model.current_round = self.current_round\n",
    "\n",
    "            results = self.send_model_and_wait(targets=clients, data=model)\n",
    "\n",
    "            # Aggregate results receieved from clients.\n",
    "            aggregate_results = self.aggregate(results, aggregate_fn=self.newton_raphson_aggregator_fn)\n",
    "\n",
    "            # Update global model based on the following formula:\n",
    "            # weights = weights + updates, where\n",
    "            # updates = -damping_factor * Hessian^{-1} . Gradient\n",
    "            self.update_model(model, aggregate_results)\n",
    "\n",
    "            # Save global model.\n",
    "            self.save_model(model)\n",
    "\n",
    "        self.info(\"Finished FedAvg.\")\n",
    "\n",
    "```\n",
    "As you can see the `run()` method is the only method we need to implement. Its nothing but a for loop that sends the model to the clients and aggregate the results. \n",
    "\n",
    "### Model Loader\n",
    "\n",
    "we need to decide how to load the initial model on the server side. We decide to implement a custom persistor that loads the model from a numpy file. \n",
    "\n",
    "```python\n",
    "\n",
    "class NewtonRaphsonModelPersistor(NPModelPersistor):\n",
    "    \"\"\"\n",
    "    This class defines the persistor for Newton Raphson model.\n",
    "\n",
    "    A persistor controls the logic behind initializing, loading\n",
    "    and saving of the model / parameters for each round of a\n",
    "    federated learning process.\n",
    "\n",
    "    In the 2nd order Newton Raphson case, a model is just a\n",
    "    1-D numpy vector containing the parameters for logistic\n",
    "    regression. The length of the parameter vector is defined\n",
    "    by the number of features in the dataset.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_dir=\"models\", model_name=\"weights.npy\", n_features=13):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "        self.n_features = n_features\n",
    "\n",
    "        # A default model is loaded when no local model is available.\n",
    "        # This happen when training starts.\n",
    "        #\n",
    "        # A `model` for a binary logistic regression is just a matrix,\n",
    "        # with shape (n_features + 1, 1).\n",
    "        # For the UCI ML Heart Disease dataset, the n_features = 13.\n",
    "        #\n",
    "        # A default matrix with value 0s is created.\n",
    "        #\n",
    "        self.default_data = np.zeros((self.n_features + 1, 1), dtype=np.float32)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Client Training Logic \n",
    "\n",
    "Now, we need to convert the centralized training logic to the federated training logic with Client API.\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "def main():\n",
    " \n",
    "    args = parse_arguments()\n",
    "\n",
    "    flare.init()\n",
    "\n",
    "    site_name = flare.get_site_name()\n",
    "    \n",
    "    # Load client site data.\n",
    "    data = load_data(args.data_root, site_name)\n",
    "\n",
    "\n",
    "    # keep running until the job is terminated or end of training round\n",
    "    while flare.is_running():\n",
    "\n",
    "        # Receive global model (FLModel) from server.\n",
    "        global_model = flare.receive()\n",
    "\n",
    "        # Get the weights, aka parameter theta for logistic regression.\n",
    "        global_weights = global_model.params[\"weights\"]\n",
    "\n",
    "        # Local validation before training\n",
    "        validation_scores = validate(data, global_weights)\n",
    "\n",
    "        # Local training\n",
    "        result_dict = train_newton_raphson(data, theta=global_weights)\n",
    "\n",
    "        # Send result to server for aggregation.\n",
    "        local_model = FLModel(params=result_dict, params_type=ParamsType.FULL)\n",
    "        local_model.meta[\"sample_size\"] = data[\"train_X\"].shape[0]\n",
    "\n",
    "        flare.send(local_model)\n",
    "\n",
    "```\n",
    "\n",
    "This is pretty straight forward. We receive the global model, perform the local training and send the result to the server. The code structure is the same to the centralized training with additional loop for the federated training. \n",
    "\n",
    "We added the sample size to the meta data so we can use it in weighted aggregation as the aggregation weight.\n",
    "\n",
    "\n",
    "### Aggregation Logic\n",
    "\n",
    "Now, lets loop at the aggregation logic. \n",
    "\n",
    "```python\n",
    "\n",
    "    def newton_raphson_aggregator_fn(self, results: List[FLModel]):\n",
    "        \"\"\"\n",
    "        This uses the default thread-safe WeightedAggregationHelper,\n",
    "        which implement a weighted average of all values received from\n",
    "        a `result` dictionary.\n",
    "\n",
    "        Args:\n",
    "            results: a list of `FLModel`s. Each `FLModel` is received\n",
    "                from a client. The field `params` is a dictionary that\n",
    "                contains values to be aggregated: the gradient and hessian.\n",
    "        \"\"\"\n",
    "        \n",
    "        # On client side the `sample_size` key is used to track the number of samples for each client.\n",
    "        for curr_result in results:\n",
    "            self.aggregator.add(\n",
    "                data=curr_result.params,\n",
    "                weight=curr_result.meta.get(\"sample_size\", 1.0),\n",
    "                contributor_name=curr_result.meta.get(\"client_name\", AppConstants.CLIENT_UNKNOWN),\n",
    "                contribution_round=curr_result.current_round,\n",
    "            )\n",
    "\n",
    "        aggregated_dict = self.aggregator.get_result()\n",
    "        \n",
    "        # Compute global model update:\n",
    "        # update = - damping_factor * Hessian^{-1} . Gradient\n",
    "        # A regularization is added to avoid empty hessian.\n",
    "        #\n",
    "        reg = self.epsilon * np.eye(aggregated_dict[\"hessian\"].shape[0])\n",
    "\n",
    "        newton_raphson_updates = self.damping_factor * np.linalg.solve(\n",
    "            aggregated_dict[\"hessian\"] + reg, aggregated_dict[\"gradient\"]\n",
    "        )\n",
    "        \n",
    "        # Convert the aggregated result to `FLModel`, this `FLModel`\n",
    "        # will then be used by `update_model` method from the base class,\n",
    "        # to update the global model weights.\n",
    "        #\n",
    "        aggr_result = FLModel(\n",
    "            params={\"newton_raphson_updates\": newton_raphson_updates},\n",
    "            params_type=results[0].params_type,\n",
    "            meta={\n",
    "                \"nr_aggregated\": len(results),\n",
    "                AppConstants.CURRENT_ROUND: results[0].current_round,\n",
    "                AppConstants.NUM_ROUNDS: self.num_rounds,\n",
    "            },\n",
    "        )\n",
    "        return aggr_result\n",
    "\n",
    "    def update_model(self, model, model_update, replace_meta=True) -> FLModel:\n",
    "        \"\"\"\n",
    "        Update logistic regression parameters based on\n",
    "        aggregated gradient and hessian.\n",
    "\n",
    "        \"\"\"\n",
    "        if replace_meta:\n",
    "            model.meta = model_update.meta\n",
    "        else:\n",
    "            model.meta.update(model_update.meta)\n",
    "\n",
    "        model.metrics = model_update.metrics\n",
    "        model.params[NPConstants.NUMPY_KEY] += model_update.params[\"newton_raphson_updates\"]\n",
    "\n",
    "```\n",
    "Again, we just need to use FLModel to store the result and update the model. \n",
    "\n",
    "\n",
    "### Job Configuration\n",
    "\n",
    "With the above steps, we have converted the centralized training to the federated training. \n",
    "\n",
    "Now, lets connect the pieces together and define the job configuration and run with simulator. \n",
    "\n",
    "In this example, we decided to sub-process instead of in-process training. \n",
    "\n",
    "We manually define the job configuration and run with simulator. \n",
    "\n",
    "#### server job configuration\n",
    "\n",
    "The key is defined a workflow ```FedAvgNewtonRaphson``` and corresponding arguments: number round, clients and damping factor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5767f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat code/newton_raphson/app/config/config_fed_server.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13fdf5d",
   "metadata": {},
   "source": [
    "#### client job configuration\n",
    "\n",
    "Notice that we used the ClientAPILauncherExecutor with a Cell Pipe, we also need a separate pipe for metrics relay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b8eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat code/newton_raphson/app/config/config_fed_client.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3fe4de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b72ef2b",
   "metadata": {},
   "source": [
    "## Running Federated Logistic Regression Job\n",
    "\n",
    "Execute the following command to launch federated logistic\n",
    "regression. This will run in `nvflare`'s simulator mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faea343",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvflare simulator -w /tmp/nvflare/job/lr/workspace -n 4 -t 4 code/newton_raphson/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Accuracy and precision for each site can be viewed in Tensorboard:\n",
    "```\n",
    "tensorboard --logdir=/tmp/nvflare/job/lr/workspace/server/simulate_job/tb_events\n",
    "```\n",
    "As can be seen from the figure below, per-site evaluation metrics in\n",
    "federated logistic regression are on-par with the centralized version.\n",
    "\n",
    "<img src=\"./code/figs/tb-metrics.png\" alt=\"Tensorboard metrics server\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0858459",
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir=/tmp/nvflare/job/lr/workspace/server/simulate_job/tb_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8383681",
   "metadata": {},
   "source": [
    "Now that we have converted the centralized logistic regression to federated learning, let's move on to the next example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990f546",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_env",
   "language": "python",
   "name": "nvflare_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
