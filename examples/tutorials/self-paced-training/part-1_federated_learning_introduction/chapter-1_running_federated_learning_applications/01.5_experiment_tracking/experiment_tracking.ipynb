{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75b2253-cba8-4579-907b-09311e0da587",
   "metadata": {},
   "source": [
    "# Experiment Tracking\n",
    "\n",
    "When you’re conducting machine learning training, especially in distributed settings like federated learning, it’s crucial to monitor training and evaluation metrics closely. \n",
    "\n",
    "NVIDIA FLARE provides built-in integration with experiment tracking systems—MLflow, Weights & Biases, and TensorBoard—to facilitate comprehensive monitoring of these metrics.\n",
    "\n",
    "## Introduction to distributed experiment tracking\n",
    "\n",
    "In a federated computing setting, data is distributed across multiple devices or systems, and training is run on each device independently while preserving each client’s data privacy.\n",
    "\n",
    "There are two ways to interact with ML experiment tracking tools:\n",
    "\n",
    "<img src=\"img/metrics-streaming-fl-server-clients.png\" alt=\"experiment_tracking\" width=\"60%\">\n",
    "\n",
    "- Decentralized tracking (client-side experiment tracking): Each client directly sends the log metrics/parameters to the ML experiment tracking server (like MLflow or Weights and Biases) or local file system (like TensorBoard).\n",
    "- Centralized tracking (aggregated experiment tracking): Clients send the log metrics/parameters to the FL server, and the FL server sends the metrics to the ML experiment tracking server or local file system.\n",
    "\n",
    "The NVIDIA FLARE job configuration enables you to choose the tracking scenario or system that best fits your needs. When users need to migrate from one experiment tracking system to another using NVIDIA FLARE, you can modify the job configuration without rewriting the experiment tracking code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47399ea6",
   "metadata": {},
   "source": [
    "The `nvflare.client.tracking` API enables you to flexibly redirect your logging metrics to any destination. The syntax you use (MLflow, Weights & Biases, or TensorBoard) doesn't matter, as you can stream the collected metrics to any supported experiment tracking system. The choice between MLflowWriter, WandBWriter, or TBWriter depends on your existing code and requirements.\n",
    "\n",
    "- MLflowWriter uses the MLflow API operation log_metric.\n",
    "- TBWriter uses the TensorBoard SummaryWriter operation add_scalar.\n",
    "- WandBWriter uses the API operation log.\n",
    "\n",
    "Depending on your existing code or familiarity with these systems, you can choose any writer. After you’ve modified the training code, you can use the NVIDIA FLARE job configuration to configure the system to stream the logs appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d1bc5f",
   "metadata": {},
   "source": [
    "To see how to use experiment tracking with TensorBoard, see [Experiment tracking with TensorBoard](./01.5.1_experiment_tracking_with_tensorboard/experiment_tracking_tensorboard.ipynb).\n",
    "\n",
    "For how to use `MLflowReceiver` to set up experiment tracking for MLflow, see [Experiment Tracking with MLflow](./01.5.2_experiment_tracking_with_mlflow/experiment_tracking_mlflow.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
