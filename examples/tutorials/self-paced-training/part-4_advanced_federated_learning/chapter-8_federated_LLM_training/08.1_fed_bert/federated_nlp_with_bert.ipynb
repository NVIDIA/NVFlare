{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e85865-d59b-4809-9ae0-ca5260df37bc",
   "metadata": {},
   "source": [
    "# Federated NLP with BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466606e1-7f8f-45e4-bed1-c136d89258c1",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "In this example, we show how to use [NVIDIA FLARE](https://nvidia.github.io/NVFlare) for a Natural Language Processing (NLP) task using [BERT](https://github.com/google-research/bert) model from [Hugging Face](https://huggingface.co/). We select [BERT-base-uncased](https://huggingface.co/bert-base-uncased) as our base model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ca051-add4-474b-9637-24c16040d7b6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required packages for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d1f87-502a-4c30-aa40-f55ae65a1da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734aa5da-3e4e-4c22-a0d2-b8ee6b6be142",
   "metadata": {},
   "source": [
    "## Download Data \n",
    "The raw data can be accessed from [official page](https://www.ncbi.nlm.nih.gov/CBBresearch/Dogan/DISEASE/). \n",
    "In this example, we use the preprocessed csv-files from the reference repo above, which can be downloaded [here](https://drive.google.com/drive/folders/13wROtEAnMgWpLMIGHB5CY1BQ1Xe2XqhG). \n",
    "\n",
    "In the following, we download three files `train.csv`, `dev.csv`, and `test.csv` and save them to `/tmp/nvflare/dataset/nlp_ner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd478cb-1565-4283-a4bb-87f15585932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir -p /tmp/nvflare/dataset/nlp_ner\n",
    "wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1YWGBElsqj5ENsW0PtYwMlk_ShBt8MsLD' -O /tmp/nvflare/dataset/nlp_ner/dev.csv\n",
    "wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=12kXGQPW-do-F7T-TLGycl0DCw6eQIaZc' -O /tmp/nvflare/dataset/nlp_ner/test.csv\n",
    "wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fjsf0jFKWu_-bbx236oB6e7DqOqGmw3y' -O /tmp/nvflare/dataset/nlp_ner/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3f038-9b4c-416e-abeb-132625e7fefa",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "We then use the preprocessed data to generate random splits for both 4-client and 2-client experiments. \n",
    "Please modify the `DATASET_ROOT` below to point to folder containing the four downloaded csv-files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459c5dc6-c423-4d80-9577-add2d062c326",
   "metadata": {},
   "outputs": [],
   "source": [
    "! code/prepare_data.sh /tmp/nvflare/dataset/nlp_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9186c7-1eb7-4eec-bfcd-ec64d12ceaf7",
   "metadata": {},
   "source": [
    "The expected output is\n",
    "```\n",
    "4-client\n",
    "(7594, 5) (2531, 5)\n",
    "(5063, 5) (2531, 5)\n",
    "(2532, 5) (2531, 5)\n",
    "(2532, 5) (2532, 5)\n",
    "(950, 5) (316, 5)\n",
    "(634, 5) (316, 5)\n",
    "(318, 5) (316, 5)\n",
    "(318, 5) (318, 5)\n",
    "```\n",
    "The task here is to categorize each word in the text into three classes specified by the label. For example, the sentence \n",
    "`Recent progress has resulted in part of the gene mutated in Duchenne and the milder Becker muscular dystrophies being cloned and has suggested that the gene itself extends over 1 , 000 to 2 , 000 kilobases ( kb ) .` into label vector `O O O O O O O O O O O B I I I I I I O O O O O O O O O O O O O O O O O O O O O O O`. `B` marks the beginning of an entity, `I` marks each entity word, and `O` represents other words.\n",
    "Let's take a closer look at the word-label correspondence:\n",
    "![data sample](./figs/sample.png)\n",
    "As shown above, the task is to capture the keywords related to medical findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62476dd2-97e7-48ad-908f-6df02f48f86e",
   "metadata": {},
   "source": [
    "## Run automated experiments\n",
    "We run the federated training on 4 clients for BERT model using NVFlare Simulator via [JobAPI](https://nvflare.readthedocs.io/en/main/programming_guide/fed_job_api.html). To save time, we only run 5 rounds of fedrated training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48f1d5f-e656-4f71-b925-94035c60ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd code\n",
    "! python nlp_fl_job.py --model_name Bert\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0683f68f-63bc-463a-bae2-f17d51fe735c",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Validation curve on each site\n",
    "In this example, each client computes their validation scores using their own\n",
    "validation set. We recorded the loss, F1 score, precision, and recall. \n",
    "The curves can be viewed with TensorBoard, each training for 50 epochs (50 FL rounds, 1 local epoch per round).\n",
    "\n",
    "For BERT model, the TensorBoard curves can be visualized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a719a9-5dfb-4a8e-a540-08fb05492495",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/nvflare/workspace/works/Bert/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a42d7c-641f-45f3-92fc-367264cae669",
   "metadata": {},
   "source": [
    "### Testing score\n",
    "The testing score is computed for the global model over the testing set.\n",
    "We provide a script for performing validation on testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c142aa0-4502-4108-9b4d-462050d37a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd code\n",
    "! sh test_global_model.sh /tmp/nvflare/dataset/nlp_ner\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32473e-2338-4176-b25d-ec7976f8440d",
   "metadata": {},
   "source": [
    "The test results are:\n",
    "```\n",
    "BERT\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           _       0.83      0.92      0.87      1255\n",
    "\n",
    "   micro avg       0.83      0.92      0.87      1255\n",
    "   macro avg       0.83      0.92      0.87      1255\n",
    "weighted avg       0.83      0.92      0.87      1255\n",
    "```\n",
    "Note that training is not deterministic so the numbers can have some variations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8076c89e-39b2-44f3-903c-fc9ff8446d67",
   "metadata": {},
   "source": [
    "In this section, we showed how to train a BERT model with standard Pytorch training loop. Now let's move on to the next section [LLM Supervised Fine-Tuning (SFT)](../08.2_llm_sft/LLM_SFT.ipynb) where we will see how to utilize existing Trainer scripts via HuggingFace APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26111bcd-ed0a-4298-9956-b1e821409197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
