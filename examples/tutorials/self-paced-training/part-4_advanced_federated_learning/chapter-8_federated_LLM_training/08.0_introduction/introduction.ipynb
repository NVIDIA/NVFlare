{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e333915-6759-4d82-9f83-2bccc42ca047",
   "metadata": {},
   "source": [
    "# Introduction: Federated Language Models - from NLP to LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96f5007-8c62-4d61-bd97-6b31a3f5b0db",
   "metadata": {},
   "source": [
    "In this chapter, we will explore the federated learning applications on language models.\n",
    "\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence, focuses on enabling computers to process and analyze natural language data. Recently, Large Language Models (LLMs) have emerged as a transformative force in the field of NLP, enabling AI to understand, generate, and interact with human language at an unprecedented scale. Models such as BERT and GPT is able to leverage vast amounts of text data and deep learning techniques to perform various linguistic tasks, including text generation, translation, summarization, and question-answering.\n",
    "\n",
    "The development of LLMs relies on robust training schemes that enable these models to capture linguistic structures, contextual dependencies, and semantic meanings. Common training methodologies include unsupervised pretraining on large text corpora, followed by further fine-tuning using supervised (supervised finetuning - SFT) or reinforcement learning (reinforcement learning from human feedback - RLHF) approaches, refining their capabilities for practical applications with human interactions.\n",
    "\n",
    "Further, when adapting to a particular downstream task, instead of making updates to all model parameters as SFT/RLHF which can be computationally expensive and memory-intensive, Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a more efficient approach.  Techniques such as Low-Rank Adaptation (LoRA), P-Tuning, and Adapter Layers enable fine-tuning by updating only a small subset of parameters, significantly reducing computational costs while maintaining performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb8b73a-5058-44fe-8ecc-e355ef7178fb",
   "metadata": {},
   "source": [
    "In the following sections, we will start with federated learning using a smaller-scale BERT model, then we extend our study to more recent open-source LLMs and their SFT and PEFT in a federated finetuning scheme. And finally to address a major challenge in federated LLM training - communication efficiency, we further visit potential solutions including quantization and streaming, and we will conclude with a recap of the covered topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bffcb04-6839-4463-b4a5-e1792024adce",
   "metadata": {},
   "source": [
    "8.1. **Federated BERT**\n",
    "\n",
    "Task-specific model training with BERT in a federated setting\n",
    "* [Federated NLP with BERT Model](../08.1_fed_bert/federated_nlp_with_bert.ipynb)\n",
    "\n",
    "8.2. **Federated LLM Training with SFT**\n",
    "\n",
    "Supervised Fine-Tuning and its role in adapting LLMs in federated learning\n",
    "* [Federated LLM Tuning with SFT](../08.2_llm_sft/LLM_SFT.ipynb)\n",
    "\n",
    "8.3. **Federated LLM Training with PEFT**\n",
    "\n",
    "Importance of PEFT in adapting LLMs for specific tasks, which can be achieve in a federated setting\n",
    "* [Federated LLM Tuning with PEFT](../08.3_llm_peft/LLM_PEFT.ipynb)\n",
    "\n",
    "8.4. **Model Transmission with Quantization**\n",
    "\n",
    "One major hurdle of adapting LLMs in federated learning is the significant communication burden when performing federated SFT. To reduce the message size, quantization method can be applied as filters.\n",
    "* [Model Quantization for Transmission](../08.4_llm_quantization/LLM_quantization.ipynb)\n",
    "\n",
    "8.5 **Model Transmission with Streaming**\n",
    "\n",
    "While quantization reduced communication cost, system memory requirement is still high for prepareing the message on either side. Therefore, we enabled streaming capabilities for more efficient and robust model communication.\n",
    "* [Message Streaming for Model Transmission](../08.5_llm_streaming/LLM_streaming.ipynb)\n",
    "\n",
    "8.6. **Recap**\n",
    "\n",
    "[Recap](../08.6_recap/recap.ipynb) for federated LLM applications and features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8864f21-ce74-4adf-8b5b-240879424424",
   "metadata": {},
   "source": [
    "Let's get started with [Federated NLP with BERT Model](../08.1_fed_bert/federated_nlp_with_bert.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceaab09-f41e-41e4-8ecd-a784328b468a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
