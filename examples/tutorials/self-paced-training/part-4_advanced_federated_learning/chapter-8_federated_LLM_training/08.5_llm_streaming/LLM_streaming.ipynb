{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ad30489-5e3d-4994-966d-666bd40a13e0",
   "metadata": {},
   "source": [
    "# Object Streaming\n",
    "\n",
    "## Overview\n",
    "The examples here demonstrate how to use object streamers to send large objects in a memory-efficient manner.\n",
    "\n",
    "Current default setting is to send and receive large objects in full, so extra memory will be needed and allocated to hold the received message. \n",
    "This works fine when the message is small, but can become a limit when model size is large, e.g. for large language models.\n",
    "\n",
    "To save on memory usage, we can stream the message send / receive: when sending large objects (e.g. a dict),\n",
    "streamer sends containers entry by entry (e.g. one dict item each time); further, if we save the object to a file, \n",
    "streamer can send the file by chunks (default chunk size is 1MB).\n",
    "\n",
    "Thus, the memory demand can be reduced to the size of the largest entry for container streaming; while nearly no extra memory is needed for file\n",
    "streaming. For example, if sending a dict with 10 1GB entries, without streaming, it will take 10GB extra space to send the dict. \n",
    "With container streaming, it only requires extra 1GB; and if saved to a file before sending, it only requires 1MB extra space to send the file.\n",
    "\n",
    "All examples are run with NVFlare Simulator via [JobAPI](https://nvflare.readthedocs.io/en/main/programming_guide/fed_job_api.html).\n",
    "## Concepts\n",
    "\n",
    "### Object Streamer\n",
    "ObjectStreamer is the base class to stream an object piece by piece. The `StreamableEngine` built in the NVFlare can\n",
    "stream any implementations of ObjectSteamer\n",
    "\n",
    "The following implementations are included in NVFlare,\n",
    "\n",
    "* `ContainerStreamer`: This class is used to stream a container entry by entry. Currently, dict, list and set are supported\n",
    "* `FileStreamer`: This class is used to stream a file\n",
    "\n",
    "Note that the container streamer split the stream by the top level entries. All the sub entries of a top entry are expected to be\n",
    "sent as a whole, therefore the memory is determined by the largest entry at top level.\n",
    "\n",
    "### Object Retriever\n",
    "Building upon the streamers, `ObjectRetriever` is designed for easier integration with existing code: to request an object to be streamed from a remote site. It automatically sets up the streaming\n",
    "on both ends and handles the coordination.\n",
    "\n",
    "Similarly, the following implementations are available,\n",
    "\n",
    "* `ContainerRetriever`: This class is used to retrieve a container from remote site using `ContainerStreamer`.\n",
    "* `FileRetriever`: This class is used to retrieve a file from remote site using `FileStreamer`.\n",
    "\n",
    "Note that to use ContainerRetriever, the container must be given a name and added on the sending site,\n",
    "```\n",
    "ContainerRetriever.add_container(\"model\", model_dict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736c3e06-c9e2-48ee-b787-3d00efa8d37d",
   "metadata": {},
   "source": [
    "## Full-scale Examples and Comparisons\n",
    "In the following, we will demonstrate how to use the streamer with Retriever in a workflow with real large language model object, \n",
    "and compare the memory usage with and without streaming. To track the memory usage, we use a simple script `utils/log_memory.sh`. \n",
    "Note that the tracked usage is not fully accurate, but it is sufficient to give us a rough idea.\n",
    "\n",
    "With a simple [controller](src/streaming_controller.py) and [executor](src/streaming_executor.py), we simulate a single communication between server and client: server load a `llama-3.2-1b` model, and send to client via three transmission modes: regular, container, and file. This process (clients receiving global model) is often the first stage of a federated learning round, thus the communication burden is realistically reflected.  \n",
    "\n",
    "All three settings: regular, container streaming, and file streaming, are integrated in the same script to avoid extra variabilities.\n",
    "To run the examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a152f129-5b3d-4a17-8355-b4627f9d8e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash regular_transmission.sh\n",
    "! bash container_stream.sh\n",
    "! bash file_stream.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffbeec-fa29-46ca-b103-02e81ac12cce",
   "metadata": {},
   "source": [
    "We then examine the memory usage by comparing the peak memory usage of the three settings. The results are shown below,\n",
    "note that the numbers here are the results of one experiment on one machine, and can be highly variable depending on the system and the environment.\n",
    "\n",
    "| Setting               | Peak Memory Usage (MB) | Job Finishing Time (s) |\n",
    "|-----------------------|------------------------|------------------------|\n",
    "| Regular Transmission  | 42,427                 | 47                     |\n",
    "| Container Streaming   | 23,265                 | 50                     |\n",
    "| File Streaming        | 19,176                 | 170                    |\n",
    "\n",
    "As shown, the memory usage is significantly reduced by using streaming, especially for file streaming, \n",
    "while file streaming takes much longer time to finish the job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3805e71-d929-4d65-9f5d-bc50f799d194",
   "metadata": {},
   "source": [
    "Now that we covered LLM-related features, let's have a [recap](../08.6_recap/recap.ipynb) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248cb159-59a1-45b2-8dd9-88f653b22511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
