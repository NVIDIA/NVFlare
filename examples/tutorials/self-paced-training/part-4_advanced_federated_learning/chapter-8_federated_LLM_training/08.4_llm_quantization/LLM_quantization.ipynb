{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac4d921-f6ec-4d6e-b509-5d49edea9fd5",
   "metadata": {},
   "source": [
    "# Model Quantization for Communication\n",
    "In the previous examples, we used numpy in float32 for communication. To reduce the message size, we can use model precision conversion and quantization \n",
    "from float32 to 16-bit, 8-bit, and 4-bit for communication. Quantization is enabled by NVFlare's [filter mechanism](https://nvflare.readthedocs.io/en/main/programming_guide/filters.html). We can use the following command to run the federated training with model quantization.\n",
    "16-bit is a direct precision conversion, while 8-bit, 4-bit quantization is performed by [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes/tree/main).\n",
    "Note that 4-bit quantizations (`fp4` or `nf4`) need device support."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92762739-4f73-4d52-9fb6-a8f4a3989eb1",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Again, we use one dataset to illustrate the SFT. We download and preprocess [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a9f90e-e7ee-4720-9813-6c94df303083",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://huggingface.co/datasets/databricks/databricks-dolly-15k /tmp/nvflare/dataset/llm/dolly\n",
    "! python utils/preprocess_dolly.py --training_file /tmp/nvflare/dataset/llm/dolly/databricks-dolly-15k.jsonl --output_dir /tmp/nvflare/dataset/llm/dolly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d7cfd0-dd29-4cfd-960e-45315a6a09c7",
   "metadata": {},
   "source": [
    "We run the same SFT pipeline with different quantization configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c04163-b660-4f64-a41d-a8b480dfdf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python sft_job.py --client_ids dolly --data_path /tmp/nvflare/dataset/llm/ --workspace_dir /tmp/nvflare/workspace/llm/dolly_fl_sft_16 --job_dir /tmp/nvflare/workspace/jobs/llm_hf_sft_16 --train_mode SFT --quantize_mode float16\n",
    "! python sft_job.py --client_ids dolly --data_path /tmp/nvflare/dataset/llm/ --workspace_dir /tmp/nvflare/workspace/llm/dolly_fl_sft_8 --job_dir /tmp/nvflare/workspace/jobs/llm_hf_sft_8 --train_mode SFT --quantize_mode blockwise8\n",
    "! python sft_job.py --client_ids dolly --data_path /tmp/nvflare/dataset/llm/ --workspace_dir /tmp/nvflare/workspace/llm/dolly_fl_sft_fp4 --job_dir /tmp/nvflare/workspace/jobs/llm_hf_sft_fp4 --train_mode SFT --quantize_mode float4\n",
    "! python sft_job.py --client_ids dolly --data_path /tmp/nvflare/dataset/llm/ --workspace_dir /tmp/nvflare/workspace/llm/dolly_fl_sft_nf4 --job_dir /tmp/nvflare/workspace/jobs/llm_hf_sft_nf4 --train_mode SFT --quantize_mode normfloat4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559bdca9-b9c7-45dd-93b8-34da452fd3e2",
   "metadata": {},
   "source": [
    "For message reduce, from float32 to 16-/8-/4-bit, the message size (in MB) of Llama-3.2-1B model are reduced to: \n",
    "\n",
    "| Quantization      | Raw Model Size | Quantized Model Size | Quantization Meta Size |\n",
    "|-------------------|----------------|----------------------|------------------------|\n",
    "| float16           | 5716.26        | 2858.13              | 0.00                   |\n",
    "| blockwise8        | 5716.26        | 1429.06              | 1.54                   |\n",
    "| float4            | 5716.26        | 714.53               | 89.33                  |\n",
    "| normalized float4 | 5716.26        | 714.53               | 89.33                  |\n",
    "\n",
    "Note that quantization will generate additional meta data, which can be significant for 4-bit cases.\n",
    "\n",
    "## Model Communication with Tensor\n",
    "In addition, since the model is trained with bf16, instead of first converting to numpy in float32, we can directly communicate with tensor in bf16 to avoid the message size inflation due to the conversion. \n",
    "We can use the following command to run the federated training with direct tensor communication, without and with quantization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f12509-bc5f-49b8-969b-185bb3310e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python sft_job.py --client_ids dolly --data_path /tmp/nvflare/dataset/llm/ --workspace_dir /tmp/nvflare/workspace/llm/dolly_fl_sft_tensor --job_dir /tmp/nvflare/workspace/jobs/llm_hf_sft_tensor --train_mode SFT  --message_mode tensor\n",
    "! python sft_job.py --client_ids dolly --data_path /tmp/nvflare/dataset/llm/ --workspace_dir /tmp/nvflare/workspace/llm/dolly_fl_sft_tensor_fp4 --job_dir /tmp/nvflare/workspace/jobs/llm_hf_sft_tensor_fp4 --train_mode SFT  --message_mode tensor --quantize_mode float4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd4997-493a-40f5-82dc-86ec7a224513",
   "metadata": {},
   "source": [
    "In this case, since the tensor is in bf16, and the quantization reduces it to float4, the message size change is thus:\n",
    "```\n",
    "Before quantization: 2858.13 MB. After quantization: 714.53 MB with meta: 89.33 MB.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63d733d-1a4f-4f41-b7c1-62f204991a46",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe7fba-5ed9-425a-9b6a-678619a2a759",
   "metadata": {},
   "source": [
    "The SFT curves are shown below, we can see it achieves decent alignments. These results show that for the example training schemes and data, model precision conversion / quantization does not significantly impact the training while reducing the message size to 1/2, 1/4, and even 1/8, which can significantly reduce the message size, making it crucial for transmitting LLM updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0632ec-1df1-4894-a1a9-557f568fd468",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/nvflare/workspace/llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4637b943-76da-4b3e-8e57-353d4ec0d17e",
   "metadata": {},
   "source": [
    "Quantization significantly reduced the communication burden by reducinng the message size sent over the network, however at local level, memory usage is still demanding to prepare the messages - large memory needs to be allocated to hold the LLM weights. Therefore, let's move on to the next section addressing this challenge - [LLM Streaming](../08.5_llm_streaming/LLM_streaming.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834dd765-3321-4644-b64d-e3a796579a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
