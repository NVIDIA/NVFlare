{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da4b68c-e68b-4245-b5a6-fba66d3af819",
   "metadata": {},
   "source": [
    "# LLM Supervised Fine-Tuning (SFT) via HuggingFace Trainer APIs\n",
    "In this section, we illustrate how to use [NVIDIA FLARE](https://nvidia.github.io/NVFlare) for Large Language Models (LLMs) SFT task. Unlike the last section [Federated NLP with BERT Model](../08.1_fed_bert/federated_nlp_with_bert.ipynb) where we showed standard Pytorch training logic, it illustrates how to adapt a local training script with [HuggingFace](https://huggingface.co/) Trainer to NVFlare, which is widely used in LLM training.\n",
    "\n",
    "We show supervised fine-tuning (SFT) using the [SFT Trainer](https://huggingface.co/docs/trl/sft_trainer) from [HuggingFace](https://huggingface.co/), together with the [Llama-3.2-1B model](https://huggingface.co/meta-llama/Llama-3.2-1B) to showcase the functionality of federated SFT, allowing HuggingFace models to be trained and adapted to federated application with NVFlare. All other models from HuggingFace can be easily adapted following the same steps.\n",
    "\n",
    "We conducted these experiments on a single 48GB RTX 6000 Ada GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b50353e-1ad9-419c-8712-187a49879978",
   "metadata": {},
   "source": [
    "## Setup\n",
    "To use Llama-3.2-1B model, please request access to the model here https://huggingface.co/meta-llama/Llama-3.2-1B and login with an access token using huggingface-cli. Git LFS is also necessary for downloads, please follow the steps in this [link](https://github.com/git-lfs/git-lfs/blob/main/INSTALLING.md).\n",
    "\n",
    "Install required packages for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7de64-ce02-45c6-8718-1bd6c08c91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814dedb-6d93-4782-a9b5-68644b901184",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We use one dataset to illustrate the SFT. We download and preprocess [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2924146-d635-4e4d-bdf6-50b87a035de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://huggingface.co/datasets/databricks/databricks-dolly-15k /tmp/nvflare/dataset/llm/dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056d9797-3c18-4310-9f4e-26d345623ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/preprocess_dolly.py --training_file /tmp/nvflare/dataset/llm/dolly/databricks-dolly-15k.jsonl --output_dir /tmp/nvflare/dataset/llm/dolly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f5eff-f88c-42b9-aa61-604f68b8b5ec",
   "metadata": {},
   "source": [
    "## Adaptation of Centralized Training Script to Federated\n",
    "To illustrate the adaptation process, we use a single dataset with three training epochs. \n",
    "### One-call training\n",
    "Centralized trainings, as the baseline for comparison with other results, are done with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac443b-25cf-4b45-8876-41ff91bd1ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/hf_sft_peft.py --output_path /tmp/nvflare/workspace/llm/dolly_cen_sft --train_mode SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35043ab0-fdbc-4cf2-b79d-d532724d9364",
   "metadata": {},
   "source": [
    "### Adaptation Step 1: iterative training\n",
    "To adapt the centralized training script to federated application, we first need to \"break\" the single call to `trainer.train()` into iterative calls, one for each round of training.\n",
    "For this purpose, we provided `utils/hf_sft_peft_iter.py` as an example, which is a modified version of `utils/hf_sft_peft.py`.\n",
    "Their differences are highlighted below:\n",
    "\n",
    "![diff](./figs/diff.png)\n",
    "\n",
    "Note that the `trainer.train()` call is replaced by a `for` loop, and the three training epochs becomes three rounds, one epoch per round. \n",
    "\n",
    "This setting (1 epoch per round) is for simplicity of this example. In practice, we can set the number of rounds and local epoch per round according to the needs: e.g. 2 rounds with 2 epochs per round will result in 4 training epochs in total.\n",
    "\n",
    "At the beginning of each round, we intentionally load a fixed model weights saved at the beginning, over-writing the previous round's saved model weights, then call `trainer.train(resume_from_checkpoint=True)` with `trainer.args.num_train_epochs` incremented by 1 so that previous logging results are not overwritten. \n",
    "\n",
    "The purpose of doing so is to tell if the intended weights are succesfully loaded at each round. Without using a fixed starting model, even if the model weights are not properly loaded, the training loss curve will still follow the one-call result, which is not what we want to see. \n",
    "\n",
    "If the intended model weights (serving as the starting point for each round, the \"global model\" for FL use case) is properly loaded, then we shall observe a \"zig-zag\" pattern in the training loss curve. This is because the model weights are reset to the same starting point at the beginning of each round, in contrast to the one-shot centralized training, where the model weights are updated continuously, and the training loss curve should follow an overall decreasing trend.\n",
    "\n",
    "To run iterative training, we use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d9a8ba-4540-43d5-9cec-e867acc4d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/hf_sft_peft_iter.py --output_path /tmp/nvflare/workspace/llm/dolly_cen_sft_iter --train_mode SFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0671750e-ea31-4310-974a-ecadb380ff49",
   "metadata": {},
   "source": [
    "We can observe the SFT curves with tensorboard shown below. As expected, we can see the \"zig-zag\" pattern in the iterative training loss curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa413644-630c-4c4b-be72-828202b2a29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/nvflare/workspace/llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f97580-cc0e-4389-b28a-0b3b07de38a1",
   "metadata": {},
   "source": [
    "### Adaptation Step 2: federated with NVFlare\n",
    "Once we have the iterative training script ready with \"starting model\" loading capability, it can be easily adapted to a NVFlare trainer by using [Client API](../../hello-world/ml-to-fl/pt/README.md).\n",
    "\n",
    "The major code modifications are for receiving and returning the global model (replacing the constant one used by iterative training), as shown below:\n",
    "\n",
    "![diff](./figs/diff_fl_1.png)\n",
    "![diff](./figs/diff_fl_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3c82c6-d6d7-4773-8b00-132510c68adc",
   "metadata": {},
   "source": [
    "### Federated Training Results\n",
    "We run the federated training on a single client using NVFlare Simulator via [JobAPI](https://nvflare.readthedocs.io/en/main/programming_guide/fed_job_api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f3351-b1ca-4da3-8416-3e8ad1a75dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python sft_job.py --client_ids dolly --data_path /tmp/nvflare/dataset/llm/ --workspace_dir /tmp/nvflare/workspace/llm/dolly_fl_sft --job_dir /tmp/nvflare/workspace/jobs/llm_hf_sft --train_mode SFT "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eac7cf-f780-4426-b136-eee693b9b485",
   "metadata": {},
   "source": [
    "The SFT curves are shown below. With some training randomness, the two SFT training loss curves (centralized v.s. federated) align with each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3ecf1f-2729-4183-9aa0-23b91645cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/nvflare/workspace/llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0181a317-07a7-4001-bdd3-894c40a1f293",
   "metadata": {},
   "source": [
    "Now let's move on to the next section of [LLM Parameter-Efficient Fine-Tuning (PEFT)](../08.3_llm_peft/LLM_PEFT.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d64f6-cc13-405b-aea5-90cce58a0171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
