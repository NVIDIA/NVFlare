{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ef8a52f-f0bd-493c-ac70-32d5f7e5b87e",
   "metadata": {},
   "source": [
    "# LLM Parameter-Efficient Fine-Tuning (PEFT) via HuggingFace Trainer APIs\n",
    "Similar to last section [LLM Supervised Fine-Tuning (SFT)](../08.2_llm_sft/LLM_SFT.ipynb), in this section, we illustrate how to use [NVIDIA FLARE](https://nvidia.github.io/NVFlare) for Large Language Models (LLMs) PEFT task with [HuggingFace](https://huggingface.co/) Trainer APIs with [PEFT library](https://github.com/huggingface/peft).\n",
    "\n",
    "We use the same model of the [Llama-3.2-1B model](https://huggingface.co/meta-llama/Llama-3.2-1B) to showcase the functionality of federated PEFT. For PEFT, we used LoRA method, other PEFT methods (e.g. p-tuning, prompt-tuning) can be easily adapted as well by modifying the configs following [PEFT](https://github.com/huggingface/peft) examples.\n",
    "\n",
    "We conducted these experiments on a single 48GB RTX 6000 Ada GPU. \n",
    "\n",
    "To use Llama-3.2-1B model, please request access to the model here https://huggingface.co/meta-llama/Llama-3.2-1B and login with an access token using huggingface-cli.\n",
    "\n",
    "## Setup\n",
    "Git LFS is also necessary for downloads, please follow the steps in this [link](https://github.com/git-lfs/git-lfs/blob/main/INSTALLING.md).\n",
    "\n",
    "Install required packages for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32803fa2-ac9f-4e9e-b5d0-5ad5bac52bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437dc31-3073-4502-af79-7b0e981312a6",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "In this example, we use two datasets to illustrate the PEFT training.\n",
    "\n",
    "We download and preprocess three data sets: [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k), and [Oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c216f-057f-4225-bcec-1839e6139c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://huggingface.co/datasets/databricks/databricks-dolly-15k /tmp/nvflare/dataset/llm/dolly\n",
    "! git clone https://huggingface.co/datasets/OpenAssistant/oasst1 /tmp/nvflare/dataset/llm/oasst1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec5c533-b8a5-4380-be49-2eaf717c1712",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/preprocess_dolly.py --training_file /tmp/nvflare/dataset/llm/dolly/databricks-dolly-15k.jsonl --output_dir /tmp/nvflare/dataset/llm/dolly\n",
    "! python utils/preprocess_oasst1.py --training_file /tmp/nvflare/dataset/llm/oasst1/data/train-00000-of-00001-b42a775f407cee45.parquet --validation_file /tmp/nvflare/dataset/llm/oasst1/data/validation-00000-of-00001-134b8fd0c89408b6.parquet --output_dir /tmp/nvflare/dataset/llm/oasst1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258adcb-1de4-4ecf-a733-7a91b9ab1dd7",
   "metadata": {},
   "source": [
    "## Centralized Baseline\n",
    "We run three centralized baselines as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfcc12d-999d-494f-9c7b-bb747975b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python utils/hf_sft_peft.py --output_path /tmp/nvflare/workspace/llm/dolly_cen_peft --train_mode PEFT\n",
    "! python utils/hf_sft_peft.py --data_path_train /tmp/nvflare/dataset/llm/oasst1/training.jsonl --data_path_valid /tmp/nvflare/dataset/llm/oasst1/validation.jsonl --output_path /tmp/nvflare/workspace/llm/oasst_cen_peft --train_mode PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55dd56b-2ca1-4c80-8b7c-68334079528c",
   "metadata": {},
   "source": [
    "### Federated Training Results\n",
    "We run the federated training on a single client using NVFlare Simulator via [JobAPI](https://nvflare.readthedocs.io/en/main/programming_guide/fed_job_api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cc3483-311a-4858-bbb2-c3f236bb5421",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python peft_job.py --client_ids dolly oasst1 --data_path /tmp/nvflare/dataset/llm/ --workspace_dir /tmp/nvflare/workspace/llm/all_fl_peft --job_dir /tmp/nvflare/workspace/jobs/llm_fl_peft --train_mode PEFT --threads 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da8848c-b1e6-4090-adfe-8398e144cd14",
   "metadata": {},
   "source": [
    "The SFT curves are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0f887-8e43-4206-9e76-101c060854fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /tmp/nvflare/workspace/llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d614e-2f12-45dc-9997-825dd696f421",
   "metadata": {},
   "source": [
    "With SFT and PEFT examples, now let's move on to the next section of [LLM Quantization](../08.4_llm_quantization/LLM_quantization.ipynb), where we will see how to make the message transmission more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05d5a6-df1c-4e23-a928-c0542bca05b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
