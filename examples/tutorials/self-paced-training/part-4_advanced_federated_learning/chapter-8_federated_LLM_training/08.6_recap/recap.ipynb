{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b9e326e-1d97-45c5-ac54-6bf581e4223f",
   "metadata": {},
   "source": [
    "# Summary of Chapter 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1c7385-dd22-4d5c-8d2e-7a73f0b3ac2d",
   "metadata": {},
   "source": [
    "In this chapter, we visited NVFlare's offerings in enabling efficient and robust federated training of language models, especially in the era of LLMs.\n",
    "\n",
    "Specifically, the following items have been covered:\n",
    "1. **[Federated NLP with BERT Model](../08.1_fed_bert/federated_nlp_with_bert.ipynb)**: task-specific model training with BERT in a \n",
    "2. **[Federated LLM Tuning with SFT](../08.2_llm_sft/LLM_SFT.ipynb)**: supervised Fine-Tuning and its role in adapting LLMs in federated learning\n",
    "3. **[Federated LLM Tuning with PEFT](../08.3_llm_peft/LLM_PEFT.ipynb)**: PEFT in adapting LLMs for specific tasks, which can be achieve in a federated setting\n",
    "4. **[Model Quantization for Transmission](../08.4_llm_quantization/LLM_quantization.ipynb)**: reduce the message size with quantization methods so as to address the significant communication burden when performing federated LLM learning with SFT. \n",
    "5. **[Message Streaming for Model Transmission](../08.5_llm_streaming/LLM_streaming.ipynb)**: with quantization reducing communication cost, system memory requirement is still high for prepareing the message on either side. Therefore, we enabled streaming capabilities for more efficient and robust model communication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f3bf8",
   "metadata": {},
   "source": [
    "Key takeaways of this section are:\n",
    "1. NVFlare enables federated training of language models, from BERT to most recent LLMs, under popular training schemes of both SFT and PEFT.\n",
    "2. NVFlare enables efficient and robust communications, accounting for both message transmission and local memory requirements, such that the resource can be best utilized in real-life applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a32effe-3b9d-4cd8-b53c-f91907de9d95",
   "metadata": {},
   "source": [
    "With NVFlare, popular training schemes widely used in the LLM domain can be easily adopted to federated learning paradigm, unleasing more posibilities.\n",
    "\n",
    "Now let's move on to the [Chapter 9](../../chapter-9_flare_low_level_apis/09.0_introduction/introduction.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4321c7c-d56c-49b9-89a2-c503290b8232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
