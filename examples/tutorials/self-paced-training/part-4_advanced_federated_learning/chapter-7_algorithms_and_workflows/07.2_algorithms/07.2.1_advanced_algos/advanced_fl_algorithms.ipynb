{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Advanced FL Algorithms\n",
    "We provide several examples to help you quickly get started with NVFlare.\n",
    "All examples in this folder are based on using [TensorFlow](https://tensorflow.org/) as the model training framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example demonstrates TensorFlow-based federated learning algorithms,\n",
    "[FedAvg](https://arxiv.org/abs/1602.05629), [FedOpt](https://arxiv.org/abs/2003.00295), and [SCAFFOLD](https://arxiv.org/abs/1910.06378) on the CIFAR-10 dataset.\n",
    "\n",
    "In this example, the latest Client APIs were used to implement\n",
    "client-side training logics (details in file\n",
    "[`cifar10_tf_fl_alpha_split.py`](src/cifar10_tf_fl_alpha_split.py)),\n",
    "and the new\n",
    "[`FedJob`](../../../nvflare/job_config/api.py)\n",
    "APIs were used to programmatically set up an\n",
    "NVFlare job to be exported or ran by simulator (details in file\n",
    "[`tf_fl_script_runner_cifar10.py`](tf_fl_script_runner_cifar10.py)),\n",
    "alleviating the need of writing job config files, simplifying\n",
    "development process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install requirements\n",
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_NOTE:_**  We recommend either using a containerized deployment or virtual environment,\n",
    "> please refer to [getting started](https://nvflare.readthedocs.io/en/latest/getting_started.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run experiments\n",
    "\n",
    "The next examples uses simulator to run all experiments. The script\n",
    "[`tf_fl_script_runner_cifar10.py`](tf_fl_script_runner_cifar10.py)\n",
    "is the main script to be used to launch different experiments with\n",
    "different arguments (see sections below for details). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Impact of Data Heterogeneity\n",
    "First, we can run several experiment to simulate different hetereogenous datasplits to see how it impacts the FedAvg algorithm.\n",
    "\n",
    "The CIFAR10 dataset will be downloaded when running any experiment for\n",
    "the first time. TensorBoard summary logs will be generated during\n",
    "any experiment, and you can use TensorBoard to visualize the\n",
    "training and validation process as the experiment runs. Data split\n",
    "files, summary logs and results will be saved in a workspace\n",
    "directory, which defaults to `/tmp` and can be configured by setting\n",
    "`--workspace` argument of the `tf_fl_script_runner_cifar10.py`\n",
    "script.\n",
    "\n",
    "We apply Dirichlet sampling (as implemented in [FedMA](https://github.com/IBM/FedMA)) to\n",
    "CIFAR10 data labels to simulate data heterogeneity among client sites, controlled by an\n",
    "`alpha` value between 0 (exclusive) and 1. A high alpha value indicates less data\n",
    "heterogeneity, i.e., an alpha value equal to 1.0 would result in homogeneous data \n",
    "distribution among different splits.\n",
    "\n",
    "> Note, we use the following environment variables in the training, to prevent\n",
    "> TensorFlow from allocating full GPU memory all at once so we can run the clients in parallel on the same GPU:\n",
    "> `export TF_FORCE_GPU_ALLOW_GROWTH=true && export TF_GPU_ALLOCATOR=cuda_malloc_asyncp`\n",
    ">\n",
    "> You should be able to run the 8 clients in parallel on a GPU with 16GB memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change GPU index if multiple GPUs are available\n",
    "GPU_INDX=0\n",
    "\n",
    "# Run FedAvg with different alpha values\n",
    "for alpha in [1.0, 0.1]:\n",
    "    !python ./tf_fl_script_runner_cifar10.py \\\n",
    "       --algo fedavg \\\n",
    "       --n_clients 8 \\\n",
    "       --num_rounds 50 \\\n",
    "       --batch_size 64 \\\n",
    "       --epochs 4 \\\n",
    "       --alpha $alpha \\\n",
    "       --gpu $GPU_INDX \\\n",
    "       --workspace /tmp # workspace root directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the results by running `tensorboard --logdir /tmp/nvflare/jobs` in a different terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can notice the impact of data heterogeneity by varying the\n",
    "`alpha` value, where lower values cause higher heterogeneity. As can\n",
    "be observed in the table below, performance of the FedAvg decreases\n",
    "as data heterogeneity becomes higher.\n",
    "\n",
    "![Impact of client data\n",
    "heterogeneity](./figs/cifar10_tf_alphas.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 How Advanced FL Algorithms Can Help\n",
    "\n",
    "Now, let's see how advanced FL algorithms can significantly improve performance in the presence of data heterogeneity -— a common real-world challenge where clients' local data distributions differ substantially. Techniques like [FedOpt](https://arxiv.org/abs/2003.00295) extend FedAvg by applying server-side optimization methods such as Adam or momentum SGD to better adapt to diverse client updates. [SCAFFOLD](https://arxiv.org/abs/1910.06378) tackles the issue of client drift by using control variates to correct local updates, helping align them more closely with global objectives. These methods can be integrated into the client training script and server-side controllers  to improve convergence and generalization in non-IID settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 FedOpt: Server-Side Optimization\n",
    "*FedOpt* is a family of algorithms that extends FedAvg by replacing the simple averaging step at the server with a more sophisticated server-side optimizer, such as Adam, Adagrad, or momentum SGD. While clients still perform multiple steps of local training, the global model update at the server is treated as an optimization problem in its own right.\n",
    "\n",
    "**Key idea:** Instead of just averaging the client updates, the server accumulates them and applies an optimizer to adjust the global model. This helps the server adapt more effectively to inconsistent or biased updates from clients, which are common in non-IID settings.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Improved convergence speed.\n",
    "- Greater stability across diverse client data distributions.\n",
    "- Flexibility to tune optimizer hyperparameters for different tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 SCAFFOLD: Correcting Client Drift\n",
    "*SCAFFOLD* addresses a different issue known as client drift, which occurs when clients’ local updates deviate significantly from the direction that would optimize the global objective. This is especially problematic in heterogeneous environments where local optima vary widely.\n",
    "\n",
    "**Key idea:** SCAFFOLD introduces control variates—auxiliary variables maintained at both the client and server—to estimate and correct the drift. During training, each client uses its control variate to adjust its local gradient updates. After training, the server updates its global control variate based on the clients’ contributions.\n",
    "\n",
    "**Benefits:**\n",
    "\n",
    "- Reduces variance across client updates.\n",
    "- Encourages local updates to stay aligned with the global optimization direction.\n",
    "- Leads to faster and more stable convergence in non-IID settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Compare the algorithms\n",
    "Here, we use the `alpha=0.1` setting to compare FedAvg, with FedOpt, and Scaffold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change GPU index if multiple GPUs are available\n",
    "GPU_INDX=0\n",
    "\n",
    "for algo in [\"fedopt\", \"scaffold\"]:\n",
    "    !python ./tf_fl_script_runner_cifar10.py \\\n",
    "       --algo $algo \\\n",
    "       --n_clients 8 \\\n",
    "       --num_rounds 50 \\\n",
    "       --batch_size 64 \\\n",
    "       --epochs 4 \\\n",
    "       --alpha 0.1 \\\n",
    "       --gpu $GPU_INDX \\\n",
    "       --workspace /tmp # workspace root directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can visualize the results by running `tensorboard --logdir /tmp/nvflare/jobs` in a different terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compared the performance of different FL algorithms, with alpha value fixed to 0.1, i.e., a high client data heterogeneity. We can observe from the figure below that, FedOpt and SCAFFOLD achieve better performance, with better convergence rates compared to FedAvg with the same alpha setting. SCAFFOLD achieves that by adding a correction term when updating the client models, while FedOpt utilizes SGD with momentum to update the global model on the server. Both achieve better performance with the same number of training steps as FedAvg.\n",
    "\n",
    "![Comparison of FL Algorithms](./figs/cifar10_tf_algos.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
