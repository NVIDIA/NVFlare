{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514c47e2-420d-4af4-9bf0-cac337c51c39",
   "metadata": {},
   "source": [
    "# Cyclic Weight Transfer (CWT) with Cyclic Workflow\n",
    "\n",
    "In this example, we will demonstrate the Cyclic workflow using the Client API with the CIFAR10 dataset. \n",
    "\n",
    "## Cyclic Workflow\n",
    "<a id = \"cyclic_workflow\"></a>\n",
    "\n",
    "[Cyclic Weight Transfer](https://pubmed.ncbi.nlm.nih.gov/29617797) (CWT) uses the server-controlled `CyclicController` to pass the model weights from one site to the next in a cyclic fashion. \n",
    "\n",
    "In the Cyclic workflow, sites train one at a time, while sending the model to the next site. The order of the sites can be specified as fixed, random, or random (without same in a row).  A round is finished once all sites in the defined order have completed training once, and the final result is returned to the server. This differs from Scatter-and-Gather or FedAvg workflows, wherein all sites train simultaneously and aggregate their results together at the end of a round.\n",
    "\n",
    "## Converting DL training code to FL training code\n",
    "\n",
    "We will be using the [Client API FL code](../code/fl/train.py) trainer converted from the original [Training a Classifer](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4414da7f",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882e014",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea004ad8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bcc365a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../download_cifar10.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65761292",
   "metadata": {},
   "source": [
    "## Job Configuration\n",
    "\n",
    "The client configuration for the trainer with the Client API is standard with the `ScriptRunner`, and our defined `train.py` that supports the `train` task. This is the same configuration used in the SAG pt workflow.\n",
    "\n",
    "For the server configuration we use the `CyclicController` for the workflow, and define arguments such as number of rounds, order of relaying sites, and the `train` task name that the clients support. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f99a25",
   "metadata": {},
   "source": [
    "Let's first copy the required files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a81c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ../train.py train.py\n",
    "! cp ../net.py net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1495568f",
   "metadata": {},
   "source": [
    "## Run Job API\n",
    "\n",
    "Then we can use Job API to easily create a job and run in simulator. We simulate three clients, runing CWT for three rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398bbeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import Net\n",
    "from nvflare import FedJob\n",
    "from nvflare.app_common.workflows.cyclic import Cyclic\n",
    "from nvflare.app_opt.pt.job_config.model import PTModel\n",
    "from nvflare.job_config.script_runner import ScriptRunner\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_clients = 2\n",
    "    num_rounds = 3\n",
    "    train_script = \"train.py\"\n",
    "\n",
    "    job = FedJob(name=\"cyclic\")\n",
    "\n",
    "    # Define the controller workflow and send to server\n",
    "    controller = Cyclic(\n",
    "        num_clients=n_clients,\n",
    "        num_rounds=num_rounds,\n",
    "    )\n",
    "    job.to(controller, \"server\")\n",
    "\n",
    "    # Define the initial global model and send to server\n",
    "    job.to(PTModel(Net()), \"server\")\n",
    "\n",
    "    # Add clients\n",
    "    for i in range(n_clients):\n",
    "        runner = ScriptRunner(\n",
    "            script=train_script,\n",
    "            script_args=\"\",\n",
    "        )\n",
    "        job.to(runner, f\"site-{i+1}\")\n",
    "\n",
    "    job.export_job(\"/tmp/nvflare/jobs\")\n",
    "    job.simulator_run(\"/tmp/nvflare/jobs/workdir\", gpu=\"0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed5d4a",
   "metadata": {},
   "source": [
    "Ensure that the `app_script` is set to the Client API FL `train.py` code and the model path for the persistor is set to the `net.Net`.\n",
    "\n",
    "The previous cell exports the job config and executes the job in NVFlare simulator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48271064",
   "metadata": {},
   "source": [
    "Next, we use a client-controlled version of the cyclic workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef3134",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a54458d3",
   "metadata": {},
   "source": [
    "# Cyclic Weight Transfer (CWT) with Client-Controlled Cyclic Workflow\n",
    "\n",
    "In this example, we will demonstrate the Client-Controlled Cyclic Workflow using the Client API with the CIFAR10 dataset. \n",
    "This differs from the **Server-Controlled Cyclic Workflow** use above, as the server is not involved in communication with sensitive information in the case that is it not trusted. Therefore, NVFlare implements a **peer-to-peer** communication channel for CWT.\n",
    "\n",
    "## Client-Controlled Cyclic Workflow\n",
    "\n",
    "<img src=\"figs/cyclic_ccwf.png\" alt=\"cyclic ccwf\" width=35% height=35% />\n",
    "\n",
    "The `CyclicServerController` is responsible for managing the lifecycle of the job, and will assign `cyclic_config` and `cyclic_start` tasks for configuration and to begin the training workflow. The configuration includes picking the starting client, result clients, and defining the cyclic order.\n",
    "\n",
    "The `CyclicClientController` is responsible for the training logic once `cyclic_start` is sent, and the *Cyclic Workflow* is algorithmically the same as the server-controlled version. The main difference is transferring the model is now encrypted with secure **peer-to-peer** messaging, and only the result clients receive the model, rather than the server.\n",
    "\n",
    "See the [docs](https://nvflare.readthedocs.io/en/main/programming_guide/controllers/client_controlled_workflows.html#cyclic-learning) for more information about the *Client-Controlled Cyclic Workflow*.\n",
    "\n",
    "Again, we will be using the same [Client API FL code](../code/fl/train.py) trainer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737afbd2",
   "metadata": {},
   "source": [
    "## Run Job API\n",
    "\n",
    "Let's use the Job API to create a CCWF Job.\n",
    "\n",
    "We use the `add_cyclic()` function to add our server_config and client_config.\n",
    "\n",
    "First add the `CyclicServerConfig` for the `CyclicServerController` with our desired parameters.\n",
    "Here we set the required number of rounds, and also increase the max status report interval to 300 seconds.\n",
    "\n",
    "Next we add the `CyclicClientConfig` for the `CyclicClientController` that handles all `cyclic_*` tasks and maps the `learn_task_name` to the `train` task handled by the `ScriptRunner` with our `train.py` script. The `PTFileModelPersistor` with the initial `Net()` model and the `SimpleModelShareableGenerator` are also added as components in the `CyclicClientConfig`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b591d963",
   "metadata": {},
   "source": [
    "Then we can use Job API to easily create a job and run in simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c2202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from net import Net\n",
    "\n",
    "from nvflare.app_common.ccwf.ccwf_job import CCWFJob, CyclicClientConfig, CyclicServerConfig\n",
    "from nvflare.app_common.ccwf.comps.simple_model_shareable_generator import SimpleModelShareableGenerator\n",
    "from nvflare.app_opt.pt.file_model_persistor import PTFileModelPersistor\n",
    "from nvflare.job_config.script_runner import ScriptRunner\n",
    "\n",
    "n_clients = 2\n",
    "num_rounds = 3\n",
    "train_script = \"train.py\"\n",
    "\n",
    "job = CCWFJob(name=\"cifar10_cyclic\")\n",
    "\n",
    "job.add_cyclic(\n",
    "    server_config=CyclicServerConfig(num_rounds=num_rounds, max_status_report_interval=300),\n",
    "    client_config=CyclicClientConfig(\n",
    "        executor=ScriptRunner(script=train_script),\n",
    "        persistor=PTFileModelPersistor(model=Net()),\n",
    "        shareable_generator=SimpleModelShareableGenerator(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "job.export_job(\"/tmp/nvflare/jobs/job_config\")\n",
    "job.simulator_run(\"/tmp/nvflare/jobs/workdir\", n_clients=n_clients, gpu=\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251b61df",
   "metadata": {},
   "source": [
    "Again, you need to ensure that the `train_script` is set to the Client API FL `train.py` code and the model path for the persistor is set to `net.Net`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6383ebfa",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we demonstrated two implementations of Cyclic Weight Transfer (CWT) using NVFlare:\n",
    "    \n",
    "1. **Server-Controlled CWT Workflow**:\n",
    "- Uses the `CyclicController` on the server side\n",
    "- Server manages the training order and model distribution\n",
    "- Sites train sequentially, passing model weights to the next site\n",
    "- Server receives and persists the final model after each round\n",
    "    \n",
    "2. **Client-Controlled CWT Workflow**:\n",
    "- Uses `CyclicServerController` and `CyclicClientController`\n",
    "- Implements peer-to-peer communication for enhanced privacy\n",
    "- Server only manages job lifecycle and configuration\n",
    "- Clients handle model transfer and training coordination\n",
    "- Supports encrypted model transfer between clients\n",
    "\n",
    "Key Features:\n",
    "- Both workflows maintain the same algorithmic approach to CWT\n",
    "- Support for fixed, random, or random-without-repetition site ordering\n",
    "- Integration with PyTorch models and training scripts\n",
    "- Built-in support for model persistence and evaluation\n",
    "\n",
    "The main difference between the two approaches is the level of server involvement in the training process, with the client-controlled version providing enhanced privacy through peer-to-peer communication.\n",
    "\n",
    "\n",
    "Next, we will have a look at a [swarm learning](../07.2.3_swarm_learning/swarm_learning.ipynb) example, which also covers client-controlled cross-site evaluation workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbf1152",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
