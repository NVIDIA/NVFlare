{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "851e4197-db50-4054-84da-4faf4129e22c",
   "metadata": {},
   "source": [
    "# Federated XGBoost\n",
    "Several mechanisms have been proposed for training an XGBoost model in a federated learning setting.\n",
    "In these examples, we illustrate the use of NVFlare to carry out *horizontal* federated learning using two approaches: histogram-based collaboration and tree-based collaboration.\n",
    "And *vertical* federated learning using histogram-based collaboration.\n",
    "\n",
    "## Horizontal Federated XGBoost\n",
    "Under horizontal setting, each participant joining the federated learning will have part of \n",
    "the whole data samples / instances / records, while each sample has all the features.\n",
    "\n",
    "### Histogram-based Collaboration\n",
    "The histogram-based collaboration federated XGBoost approach leverages NVFlare integration of [federated learning support](https://github.com/dmlc/xgboost/issues/7778) in the XGBoost open-source library,\n",
    "which allows the existing *distributed* XGBoost training algorithm to operate in a federated manner,\n",
    "with the federated clients acting as the distinct workers in the distributed XGBoost algorithm.\n",
    "\n",
    "In distributed XGBoost, the individual workers share and aggregate gradient information about their respective portions of the training data,\n",
    "as required to optimize tree node splitting when building the successive boosted trees.\n",
    "\n",
    "The shared information is in the form of quantile sketches of feature values as well as corresponding sample gradient and sample Hessian histograms.\n",
    "\n",
    "Under federated histogram-based collaboration, precisely the same information is exchanged among the clients.\n",
    "The main differences are that the data is partitioned across the workers according to client data ownership, rather than being arbitrarily partionable, and all communication is via an aggregating federated [gRPC](https://grpc.io) server instead of direct client-to-client communication.\n",
    "Histograms from different clients, in particular, are aggregated in the server and then communicated back to the clients.\n",
    "\n",
    "### Tree-based Collaboration\n",
    "Under tree-based collaboration, individual trees are independently trained on each client's local data without aggregating the global sample gradient histogram information.\n",
    "Trained trees are collected and passed to the server / other clients for aggregation and / or further boosting rounds.\n",
    "Under this setting, we can further distinguish between two types of tree-based collaboration: cyclic and bagging.\n",
    "\n",
    "#### Cyclic Training\n",
    "\"Cyclic XGBoost\" is one way of performing tree-based federated boosting with \n",
    "multiple sites: at each round of tree boosting, instead of relying on the whole \n",
    "data statistics collected from all clients, the boosting relies on only 1 client's \n",
    "local data. The resulting tree sequence is then forwarded to the next client for \n",
    "next round's boosting. Such training scheme have been proposed in literatures [1] [2].\n",
    "\n",
    "#### Bagging Aggregation\n",
    "\n",
    "\"Bagging XGBoost\" is another way of performing tree-based federated boosting with multiple sites: at each round of tree boosting, all sites start from the same \"global model\", and boost a number of trees (in current example, 1 tree) based on their local data. The resulting trees are then send to server. A bagging aggregation scheme is applied to all the submitted trees to update the global model, which is further distributed to all clients for next round's boosting. \n",
    "\n",
    "This scheme bears certain similarity to the [Random Forest mode](https://xgboost.readthedocs.io/en/stable/tutorials/rf.html) of XGBoost, where a `num_parallel_tree` is boosted based on random row/col splits, rather than a single tree. Under federated learning setting, such split is fixed to clients rather than random and without column subsampling. \n",
    "\n",
    "In addition to basic uniform shrinkage setting where all clients have the same learning rate, based on our research, we enabled scaled shrinkage across clients for weighted aggregation according to each client's data size, which is shown to significantly improve the model's performance on non-uniform quantity splits.\n",
    "\n",
    "Specifically, the global model is updated by aggregating the trees from all clients as a forest, and the global model is then broadcasted back to all clients for local prediction and further training.\n",
    "\n",
    "The XGBoost Booster API is leveraged to create in-memory Booster objects that persist across rounds to cache predictions from trees added in previous rounds and retain other data structures needed for training.\n",
    "\n",
    "## Vertical Federated XGBoost\n",
    "Under vertical setting, each participant joining the federated learning will \n",
    "have part of the whole features, while each site has all the overlapping instances.\n",
    "\n",
    "### Private Set Intersection (PSI)\n",
    "In this tutorial, we assume that all parties hold the same population but different features. \n",
    "\n",
    "In reality, however, not every site will have the same set of data samples (rows), ad we shall use PSI to first compare encrypted versions of the sites' datasets in order to jointly compute the intersection based on common IDs. To learn more about our PSI protocol implementation, see our [psi example](https://github.com/NVIDIA/NVFlare/tree/main/examples/advanced/psi/README.md).\n",
    "\n",
    "### Histogram-based Collaboration\n",
    "Similar to its horizontal counterpart, under vertical collaboration, histogram-based collaboration will \n",
    "aggregate the gradient information from each site and update the global model accordingly, resulting in\n",
    "the same model as the centralized / histogram-based horizontal training. \n",
    "We leverage the [vertical federated learning support](https://github.com/dmlc/xgboost/issues/8424) in the XGBoost open-source library. This allows for the distributed XGBoost algorithm to operate in a federated manner on vertically split data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8718f40-04e8-42e9-85f1-e256d00f4a5c",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Download and Store Data\n",
    "To run the examples, we first download the dataset from this link, which is a single .csv file. By default, we assume the dataset is downloaded, uncompressed, and stored in /tmp/nvflare/dataset/creditcard.csv. Alternatively, the following can be used to download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e134c6-26b0-4535-bdc0-0be1801efd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be5301-ba4c-4139-9749-01064b45fd0d",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "To prepare data for further experiments, we perform the following steps:\n",
    "1. Split the dataset into training/validation and testing sets. \n",
    "2. Split the training/validation set: \n",
    "    * Into \"train\" and \"valid\" for baseline centralized training.\n",
    "    * Into \"train\" and \"valid\" for each client under horizontal setting. \n",
    "    * Into \"train\" and \"valid\" for each client under vertical setting.\n",
    "\n",
    "Data splits used in this example can be generated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10141e9b-b060-4ae6-9c43-303ea3f5ef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "prepare_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317fab1c-f229-4ad3-a263-5ca0974d9245",
   "metadata": {},
   "source": [
    "This will generate data splits for 3 clients under all experimental settings.\n",
    "\n",
    "> **_NOTE:_** In this section, we have divided the dataset into separate columns for each site,\n",
    "> assuming that the datasets from different sites have already been joined using Private Set\n",
    "> Intersection (PSI). However, in practice, each site initially has its own separate dataset. To\n",
    "> combine these datasets accurately, you need to use PSI to match records with the same ID across\n",
    "> different sites. \n",
    "\n",
    "> **_NOTE:_** The generated data files will be stored in the folder `/tmp/nvflare/dataset/xgb_dataset/`,\n",
    "> and will be used by jobs by specifying the path within `config_fed_client`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd0b0d8-2e38-498a-becc-d34ce4b57229",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "We first run the centralized trainings to get the baseline performance, then run the federated XGBoost training using NVFlare Simulator via [JobAPI](https://nvflare.readthedocs.io/en/main/programming_guide/fed_job_api.html).\n",
    "\n",
    "### Centralized Baselines\n",
    "For centralize training, we train the XGBoost model on the whole dataset, as well as subsets with different subsample rates\n",
    "and parallel tree settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b64a449-45f2-45af-8f7f-161a767a2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh \n",
    "run_experiment_centralized.sh ${DATASET_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb6344-3bed-4089-a546-1818588f5751",
   "metadata": {},
   "source": [
    "The results by default will be stored in the folder `/tmp/nvflare/workspace/xgboost/centralized/`.\n",
    "\n",
    "### Horizontal Experiments\n",
    "The following cases will be covered:\n",
    "- Histogram-based collaboration\n",
    "- Tree-based collaboration with cyclic training \n",
    "- Tree-based collaboration with bagging training \n",
    "\n",
    "The experiments can be run with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6189d1-285d-4473-9f9f-71b40c794c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "run_experiment_horizontal_histogram.sh\n",
    "run_experiment_horizontal_tree.sh\n",
    "run_experiment_vertical.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d72a3ee-b504-43ef-8134-c78248f0a2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
