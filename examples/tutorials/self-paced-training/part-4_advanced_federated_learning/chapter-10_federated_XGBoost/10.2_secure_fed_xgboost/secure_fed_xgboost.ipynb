{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8a97688-34ef-4425-9b25-d4c9fb086ad5",
   "metadata": {},
   "source": [
    "# Secure Federated XGBoost with Homomorphic Encryption\n",
    "This section illustrates the use of [NVIDIA FLARE](https://nvflare.readthedocs.io/en/main/index.html) enabling secure federated [XGBoost](https://github.com/dmlc/xgboost) under both horizontal and vertical collaborations.\n",
    "The examples are based on a [finance dataset](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) to perform fraud detection.\n",
    "\n",
    "## Secure Federated Training of XGBoost\n",
    "In last section, we visited several mechanisms for training an XGBoost model in a federated learning setting, including histogram-based vertical, histogram-based horizontal, and tree-based horizontal methods. \n",
    "\n",
    "In this example, we further extend the existing histogram-based horizontal and vertical federated learning approaches to support secure federated learning using homomorphic encryption. Depending on the characteristics of the data to be encrypted, we can choose between [CKKS](https://github.com/OpenMined/TenSEAL) and [Paillier](https://github.com/intel/pailliercryptolib_python).\n",
    "\n",
    "In the following, we illustrate both *histogram-based* *horizontal* and *vertical* federated XGBoost, *with* homomorphic encryption. We leverage the [vertical federated learning with secure features support](https://github.com/dmlc/xgboost/issues/9987) and [horizontal federated learning with secure features support](https://github.com/dmlc/xgboost/issues/10170) in the XGBoost open-source library.\n",
    "\n",
    "### Secure Vertical Federated Training of XGBoost\n",
    "For vertical XGBoost, the active party holds the label, which can be considered “the most valuable asset” for the whole process, and should not be accessed by passive parties. Therefore, the active party in this case is the “major contributor” from a model training perspective, with a concern of leaking this information to passive clients. In this case, the security protection is mainly against passive clients over the label information. \n",
    "\n",
    "To protect label information for vertical collaboration, at every round of XGBoost after the active party computes the gradients for each sample, the gradients will be encrypted before sending to passive parties (Figure 1). Upon receiving the encrypted gradients (ciphertext), they will be accumulated according to the specific feature distribution at each passive party. The resulting cumulative histograms will be returned to the active party, decrypted, and further used for tree building by the active party.\n",
    "\n",
    "![secure_vert_hist](./figs/secure_vert.png)\n",
    "\n",
    "### Secure Horizontal Federated Training of XGBoost\n",
    "For horizontal XGBoost, each party holds “equal status” (whole feature and label for partial population), while the federated server performs aggregation, without owning any data. Hence in this case, clients have a concern of leaking information to the server, and to each other. Hence, the information to be protected is each clients’ local histograms.\n",
    "\n",
    "To protect the local histograms for horizontal collaboration, the histograms will be encrypted before sending to the federated server for aggregation. The aggregation will then be performed over ciphertexts and the encrypted global histograms will be returned to clients, where they will be decrypted and used for tree building. In this way, the server will have no access to the plaintext histograms, while each client will only learn the global histogram after aggregation, rather than individual local histograms.\n",
    "\n",
    "![secure_hori_hist](./figs/secure_hori.png)\n",
    "\n",
    "### Encryption with proper HE schemes\n",
    "With multiple libraries covering various HE schemes both with and without GPU support, it is important to properly choose the most efficient scheme for the specific needs of a particular federated XGBoost setting. Let’s look at one  example, assume N=5 number of participants, M=200K total number of data samples, J=30 total number of features, and each feature histogram has K=256 slots.  Depending on the type of federated learning applications: (Vertical or Horizontal application, we will need different algorithms. \n",
    "\n",
    "For vertical application, the encryption target is the individual g/h numbers, and the computation is to add the encrypted numbers according to which histogram slots they fall into. As the number of g/h is the same as the sample number, for each boosting round in theory:\n",
    "\n",
    "The total encryption needed will be M * 2 = 400k (g and h), and each time encrypts a single number\n",
    "The total encrypted addition needed will be (M – K) * 2 * J ≈ 12m\n",
    "In this case, an optimal scheme choice would be Paillier because the encryption needs to be performed over a single number. Using schemes targeting vectors like CKKS would be a significant waste of space. \n",
    "\n",
    "For horizontal application, on the other hand, the encryption target is the local histograms G/H, and the computation is to add local histograms together to form the global histogram. For each boosting round:\n",
    "\n",
    "The total encryption needed will be N * 2 = 10 (G and H), and each time encrypts a vector of length J * K = 7680\n",
    "The total encrypted addition needed will be (N – 1) * 2 = 18\n",
    "In this case, an optimal scheme choice would be CKKS because it is able to handle a histogram vector (with length 7680, for example) in one shot.\n",
    "\n",
    "We provide encryption solutions both with CPU-only, and with efficient GPU acceleration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f5d29c-0f4b-484e-99d7-75adf79a996c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required packages for data download and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360512ae-bf5b-4868-893f-1e8df392bc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66f4223-63a2-438c-97cd-c0bfbdd6c442",
   "metadata": {},
   "source": [
    "## Encryption Plugins\n",
    "The secure XGBoost requires encryption plugins to work. The plugins are distributed with NVFlare package. If you build NVFlare from source, you need\n",
    "to build the plugins following the instructions in this [README](https://github.com/NVIDIA/NVFlare/blob/main/integration/xgboost/encryption_plugins/README.md)\n",
    "\n",
    "The build process will generate 2 .so files: libcuda_paillier.so and libnvflare.so. Configure the path accordingly following the instructions in \n",
    "[XGBoost User Guide](https://nvflare.readthedocs.io/en/main/user_guide/federated_xgboost/secure_xgboost_user_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de13fdf-da09-4672-a1fe-e5653ceb47bc",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "We follow the same data preparation process as regular federated without secure features. Download and Store Data To run the examples, we use the same data as the last section. We download the dataset and stored in /tmp/nvflare/dataset/creditcard.csv with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4ed08-fcf2-4774-a802-260ffc74870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
    "! mkdir -p /tmp/nvflare/dataset/\n",
    "! cp {path}/creditcard.csv /tmp/nvflare/dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c92744-b97b-4745-9efb-d61230c6cacb",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "To prepare data for further experiments, we perform the following steps:\n",
    "1. Split the dataset into training/validation and testing sets. \n",
    "2. Split the training/validation set: \n",
    "    * Into \"train\" and \"valid\" for baseline centralized training.\n",
    "    * Into \"train\" and \"valid\" for each client under horizontal setting. \n",
    "    * Into \"train\" and \"valid\" for each client under vertical setting.\n",
    "\n",
    "Data splits used in this example can be generated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c65eb5-aa3f-4d6f-bb63-97a9653115cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash prepare_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e955ae-d064-4e69-b8fd-c3e685fd83f8",
   "metadata": {},
   "source": [
    "This will generate data splits for 3 clients under all experimental settings.\n",
    "\n",
    "> **_NOTE:_** In this section, we have divided the dataset into separate columns for each site,\n",
    "> assuming that the datasets from different sites have already been joined using Private Set\n",
    "> Intersection (PSI). However, in practice, each site initially has its own separate dataset. To\n",
    "> combine these datasets accurately, you need to use PSI to match records with the same ID across\n",
    "> different sites. \n",
    "\n",
    "> **_NOTE:_** The generated data files will be stored in the folder `/tmp/nvflare/dataset/xgb_dataset/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a8e32b-22e2-44d0-ae02-e1611eea0c0c",
   "metadata": {},
   "source": [
    "## Run Baseline and Standalone Experiments\n",
    "First, we run the baseline centralized training and standalone federated XGBoost training for comparison.\n",
    "In this case, we utilized the `mock` plugin to simulate the homomorphic encryption process. \n",
    "For more details regarding federated XGBoost and the interface-plugin design,\n",
    "please refer to our [documentation](https://nvflare.readthedocs.io/en/main/user_guide/federated_xgboost/secure_xgboost_user_guide.html).\n",
    "\n",
    "To run all experiments, we provide a script for all settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18fcc9-73fc-4c9d-8083-20bb2ea94fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash run_training_standalone.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be6ddce-bf4b-499e-b135-5592c5328967",
   "metadata": {},
   "source": [
    "This will cover baseline centralized training, federated xgboost run in the same machine\n",
    "(server and clients are running in different processes) with and without secure feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb613fa-045a-4fc5-aa6b-f7a2779619f7",
   "metadata": {},
   "source": [
    "## Run Federated Experiments with NVFlare\n",
    "We then run the federated XGBoost training using NVFlare Simulator via [JobAPI](https://nvflare.readthedocs.io/en/main/programming_guide/fed_job_api.html), without and with homomorphic encryption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297293bf-bf99-4a93-ab91-1757c96e61bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! python xgb_fl_job.py --data_root /tmp/nvflare/dataset/xgb_dataset/horizontal_xgb_data --data_split_mode horizontal\n",
    "! python xgb_fl_job.py --data_root /tmp/nvflare/dataset/xgb_dataset/horizontal_xgb_data --data_split_mode horizontal --secure True\n",
    "! python xgb_fl_job.py --data_root /tmp/nvflare/dataset/xgb_dataset/vertical_xgb_data --data_split_mode vertical\n",
    "! python xgb_fl_job.py --data_root /tmp/nvflare/dataset/xgb_dataset/vertical_xgb_data --data_split_mode vertical --secure True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ad13a-0ca5-47bf-846a-c02ba88ef68a",
   "metadata": {},
   "source": [
    "Secure horizontal needs additional tenseal context provisioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf9590-89a6-454d-bdd8-076af767efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "! nvflare provision -p project.yml -w /tmp/nvflare/workspace/fedxgb_secure/train_fl/works/horizontal_secure\n",
    "! nvflare simulator /tmp/nvflare/workspace/fedxgb_secure/train_fl/jobs/horizontal_secure -w /tmp/nvflare/workspace/fedxgb_secure/train_fl/works/horizontal_secure/example_project/prod_00/site-1 -n 3 -t 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8cbd7-58da-49d6-ae94-9ad013521829",
   "metadata": {},
   "source": [
    "## Results\n",
    "Comparing the AUC results with centralized baseline, we have four observations:\n",
    "1. The performance of the model trained with homomorphic encryption is identical to its counterpart without encryption.\n",
    "2. Vertical federated learning (both secure and non-secure) have identical performance as the centralized baseline.\n",
    "3. Horizontal federated learning (both secure and non-secure) have performance slightly different from the centralized baseline. This is because under horizontal FL, the local histogram quantiles are based on the local data distribution, which may not be the same as the global distribution.\n",
    "4. GPU leads to different results compared to CPU, which is expected as the GPU involves some data conversions.\n",
    "\n",
    "Below are sample results for CPU training:\n",
    "\n",
    "The AUC of vertical learning (both secure and non-secure):\n",
    "```\n",
    "[0]\teval-auc:0.90515\ttrain-auc:0.92747\n",
    "[1]\teval-auc:0.90516\ttrain-auc:0.92748\n",
    "[2]\teval-auc:0.90518\ttrain-auc:0.92749\n",
    "```\n",
    "The AUC of horizontal learning (both secure and non-secure):\n",
    "```\n",
    "[0]\teval-auc:0.89789\ttrain-auc:0.92732\n",
    "[1]\teval-auc:0.89791\ttrain-auc:0.92733\n",
    "[2]\teval-auc:0.89791\ttrain-auc:0.92733\n",
    "```\n",
    "\n",
    "Comparing the tree models with centralized baseline, we have the following observations:\n",
    "1. Vertical federated learning (non-secure) has exactly the same tree model as the centralized baseline.\n",
    "2. Vertical federated learning (secure) has the same tree structures as the centralized baseline, however, it produces different tree records at different parties - because each party holds different feature subsets, as illustrated below.\n",
    "3. Horizontal federated learning (both secure and non-secure) have different tree models from the centralized baseline.\n",
    "\n",
    "|     ![Tree Structures](./figs/tree.base.png)      |\n",
    "|:-------------------------------------------------:|\n",
    "|                 *Baseline Model*                  |\n",
    "| ![Tree Structures](./figs/tree.vert.secure.0.png) |\n",
    "|        *Secure Vertical Model at Party 0*         |\n",
    "| ![Tree Structures](./figs/tree.vert.secure.1.png) |\n",
    "|        *Secure Vertical Model at Party 1*         |\n",
    "| ![Tree Structures](./figs/tree.vert.secure.2.png) |\n",
    "|        *Secure Vertical Model at Party 2*         |\n",
    "\n",
    "In this case we can notice that Party 0 holds Feature 7 and 10, Party 1 holds Feature 14, 17, and 12, and Party 2 holds none of the effective features for this tree - parties who do not hold the feature will and should not know the split value if it.\n",
    "\n",
    "By combining the feature splits at all parties, the tree structures will be identical to the centralized baseline model.\n",
    "\n",
    "When comparing the training and validation accuracy as well as the model outputs,\n",
    "experiments conducted with NVFlare produce results that are identical\n",
    "to those obtained from standalone scripts.\n",
    "\n",
    "For more information on the secure xgboost user guide please refer to\n",
    "https://nvflare.readthedocs.io/en/main/user_guide/federated_xgboost/secure_xgboost_user_guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa00500-6ae1-49fa-82ca-1c5ab13b1339",
   "metadata": {},
   "source": [
    "Now that we covered federated XGBoost under various settings: histogram-based and tree-based, horizontal and vertical, regular and secured. Let's have a [recap](../10.3_recap/recap.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da46255-f85d-4ecb-9832-177725afdc62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
