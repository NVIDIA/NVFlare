{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cada310b-e776-4b9a-aabe-f111c31efcc2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Provision and Run with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0653cbf2-92f2-4a22-8317-69cfb0266e92",
   "metadata": {},
   "source": [
    "Provisioning and deploying using Docker containers could be a convenient way to ensure a uniform OS and software environment across client and server systems. Docker deployment can be used as an alternative to the bare-metal deployment described during the sections before.\n",
    "\n",
    "Before starting, make sure you have Docker installed and set up on all participants' system (server and clients).\n",
    "\n",
    "> **Note**: you will need to install a supported container runtime and the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) to enable support for GPUs.\n",
    "\n",
    "In this notebook, we will walk you through the following items for containerized provision and deployment:\n",
    "- Building a Docker image\n",
    "- Provision a project with the Docker image\n",
    "- Starting the server, clients and admin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fda7082-767b-48fb-b73e-2b75b320bf53",
   "metadata": {},
   "source": [
    "# Build a Docker Image\n",
    "\n",
    "Before starting a containerized provision and deployment, we must build a Docker image with NVIDIA FLARE and other runtime dependencies installed for the project. You have the flexibility to create a Dockerfile however you want, as long as all the dependencies are included in the image. But here is an [example Dockerfile](code/Dockerfile) that you can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92746594-b27c-4c89-80ef-17256c533b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat code/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aba3a0-e65a-49f7-9f27-7975bc335079",
   "metadata": {},
   "source": [
    "Note that this Dockerfile uses the [NGC `pytorch:24.07-py3` base image](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch), clones the NVIDIA FLARE repository and installs the latest version of FLARE. \n",
    "\n",
    "> **Note**: you can customize the base image and the specific version of NVIDIA FLARE to be installed based on the requirements of the project. But be careful: it's recommended to use the same FLARE version for provisioning the project and for building the Docker image. Otherwise, runtime errors might occur.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e2fd0-bbbc-496a-8b81-cec31974fc24",
   "metadata": {},
   "source": [
    "\n",
    "Run the following command to build a Docker image for our provision and deployment later. You can use any name for the image, here we are using the name `nvflare-pt-docker`.\n",
    "\n",
    "> **Note**: it's recommended to run the command in a separate terminal instead of inside this notebook, because the output in the notebook might keep appending to track the status of the docker build command, and may use too much memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f41f3a-03f8-42e4-882f-fee4d5e8342f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker build -t nvflare-pt-docker . -f code/Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a18f9-650d-4b35-bbdc-52ba07761d51",
   "metadata": {},
   "source": [
    "Once the build is complete, you can verify that the Docker image has been built with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740bb26e-87f5-4561-9384-189c0c423822",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images | grep nvflare-pt-docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da97d0cb-d174-4477-8809-c0e16047c8d5",
   "metadata": {},
   "source": [
    "> **Note**: the same Docker image needs to be built on the system of every participant that intends to start using Docker.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a56b5-9cfa-4364-a8e8-0976a839b3f2",
   "metadata": {},
   "source": [
    "# Provisioning a Project with the Docker Image\n",
    "\n",
    "To provision a project with the Docker image, we need to perform the same steps as described previously in [Provision Using `nvflare provision` CLI](../04.1_provision_via_cli/provision_via_cli.ipynb). The only modification needed is to make sure that the server and clients can use the docker image `nvflare-pt-docker` when they start. This can be done by modifying the project configuration file. \n",
    "\n",
    "Take a look at the example configuration file in [`code/project.yml`](code/project.yml). You will notice that the the only difference between this configuration file and [the one in \"Provision Using `nvflare provision` CLI\"](../04.1_provision_via_cli/code/project.yml), is a newly added argument under the `nvflare.lighter.impl.static_file.StaticFileBuilder` builder:\n",
    "```\n",
    "builders:\n",
    "  - path: nvflare.lighter.impl.static_file.StaticFileBuilder\n",
    "    args:\n",
    "      # when docker_image is set to a docker image name, docker.sh will be generated on server/client/admin\n",
    "      docker_image: nvflare-pt-docker\n",
    "```\n",
    "\n",
    "By doing this, the provisioning process will create a `docker.sh` script for all participants of the project.\n",
    "\n",
    "Now let's go ahead and provision a project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bbd6c-2032-4a87-84b5-0b1a84d3bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvflare provision -p ./code/project.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d872d8a-9e44-49dd-94b1-7862b3815ffe",
   "metadata": {},
   "source": [
    "# Starting the Participants with Docker\n",
    "\n",
    "After a successful provisioning, you will find a new `docker.sh` script inside of the server, clients and admin's startup folder. \n",
    "\n",
    "### Starting the Server\n",
    "\n",
    "The content of the server side `docker.sh` (located at [`workspace/example_project/prod_00/localhost/startup/docker.sh`](workspace/example_project/prod_00/localhost/startup/docker.sh) after provisioning) should look like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f9161-4f91-468a-83f7-80005b9f188a",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n",
    "# docker run script for FL server\n",
    "# to use host network, use line below\n",
    "NETARG=\"--net=host\"\n",
    "# or to expose specific ports, use line below\n",
    "#NETARG=\"-p 8003:8003 -p 8002:8002\"\n",
    "DOCKER_IMAGE=nvflare-pt-docker\n",
    "echo \"Starting docker with $DOCKER_IMAGE\"\n",
    "svr_name=\"${SVR_NAME:-flserver}\"\n",
    "mode=\"${1:-r}\"\n",
    "if [ $mode = \"-d\" ]\n",
    "then\n",
    "  docker run -d --rm --name=$svr_name -v $DIR/..:/workspace/ -w /workspace \\\n",
    "  --ipc=host $NETARG $DOCKER_IMAGE /bin/bash -c \\\n",
    "  \"python -u -m nvflare.private.fed.app.server.server_train -m /workspace -s fed_server.json --set secure_train=true config_folder=config org=nvidia\"\n",
    "else\n",
    "  docker run --rm -it --name=$svr_name -v $DIR/..:/workspace/ -w /workspace/ --ipc=host $NETARG $DOCKER_IMAGE /bin/bash\n",
    "fi\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd33fd-ab4f-4130-8a66-ca7860feae32",
   "metadata": {},
   "source": [
    "You can see that this script is just executing `docker run` of the image `nvflare-pt-docker` with multiple options. \n",
    "\n",
    "By default, the `--net=host` option is set for Docker to use the host network. If this is not desired, you can comment out the line that sets the default value, and uncomment the line to set it to map ports manually: `NETARG=\"-p 8003:8003 -p 8002:8002\"`.\n",
    "\n",
    "If you run the `docker.sh` script with the `-d` flag for detached mode, the `docker run` command that is executed will launch the Docker image in detached mode and automatically start the server. Otherwise, the container will start in interactive mode so you can manually start the server with the `start.sh` command in the startup directory by typing `./startup/start.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c17e7c-0aae-44f7-bad9-388802e31b27",
   "metadata": {},
   "source": [
    "Let's go ahead and start the server, by executing the following command in a separate terminal:\n",
    "\n",
    "```bash\n",
    "./workspace/example_project/prod_00/server1/startup/docker.sh -d\n",
    "```\n",
    "\n",
    "This will run a detached container and start up the sever. We can verify that with: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d8c51c-966f-4d1b-b3a5-077bfc18a595",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4fb4e-e395-480e-8e69-d4109550dd2f",
   "metadata": {},
   "source": [
    "You can check the server's log with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a32e258-80a9-43c7-acbc-20a87f28a351",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker logs flserver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3af85d-4885-462a-90ce-fbf27fc2aa45",
   "metadata": {},
   "source": [
    "### Starting the Clients\n",
    "\n",
    "The client side `docker.sh` script is similar to the server side script. \n",
    "\n",
    "```python\n",
    "#!/usr/bin/env bash\n",
    "DIR=\"$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" >/dev/null 2>&1 && pwd )\"\n",
    "# docker run script for FL client\n",
    "# local data directory\n",
    ": ${MY_DATA_DIR:=\"/home/flclient/data\"}\n",
    "# ...\n",
    "\n",
    "NETARG=\"--net=host\"\n",
    "# FL clients do not need to open ports, so the following line is not needed.\n",
    "#NETARG=\"-p 443:443 -p 8003:8003\"\n",
    "DOCKER_IMAGE=nvflare-pt-docker\n",
    "echo \"Starting docker with $DOCKER_IMAGE\"\n",
    "mode=\"${1:--r}\"\n",
    "if [ $mode = \"-d\" ]\n",
    "then\n",
    "  docker run -d --rm --name=site-1 $GPU2USE -u $(id -u):$(id -g) \\\n",
    "  -v /etc/passwd:/etc/passwd -v /etc/group:/etc/group -v $DIR/..:/workspace/ \\\n",
    "  -v $MY_DATA_DIR:/data/:ro -w /workspace/ --ipc=host $NETARG $DOCKER_IMAGE \\\n",
    "  /bin/bash -c \"python -u -m nvflare.private.fed.app.client.client_train -m /workspace -s fed_client.json --set uid=site-1 secure_train=true config_folder=config org=nvidia\"\n",
    "else\n",
    "  docker run --rm -it --name=site-1 $GPU2USE -u $(id -u):$(id -g) \\\n",
    "  -v /etc/passwd:/etc/passwd -v /etc/group:/etc/group -v $DIR/..:/workspace/ \\\n",
    "  -v $MY_DATA_DIR:/data/:ro -w /workspace/ --ipc=host $NETARG $DOCKER_IMAGE /bin/bash\n",
    "fi\n",
    "```\n",
    "\n",
    "The Docker script for FL clients has the additional variable `MY_DATA_DIR` set to `/home/flcient/data` by default which is mapped to `/data` in the container. You can set this to a custom value by running `export MY_DATA_DIR=$SOME_DIRECTORY` before `docker.sh`.\n",
    "\n",
    "The `GPU2USE` variable is also added to keep track of the option for what GPUs to use for the container. You can modify the corresponding line to set the value of `GPU2USE` to use GPUs: \n",
    "- `--gpus=all` to use all available GPUs\n",
    "- `--gpus=2` to use two GPUs\n",
    "- `--gpus=\"device=0,2\"` for specifying specific GPUs where the numbers after `device=` are the GPU IDs\n",
    "\n",
    "> **Note:** In order to use the `--gpus` flag, you need to ensure that you have the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) installed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a609cf-c534-4d6d-a4ab-7002f717bbc4",
   "metadata": {},
   "source": [
    "Let's go ahead and start both clients in detached mode, by executing the following commands in separate terminals:\n",
    "\n",
    "```bash\n",
    "\n",
    "# Start the client site-1\n",
    "./workspace/example_project/prod_00/site-1/startup/docker.sh -d\n",
    "\n",
    "# Start the client site-2\n",
    "./workspace/example_project/prod_00/site-2/startup/docker.sh -d\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd89372a-c60c-4e57-a1db-719ce539fa4b",
   "metadata": {},
   "source": [
    "We can verify that both clients have started with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5c5721-8a3f-4c76-84b6-2fb6d614e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdf59f-59c0-427c-a444-78e0f98d1189",
   "metadata": {},
   "source": [
    "You can check the clients logs with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ebddf0-4b06-418e-8e9e-f5407751ed5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker logs site-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c42f63-e24f-4648-a754-c210f91c2582",
   "metadata": {},
   "source": [
    "And"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f15df4-d71d-4b21-9921-64dec2732ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker logs site-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ed542-832f-4577-85b1-a644a2e8e61c",
   "metadata": {},
   "source": [
    "### Starting the Admin\n",
    "\n",
    "Although there is a `docker.sh` script inside the admin's startup folder, you would not need to run the FLARE Console in a Docker container, since the admin is a user that could potentially connect from anywhere, and do not need any dependencies other than having nvflare installed for running the FLARE Console.\n",
    "\n",
    "Let's go ahead and start the admin user and connect to the FLARE Console, by executing the following command in a separate terminal:\n",
    "```bash\n",
    "./workspace/example_project/prod_00/admin\\@nvidia.com/startup/fl_admin.sh \n",
    "```\n",
    "\n",
    "Enter the admin's email address as defined in the project configuration file: `admin@nvidia.com`. Then use the sub-command `check_status server` to make sure that the server and clients have all successfully started. You should see an output similar to the following:\n",
    "\n",
    "```\n",
    "Engine status: stopped\n",
    "---------------------\n",
    "| JOB_ID | APP NAME |\n",
    "---------------------\n",
    "---------------------\n",
    "Registered clients: 2 \n",
    "-----------------------------------------------------------------------------------------------------\n",
    "| CLIENT | FQCN   | FQSN   | LEAF | TOKEN                                | LAST CONNECT TIME        |\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "| site-1 | site-1 | site-1 | True | 61bbdd09-b08d-4838-ab65-43b0d0fd022f | Wed Feb 19 22:48:17 2025 |\n",
    "| site-2 | site-2 | site-2 | True | 8c390da1-723e-4c26-88d9-a0673c3c9d53 | Wed Feb 19 22:48:14 2025 |\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "Done [4425 usecs] 2025-02-19 23:48:24.404288\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9c1c23-a753-4fdb-9f11-c56c798b33f1",
   "metadata": {},
   "source": [
    "### Submit And Run An Application \n",
    "\n",
    "With the PyTorch dependencies in the Docker containers, you can follow the instructions to export and then submit the [Hello PyTorch Example](https://nvflare.readthedocs.io/en/main/examples/hello_pt_job_api.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6b6953-59ae-4e3e-9287-cefab801bfa2",
   "metadata": {},
   "source": [
    "### Troubleshooting Connections and Configuring /etc/hosts\n",
    "To make sure that your FL server is reachable, assuming your server is running in the Docker container with the mode for `NETARG=\"--net=host\"`, you may need to add an entry in /etc/hosts (or if you are not runinng your FL server on the local host, you need to make sure the DNS is resolvable):\n",
    "```\n",
    "127.0.0.1\t localhost\n",
    "```\n",
    "\n",
    "If you are trying to run this on Mac, you may have to take additional steps to make the FL server reachable. For example, if you use Colima, you may need to figure out a way to add the following into the /etc/hosts of your container for your FL client even if you are using the script with `NETARG=\"--net=host\"`: \n",
    "```\n",
    "192.168.5.2\t localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af257e69-2bb7-49b6-ac6c-f007b0e6618e",
   "metadata": {},
   "source": [
    "### Stopping the Docker Containers\n",
    "\n",
    "After you are done and ready to stop your running Docker containers, you can use the following cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8807d89-49f5-4fd0-bf8d-b9fe01047796",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker stop flserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9547e65-c9a9-4509-b119-03c37065ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop site-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033c9100-f49f-4129-bc93-3f277ce8695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop site-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee204366-238b-4e50-bd78-1be14934b16d",
   "metadata": {},
   "source": [
    "To check that all your Docker containers have stopped, the following should display no more running containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82487d6-4dab-48c9-8991-43cbdf68f8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7702c65-7a46-47f5-a51c-a398daeb1bdc",
   "metadata": {},
   "source": [
    "**That's it, we have learned how to provision and run an FL system with Docker!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab0572-cf8c-4d89-a5ac-74903f523f71",
   "metadata": {},
   "source": [
    "# What's Next\n",
    "\n",
    "Next, we will explore cloud deployment options, starting with [deployment in AWS environment](../04.5_deployment_in_aws/deployment_in_aws.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
