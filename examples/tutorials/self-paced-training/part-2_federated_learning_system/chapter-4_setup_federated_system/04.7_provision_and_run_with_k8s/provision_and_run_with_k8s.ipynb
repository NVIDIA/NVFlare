{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3508a62-2f37-42d8-9bb1-a0b0ac560c0f",
   "metadata": {},
   "source": [
    "# Provision and Run with Kubernetes\n",
    "\n",
    "With NVIDIA FLARE, it is possible to deploy an FL system to an existing Kubernetes (k8s) cluster. In this notebook, we will walk through how to provision and run an FL system in k8s managed by [`microk8s`](https://microk8s.io/).\n",
    "\n",
    "> **Note**:\n",
    ">\n",
    "> To follow along with the example provided in this notebook, you need access to a running k8s system. See the section below on how to install and set one up on Ubuntu.\n",
    ">\n",
    "> This notebook provides a reference k8s deployment example. Depending on the Kubernetes cluster in your real-world use case, your may need to modify the deployment chart, or perform additional operations for a successful deployment.\n",
    "\n",
    "We will cover the following items:\n",
    "- Prepare for k8s deployment\n",
    "- Provision and Generate Helm Chart\n",
    "- Deploy Helm Chart\n",
    "- Verify Deployment and Check Status\n",
    "\n",
    "> **Note:**\n",
    ">\n",
    "> We will only deploy the servers and overseer to k8s, not the clients, since this is what usually makes sense in a real deployment. Clients can join the FL system in a distributed way, using for instance the shell script in their start up kit.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b01cf0-fac8-4e88-b54e-46ac8319abd8",
   "metadata": {},
   "source": [
    "# Prepare for k8s Deployment\n",
    "\n",
    "### 1. Install and Set Up `microk8s`\n",
    "\n",
    "You should first have k8s installed and set up on your system. In this notebook, we use the [`microk8s`](https://microk8s.io/) on an Ubuntu system. Check out [this tutorial](https://ubuntu.com/tutorials/install-a-local-kubernetes-with-microk8s) on how to install it on an Ubuntu system. Make sure you finish all the post-install steps, including setting up the firewall configuration, adding current user to microk8s group etc.\n",
    "\n",
    "Once you've installed `microK8s` and performed all the post-install steps, you need to make sure that the following services are enabled:\n",
    "\n",
    "```\n",
    "    dns                  # (core) CoreDNS\n",
    "    ha-cluster           # (core) Configure high availability on the current node\n",
    "    helm3                # (core) Helm 3 - Kubernetes package manager\n",
    "    hostpath-storage     # (core) Storage class; allocates storage from host directory\n",
    "    ingress              # (core) Ingress controller for external access\n",
    "    registry             # (core) Private image registry exposed on localhost:32000\n",
    "    storage              # (core) Alias to hostpath-storage add-on, deprecated\n",
    "```\n",
    "You can check which services are enabled in `microk8s` by typing the following command in a terminal:\n",
    "```\n",
    "microk8s status\n",
    "```\n",
    "If some of the services are not enabled, enable them with:\n",
    "```\n",
    "microk8s enable [SERVICE NAME]\n",
    "```\n",
    "For instance, use `microk8s enable ingress` to enable the ingress service.\n",
    "\n",
    "### 2. Allow Network Traffic\n",
    "\n",
    "We have to change the k8s cluster to allow incoming network traffic to enter the cluster, such as those from admin consoles and NVIDIA FLARE clients. After the network traffic enters the cluster, the cluster also needs to know how to route the traffic to the deployed services.\n",
    "\n",
    "To do that, we need to first edit the configmap of the ingress service. Execute the following command in a terminal:\n",
    "```bash\n",
    "microk8s kubectl edit cm nginx-ingress-tcp-microk8s-conf -n ingress\n",
    "```\n",
    "This will open a configuration file for the ingress with your default text editor. Then add the following lines to the end of this file:\n",
    "```yaml\n",
    "data:\n",
    "    \"8002\": default/server1:8002\n",
    "    \"8003\": default/server1:8003\n",
    "    \"8102\": default/server2:8102\n",
    "    \"8103\": default/server2:8103\n",
    "    \"8443\": default/overseer:8443\n",
    "```\n",
    "Save and close the file.\n",
    "\n",
    "Next, we need to edit the DaemonSet of ingress to open ports. Execute the following command in a terminal:\n",
    "```bash\n",
    "microk8s kubectl edit ds nginx-ingress-microk8s-controller -n ingress\n",
    "```\n",
    "Similarly, this will also open a configuration file with your default editor. In the configuration file, you need to look for the section `spec.template.spec.containers.ports`, which should contains some port configurations. Add the following to the existing port configurations:\n",
    "```yaml\n",
    "- containerPort: 8443\n",
    "  hostPort: 8443\n",
    "  name: overseer\n",
    "  protocol: TCP\n",
    "- containerPort: 8002\n",
    "  hostPort: 8002\n",
    "  name: server1fl\n",
    "  protocol: TCP\n",
    "- containerPort: 8003\n",
    "  hostPort: 8003\n",
    "  name: server1adm\n",
    "  protocol: TCP\n",
    "- containerPort: 8102\n",
    "  hostPort: 8102\n",
    "  name: server2fl\n",
    "  protocol: TCP\n",
    "- containerPort: 8103\n",
    "  hostPort: 8103\n",
    "  name: server2adm\n",
    "  protocol: TCP\n",
    "```\n",
    "Save and close the file.\n",
    "\n",
    "Restart `microk8s` so the modifications could take effect:\n",
    "```bash\n",
    "microk8s stop\n",
    "microk8s start\n",
    "```\n",
    "\n",
    "### 3. Prepare a Docker Image\n",
    "\n",
    "We need to provide a Docker image that contains NVIDIA FLARE and dependencies, so that the deployed services can use to spin up containers. Since we've already built one such image `nvflare-pt-docker` during [Provision and Run with Docker](../04.4_provision_and_run_with_docker/provision_and_run_with_dockers.ipynb#Build-a-Docker-Image), let's go ahead and use this image.\n",
    "\n",
    "For `microk8s` to successfully recognize the image, we need to first re-tag the image as `localhost:32000/nvflare-pt-docker` and push it to the `microk8s` internal local registry. Let's execute the following commands in a terminal:\n",
    "\n",
    "```\n",
    "docker tag nvflare-pt-docker localhost:32000/nvflare-pt-docker\n",
    "docker push localhost:32000/nvflare-pt-docker\n",
    "```\n",
    "> **Note**: make sure you have the `registry` service up-and-running in `microk8s`. The registry service will listen to port 32000 on localhost.\n",
    "\n",
    "It could take a couple of minutes before `microk8s` finishes copying the image to it's local registry.\n",
    "\n",
    "That's all for preparation, we are ready to proceed with project provisioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0752ef4-b4d2-40b1-a5f0-38bda8dc9788",
   "metadata": {},
   "source": [
    "# Provision and Generate Helm Chart\n",
    "\n",
    "To be able to deployment an FL system in k8s, the first thing we need to do is to generate a [Helm Chart](https://www.redhat.com/en/topics/devops/what-is-helm). \n",
    "\n",
    "In NVIDIA FLARE, we can custom the project configuration file and let the provision process generate a Helm Chart, by adding the following in the builder section of the configuration file:\n",
    "```yaml\n",
    "- path: nvflare.lighter.impl.helm_chart.HelmChartBuilder\n",
    "    args:\n",
    "    docker_image: localhost:32000/nvflare-pt-docker\n",
    "```\n",
    "\n",
    "The `docker_image` is the image that we prepared during the previous section, and will be used for all pods running in the k8s.\n",
    "\n",
    "An example configuration file that includes the `HelmChartBuilder` is provided in [`code/project.yml`](code/project.yml). This configuration file was set to enable the high-availability mode. Let's provision a project with this configuration file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be67ab8-97bc-4800-8de4-6d376c83f928",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvflare provision -p code/project.yml -w /tmp/workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f3376-7503-495b-b684-8f4167ce781c",
   "metadata": {},
   "source": [
    "Inside of the provision workspace, you can find generate packages for all participants, as well as the Helm Chart (folder `nvflare_hc`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d8f52a-9cf7-43fa-9194-81b11c3b88b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree /tmp/workspace/example_project/prod_00/ -L 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58ff0d-8466-400d-b86f-438029608ef6",
   "metadata": {},
   "source": [
    "Feel free to modify the contant of the Helm Chart to custom it. Once you are done, we can proceed to the deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67eb82cb-a8ff-4b59-91e9-fd47454c922f",
   "metadata": {},
   "source": [
    "# Deploy Helm Chart\n",
    "\n",
    "Run the following commands in a terminal to deploy the Helm Chart to k8s:\n",
    "```bash\n",
    "mkdir -p /tmp/nvflare\n",
    "cd /tmp/workspace/example_project/prod_00/\n",
    "microk8s helm3 install --set workspace=$(pwd) --set persist=/tmp/nvflare nvflare-helm-chart-demo nvflare_hc/\n",
    "```\n",
    "\n",
    "Note that we set `workspace/example_project/prod_00/`, which contains all provisioned packages as the `workspace` inside k8s, and mount `/tmp/nvflare` from the host system as a persistent volume in k8s. The name of the application we are deploying is `nvflare-helm-chart-demo`. Feel free to modify these settings.\n",
    "\n",
    "Once the deployment is finished, you should see output similar to the following:\n",
    "```\n",
    "NAME: nvflare-helm-chart-demo\n",
    "LAST DEPLOYED: Fri Feb 21 00:59:05 2025\n",
    "NAMESPACE: default\n",
    "STATUS: deployed\n",
    "REVISION: 1\n",
    "TEST SUITE: None\n",
    "```\n",
    "\n",
    "And you can check that the application is deployed in k8s using the following command:\n",
    "```bash\n",
    "microk8s helm3 list -A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b6a4e1-4092-423c-8502-1890c21983b1",
   "metadata": {},
   "source": [
    "# Verify Deployment and Check Status\n",
    "\n",
    "To verify the deployment, use the following command:\n",
    "```bash\n",
    "microk8s kubectl get pods\n",
    "```\n",
    "This should list the `overseer`, `server1` and `server2` as running pods:\n",
    "```\n",
    "NAME                        READY   STATUS    RESTARTS   AGE\n",
    "overseer-5cfbf5465b-v4fpn   1/1     Running   0          14s\n",
    "server1-768bc648c5-ddtvn    1/1     Running   0          14s\n",
    "server2-5d899d4856-dvrf7    1/1     Running   0          14s\n",
    "```\n",
    "\n",
    "If any of the pods display error status, you can debug all pods status with:\n",
    "```bash\n",
    "microk8s kubectl describe pods\n",
    "```\n",
    "\n",
    "You can also debug individual pod using:\n",
    "```bash\n",
    "microk8s kubectl logs [POD_NAME]\n",
    "```\n",
    "where `[POD_NAME]` is the `NAME` displayed with `microk8s kubectl get pods`.\n",
    "\n",
    "To functionally check that the servers and overseer is running normally, you can try connecting as admin user to the FL system using FLARE Console. \n",
    "\n",
    "Launch the FLARE console with the `fl_admin.sh` script inside the admin startup kit (located under `workspace/example_project/prod_00/admin@nvidia.com`). When prompt to enter the admin's email address, enter `admin@nvidia.com`. Once inside the FLARE Console, use the `check_status server` command to see the status.\n",
    "\n",
    "Note that you might need to modify the `/etc/hosts` of the system which you are using to run the FLARE Console, by adding `[IP_AADRESS] overseer server1 server2` to the file. The `[IP_AADRESS]` is the IP address of the system running the k8s deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1f94c5-06c5-4ae1-b614-e6d7f0766a66",
   "metadata": {},
   "source": [
    "**That's it, we've learned how to provision a project and deploy the servers and overseer in k8s!**\n",
    "\n",
    "If you wish to do some clean up, use the following commands to delete all deloyments and uninstall the deployed application:\n",
    "```bash\n",
    "microk8s kubectl delete --all deployments\n",
    "microk8s helm3 uninstall nvflare-helm-chart-demo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d643d22-e4fc-4196-9fd5-f509600bd0df",
   "metadata": {},
   "source": [
    "# What's Next\n",
    "\n",
    "We have finished all the content for this chapter, let's have [a recap on what we've learned](../04.8_recap/recap.ipynb)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
