{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa7a8095",
   "metadata": {},
   "source": [
    "# Chapter 3: Provision, Run and Monitor a Federated Project\n",
    "\n",
    "Welcome to Chapter 3 of the course 5 minutes to Federated Learning with NVIDIA FLARE!\n",
    "\n",
    "In chapter 2, we learned how to develop a federated application using NVIDIA FLARE's core APIs, as well as how to configure it and run it in a simulated environment. In this chapter, we take it to the next level by introducing project provisioning, PoC mode and project monitoring in NVIDIA FLARE. We first introduce the concept of project provisioning and explain how FLARE generates a federated project for real-world scenario. Then we introduce the Proof-of-Concept (PoC) mode in NVIDIA FLARE, a suite of command-line tools that allow FL developers to run a provisioned project in a sandbox environment on a single PC. Next, we walk you through NVIDIA FLARE's tools and APIs that allow you to monitor and track the progress of the federated project. Finally, we illustrate all these features with a hands-on example of CIFAR10 image classification using Pytorch.\n",
    "\n",
    "After this chapter, you will:\n",
    "- Have a basic understanding of project provision, PoC mode and project monitoring in NVIDIA FLARE\n",
    "- Be able to generate a federated project that can be deployed as a real-world application \n",
    "- Be able to run a provisioned federated job in a sanbox environment \n",
    "- Be able to track and monitor a running federated project using FLARE's tools and APIs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67739de-a1d7-43bb-a984-bbf5bb4a941b",
   "metadata": {},
   "source": [
    "# Provisioning in NVIDIA FLARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c4c5a2-e655-4040-8fd9-ffab42fd522f",
   "metadata": {},
   "source": [
    "In Chapter 1, we saw togther how to develop an example federated application using NVIDIA FLARE's APIs, and run the application in a simulated environment using FL Simulator. In that example, we defined 1 server and 2 clients as participants of the FL project. The generation and management of the participants are all managed by the FL Simulator tool, and we didn't really have the need to set them up. While this is all convenient for prototyping, for real-world federated applications, we need more fine-grained control in setting up different participants. \n",
    "\n",
    "In real-world FL deployment, different participants have their own clearly defined roles. Apart from server and clients, FL projects are usually assigned with administrators who manage and monitor project status. Proper user authentication and authorization mechanisms need to be set up to ensure a secure deployment environment for all participants to establish mutually-trusted communication channels. Also, due to the distributed nature of FL projects, all participants would need to be able to join the FL system from different locations through secure network connections, or in a dynamic way. On top of all these, extra security and resource management features might also be required due to different participants requirements in terms of privacy and compute resources.\n",
    "\n",
    "To have a managed way to properly set up an FL project and account for all the aspects mentioned above, NVIDIA FLARE introduces the concept of [**Provisioning**](https://nvflare.readthedocs.io/en/main/programming_guide/provisioning_system.html#provisioning-in-nvidia-flare). In FLARE, **provisioning is the process of settting up a federated project and participants for a scalable, extensible and secure real world deployment**. Provisioning in NVIDIA FLARE is used to:\n",
    "- Establish the identities of the server, clients, and administrators in the federation\n",
    "- Generate startup kit for each participant\n",
    "- Set up secure communication channels between participants\n",
    "\n",
    "<img src=\"../images/provision.png\" alt=\"NVFLARE Provision\" width=30% />\n",
    "\n",
    "In FLARE, provisioning can be done using either the CLI tool `nvflare provision` or webUI-based [Dashboard](https://nvflare.readthedocs.io/en/main/user_guide/dashboard_ui.html). In this chapter, we only focus on the CLI tool and we will cover Dashboard in the next chapter. \n",
    "\n",
    "The provisioning process typically involves the following steps:\n",
    "1. Configure the project using a configuration file\n",
    "2. Use the FLARE provisioning tool to generate startup kits from project configuration file\n",
    "3. Distribute the startup kits to participants\n",
    "\n",
    "Let's walk through these steps together.\n",
    "\n",
    "### 1. Configure the project\n",
    "\n",
    "The input to the povisioning process is a project configuration file (for instance typically `project.yaml`). This configuration is used to set up the FL project, and generally includes definitions for:\n",
    "- Project meta-data\n",
    "- Participants of the project\n",
    "- Builders to build project workspace\n",
    "\n",
    "An example project configuration file is provided in [`../files/project.yml`](../files/project.yml). This file was generated using the `nvflare provision` CLI tool without any arguments. You can also find master templates for more complete project configurations [here](https://nvflare.readthedocs.io/en/main/programming_guide/provisioning_system.html#default-project-yml-file).\n",
    "\n",
    "> **NOTE**:\n",
    "> When running this CLI tool without arguments, you will be prompted to select whether the generated configuration file should include [*high-availability*](https://nvflare.readthedocs.io/en/main/programming_guide/high_availability.html) features. We will not cover *high-availability* in this course, but the example file was generated without high-availability features. \n",
    "\n",
    "Let us look at the content of [`../files/project.yaml`](../files/project.yml) together (some of the details and comments are removed to faciliate explanation):\n",
    "```yaml\n",
    "api_version: 3\n",
    "name: example_project\n",
    "description: NVIDIA FLARE sample project yaml file\n",
    "\n",
    "participants:\n",
    "  - name: server1\n",
    "    type: server\n",
    "    org: nvidia\n",
    "    fed_learn_port: 8002\n",
    "    admin_port: 8003\n",
    "\n",
    "  - name: site-1\n",
    "    type: client\n",
    "    org: nvidia\n",
    "\n",
    "  - name: site-2\n",
    "    type: client\n",
    "    org: nvidia\n",
    "    \n",
    "  - name: admin@nvidia.com\n",
    "    type: admin\n",
    "    org: nvidia\n",
    "    role: project_admin\n",
    "\n",
    "builders:\n",
    "  - path: nvflare.lighter.impl.workspace.WorkspaceBuilder\n",
    "      ...\n",
    "  - path: nvflare.lighter.impl.template.TemplateBuilder\n",
    "      ...\n",
    "  - path: nvflare.lighter.impl.static_file.StaticFileBuilder\n",
    "      ...\n",
    "  - path: nvflare.lighter.impl.cert.CertBuilder\n",
    "      ...\n",
    "  - path: nvflare.lighter.impl.signature.SignatureBuilder\n",
    "```\n",
    "\n",
    "The configuration file is organized into the following 3 sections:\n",
    "\n",
    "**Project mete-data**\n",
    "  - The `api_version`: for current release of NVIDIA FLARE, the `api_version` is set to 3\n",
    "  - The `name` of the FL project, in this case, `example_project`\n",
    "  - And a short `description` of the FL project\n",
    "\n",
    "**Participants**\n",
    "\n",
    "This section is a crucial part that defines all the [participants](https://nvflare.readthedocs.io/en/main/programming_guide/provisioning_system.html#participant) involved in the project. There are multiple types of participants defined in FLARE, most common types include `server`, `client` and `admin`. Some participants are referred to as [Sites](https://nvflare.readthedocs.io/en/main/user_guide/security/terminologies_and_roles.html#site), which represent computing systems that run FLARE applications, for instance, the `server` and `client`. While some participants are referred to as [Users](https://nvflare.readthedocs.io/en/main/user_guide/security/terminologies_and_roles.html#user), who are human participants, with different access priviledges to query, monitor or manage the project, for instance, the `admin`. Developers can aslo include other types for instance the `overseer` for high-availability mode, or even add custom types, but we will not go into details on this in this course. \n",
    "\n",
    "As we can see in the example `project.yml` file, the following participants are defined:\n",
    "- `server`: here we defined 1 server. The name of the server should in general be a [fully qualified domain name](https://en.wikipedia.org/wiki/Fully_qualified_domain_name) (FQDN) to make sure that other participants can establish network connections to the server from any location. The server name can also be a system-wide known hostname.\n",
    "- `client`s: here we defined 2 clients with names `site-1` and `site-2`.\n",
    "- `admin`: here we defined 1 `project_admin` with name `admin@nvidia.com`, who has the most elevated access priviledges in the whole project. There can only 1 project admin for each project.\n",
    "\n",
    "**Builders**\n",
    "\n",
    "In NVIDIA FLARE provisioning, [builders](https://nvflare.readthedocs.io/en/main/programming_guide/provisioning_system.html#builder) are a series of Python classes that work together to generate startup kits for all participants, based on the configurations defined in the project configuration file. Builders create various aspects of the startup kits for all participants, such as workspace structure, configuration files, and security credentials. Builders specified in a configuration file will be invoked in the same order as listed in the `builders` section. This example `project.yml` file shows the usage of common builders:\n",
    "- `WorkspaceBuilder`: Creates the basic workspace structure\n",
    "- `TemplateBuilder`: Processes template files\n",
    "- `StaticFileBuilder`: Copies static files into the workspace\n",
    "- `CertBuilder`: Generates certificates and keys for secure communication\n",
    "- `SignatureBuilder`: Creates signatures for tamper-proofing\n",
    "\n",
    "Developers can also create custom builders to construct more specific and customized provisioning output. We will not deep dive into how builders work and how they can be customized. For now, the most important thing to know about builders is that they execute in specific order to build the startup kits as results of a provisioning process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cabd488-facb-4dbd-9d52-eb8fedd41969",
   "metadata": {},
   "source": [
    "### 2. Generate startup kits\n",
    "\n",
    "Let's run the provisioning process with the example configuration file [`../files/project.yml`/](../files/project.yml/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e2bad57-6e4d-43de-b061-a9d54a571183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project yaml file: /flare/notebooks/../files/project.yml.\n",
      "Generated results can be found under /flare/notebooks/./temp-workspace/example_project/prod_00. \n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./temp-workspace\n",
    "!nvflare provision -p ../files/project.yml -w ./temp-workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b1ab97-c46c-4a93-b038-ad6ca7b12bfb",
   "metadata": {},
   "source": [
    "Now let's check the structure of the output generated by provisioning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7868681-409a-4f6c-81d6-b21d92b0de2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./temp-workspace\u001b[0m\n",
      "└── \u001b[01;34mexample_project\u001b[0m\n",
      "    ├── \u001b[01;34mprod_00\u001b[0m\n",
      "    │   ├── \u001b[01;34madmin@nvidia.com\u001b[0m\n",
      "    │   │   ├── \u001b[01;34mlocal\u001b[0m\n",
      "    │   │   ├── \u001b[01;34mstartup\u001b[0m\n",
      "    │   │   └── \u001b[01;34mtransfer\u001b[0m\n",
      "    │   ├── \u001b[01;34mserver1\u001b[0m\n",
      "    │   │   ├── \u001b[01;34mlocal\u001b[0m\n",
      "    │   │   ├── \u001b[00mreadme.txt\u001b[0m\n",
      "    │   │   ├── \u001b[01;34mstartup\u001b[0m\n",
      "    │   │   └── \u001b[01;34mtransfer\u001b[0m\n",
      "    │   ├── \u001b[01;34msite-1\u001b[0m\n",
      "    │   │   ├── \u001b[01;34mlocal\u001b[0m\n",
      "    │   │   ├── \u001b[00mreadme.txt\u001b[0m\n",
      "    │   │   ├── \u001b[01;34mstartup\u001b[0m\n",
      "    │   │   └── \u001b[01;34mtransfer\u001b[0m\n",
      "    │   └── \u001b[01;34msite-2\u001b[0m\n",
      "    │       ├── \u001b[01;34mlocal\u001b[0m\n",
      "    │       ├── \u001b[00mreadme.txt\u001b[0m\n",
      "    │       ├── \u001b[01;34mstartup\u001b[0m\n",
      "    │       └── \u001b[01;34mtransfer\u001b[0m\n",
      "    ├── \u001b[01;34mresources\u001b[0m\n",
      "    │   ├── \u001b[00maws_template.yml\u001b[0m\n",
      "    │   ├── \u001b[00mazure_template.yml\u001b[0m\n",
      "    │   └── \u001b[00mmaster_template.yml\u001b[0m\n",
      "    └── \u001b[01;34mstate\u001b[0m\n",
      "        └── \u001b[00mcert.json\u001b[0m\n",
      "\n",
      "21 directories, 7 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./temp-workspace -L 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74dfe53-cef0-4714-8fb4-77069646af90",
   "metadata": {},
   "source": [
    "We can see that provisioning generates a well-defined directory hierarchy.\n",
    "\n",
    "At the top level, we have a root folder with the FL project's name, in this case, `example_project`. Under `example_project`, we can see multiple subfolders:\n",
    "- `resources`: this directory may contain shared resources or additional files needed for the project.\n",
    "- `state`: this is a directory to maintain state information about the provisioning process or current status of participants.\n",
    "- `prod_NN`: this folder contains the **startup kits** generated from a successful provisioning command for all participants. The number `NN` increases with each successful provision run, indicating different provisioning sessions. In this example case, since we are running the provisioning command for the first time, the folder name is `prod_00`.\n",
    "\n",
    "**A startup kit is a folder with a set of files, scripts and credentials generated by provisioning for each participant in the FL project**. A startup kit is local to a specific participant and typically include:\n",
    "- Authentication credentials\n",
    "- Authorization policies\n",
    "- Signatures for tamper-proof mechanisms\n",
    "- Convenient shell scripts for launching the participant\n",
    "\n",
    "We can see that in this example, there are 4 startup kits generated, one for each participants, i.e., 1 server, 2 clients and 1 admin. Each folder is named after its corresponding participant's name as indicated in the configuration file. \n",
    "\n",
    "Let's look into the content of the generated startup kits for the server, clients and admin. \n",
    "\n",
    "Server and clients have similar startup kit content structure, so let's just display the server startup kit files with the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22b837ec-e5a5-4ffd-ab4a-0bbb0cc5f23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./temp-workspace/example_project/prod_00/server1/\u001b[0m\n",
      "├── \u001b[01;34mlocal\u001b[0m\n",
      "│   ├── \u001b[00mauthorization.json.default\u001b[0m\n",
      "│   ├── \u001b[00mlog.config.default\u001b[0m\n",
      "│   ├── \u001b[00mprivacy.json.sample\u001b[0m\n",
      "│   └── \u001b[00mresources.json.default\u001b[0m\n",
      "├── \u001b[00mreadme.txt\u001b[0m\n",
      "├── \u001b[01;34mstartup\u001b[0m\n",
      "│   ├── \u001b[00mfed_server.json\u001b[0m\n",
      "│   ├── \u001b[00mrootCA.pem\u001b[0m\n",
      "│   ├── \u001b[00mserver.crt\u001b[0m\n",
      "│   ├── \u001b[00mserver.key\u001b[0m\n",
      "│   ├── \u001b[00msignature.json\u001b[0m\n",
      "│   ├── \u001b[01;32mstart.sh\u001b[0m\n",
      "│   ├── \u001b[01;32mstop_fl.sh\u001b[0m\n",
      "│   └── \u001b[01;32msub_start.sh\u001b[0m\n",
      "└── \u001b[01;34mtransfer\u001b[0m\n",
      "\n",
      "4 directories, 13 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./temp-workspace/example_project/prod_00/server1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94c23e9-4948-419b-972b-31f4d4e7d1e5",
   "metadata": {},
   "source": [
    "Different subfolders are organized as follows: \n",
    "- The `local` subfolder: this folder contains configuration files for site-specific local policies for authorization, logging, privacy and resource access. Each site can modify these configuration files to set up their local policies. We will see more details on local site policy management in the next chapter.\n",
    "- The `startup` subfolder: this folder contains shell script for a participant to start / join (`start.sh`) or leave the FL project (`stop_fl.sh`). It also contains certificates, signature and key files, which are essential to maintain secure connections between different participants. Noted that the signatures and certificates files are integral to ensuring the security and authenticity of a participants. Manual modification of these files after provisioning may prevent a participant to connect to the FL system. \n",
    "- The `transfer` subfolder is used to store artifacts such as custom application, scripts or files during project runtime.\n",
    "\n",
    "Now let's look at the content of `admin`'s startup kit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb7a2c9e-d960-4f69-a289-7e8d5ee1dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./temp-workspace/example_project/prod_00/admin@nvidia.com/\u001b[0m\n",
      "├── \u001b[01;34mlocal\u001b[0m\n",
      "├── \u001b[01;34mstartup\u001b[0m\n",
      "│   ├── \u001b[00mclient.crt\u001b[0m\n",
      "│   ├── \u001b[00mclient.key\u001b[0m\n",
      "│   ├── \u001b[00mfed_admin.json\u001b[0m\n",
      "│   ├── \u001b[01;32mfl_admin.sh\u001b[0m\n",
      "│   ├── \u001b[00mreadme.txt\u001b[0m\n",
      "│   └── \u001b[00mrootCA.pem\u001b[0m\n",
      "└── \u001b[01;34mtransfer\u001b[0m\n",
      "\n",
      "4 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./temp-workspace/example_project/prod_00/admin@nvidia.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865ea69-0566-4f12-85c0-a560db23630f",
   "metadata": {},
   "source": [
    "The `admin`'s startup kit has similar structure. \n",
    "- The `local` subfolder is empty, since local policies are not needed for `admin`, a human participant who manages the project.\n",
    "- In the `startup` subfolder, apart from certificates and keys, the `fl_admin.sh` shell script allows the `admin` user to login to [**FLARE Console**](https://nvflare.readthedocs.io/en/main/real_world_fl/operation.html#operating-nvflare) to perform management tasks from anywhere with secure network connection. We will look at the **FLARE Console** later.\n",
    "- The `transfer` folder in `admin`'s startup kit is the default location to hold various runtime data, for instance, downloads of federated jobs from the server workspace when they finish. It can also be used to link or hold federated applications developed using FLARE, so that the `admin` user can easily submit the application to the server for running. We will see how this works in details later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43a380-5b37-4c4d-b3a2-b7983e473b48",
   "metadata": {},
   "source": [
    "### 3. Distribute the startup kits to participants\n",
    "\n",
    "Now with startup kits generated, the last step is to distribute these kits to the corresponding participants. You can use email, sftp etc. to do so, as long as you can ensure that it is secure. In general, each site should have an organization admin to receive or download the startup kits. The organization admin can then install their own packages, start the services, map the data location, and instrument the authorization policies and organization level site privacy policies.\n",
    "\n",
    "**That's it! We have learned how to properly set up a federated project and generate startup kits for all participants using FLARE's provisioning process!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151f682d-bd5f-4ef2-8cd3-197bdfccd0c8",
   "metadata": {},
   "source": [
    "# Proof-of-Concept Mode\n",
    "\n",
    "Now let's put what we've learn about FLARE provisioning into action: let us run a federated application with properly provisioned server, clients and admin!\n",
    "\n",
    "While it sounds exciting, this is however not a trivial task. We would have to set up distributed computing environments for the server, clients and admin. It would be nice if there is a way to alleviate this set-up. Introducing NVIDIA FLARE's Proof-of-Concept, or [PoC mode](https://nvflare.readthedocs.io/en/main/user_guide/poc_command.html). PoC mode allows users to test the features of FLARE provisioning and deployment on a single machine, without the overhead of a true distributed deployment and the need to establish secure communication between server and client systems.\n",
    "\n",
    "Compared to the FL Simulator, where the job run is automated on a single system, PoC mode allows you to establish and connect distinct server, client and admin in different processes. PoC mode also opens possibility for admin to orchestrate and manage the project using the [FLARE Console](https://nvflare.readthedocs.io/en/main/real_world_fl/operation.html#operating-nvflare) or [FLARE API](https://nvflare.readthedocs.io/en/main/real_world_fl/flare_api.html), making it a useful tool in preparation for a real-world distributed deployment.\n",
    "\n",
    "Developers often start with PoC mode to test their applications locally before transitioning to a production environment where provisioning is essential. The transition involves moving from a simplified setup to a more secure configuration that includes mutual authentication and authorization policies.\n",
    "\n",
    "To get started, let's look at the NVFlare CLI usage for PoC mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff81d6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: nvflare poc [-h] [--prepare] [--start] [--stop] [--clean]\n",
      "                   {prepare,prepare-jobs-dir,start,stop,clean} ...\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --prepare             deprecated, suggest use 'nvflare poc prepare'\n",
      "  --start               deprecated, suggest use 'nvflare poc start'\n",
      "  --stop                deprecated, suggest use 'nvflare poc stop'\n",
      "  --clean               deprecated, suggest use 'nvflare poc clean'\n",
      "\n",
      "poc:\n",
      "  {prepare,prepare-jobs-dir,start,stop,clean}\n",
      "                        poc subcommand\n",
      "    prepare             prepare poc environment by provisioning local project\n",
      "    prepare-jobs-dir    prepare jobs directory\n",
      "    start               start services in poc mode\n",
      "    stop                stop services in poc mode\n",
      "    clean               clean up poc workspace\n"
     ]
    }
   ],
   "source": [
    "!nvflare poc -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe933b1-e27e-475f-9f32-4ff73d118560",
   "metadata": {},
   "source": [
    "As we can see, the PoC mode is composed of multiple subcommands. Let's quickly walk through each of the subcommands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e748725c-dc03-4dd9-a225-5a5b9d147783",
   "metadata": {},
   "source": [
    "#### `nvflare poc prepare`\n",
    "\n",
    "This is the first command to run when launching a PoC mode project. This command generates a local project workspace with startup kits for all participants. Internally it runs the provision process either with a user-specified configuration file or with a default one.\n",
    "\n",
    "Here is the help info for `nvflare poc prepare`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94bf0f3a-e8b0-49f2-a526-80244674c482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: nvflare poc prepare [-h] [-n [NUMBER_OF_CLIENTS]] [-c [CLIENTS ...]]\n",
      "                           [-he] [-i [PROJECT_INPUT]] [-d [DOCKER_IMAGE]]\n",
      "                           [-debug]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -n [NUMBER_OF_CLIENTS], --number_of_clients [NUMBER_OF_CLIENTS]\n",
      "                        number of sites or clients, default to 2\n",
      "  -c [CLIENTS ...], --clients [CLIENTS ...]\n",
      "                        Space separated client names. If specified,\n",
      "                        number_of_clients argument will be ignored.\n",
      "  -he, --he             enable homomorphic encryption.\n",
      "  -i [PROJECT_INPUT], --project_input [PROJECT_INPUT]\n",
      "                        project.yaml file path, If specified,\n",
      "                        'number_of_clients','clients' and 'docker' specific\n",
      "                        options will be ignored.\n",
      "  -d [DOCKER_IMAGE], --docker_image [DOCKER_IMAGE]\n",
      "                        generate docker.sh based on the docker_image, used in\n",
      "                        '--prepare' command. and generate docker.sh\n",
      "                        'start/stop' commands will start with docker.sh\n",
      "  -debug, --debug       debug is on\n"
     ]
    }
   ],
   "source": [
    "!nvflare poc prepare -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a4314-ed95-4518-80f5-54572f696683",
   "metadata": {},
   "source": [
    "By default, the command creates a workspace in `/tmp/nvflare/poc`. However, we can override the environment variable `NVFLARE_POC_WORKSPACE` to modify the workspace location. \n",
    "\n",
    "It is optional to specify a configuration file with the `-i` argument, or specifiy the number of clients: if unspecified, a default template will be used to provision startup kits for 1 server, 2 clients and 1 admin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a1ee9c-5392-462c-aa82-6a8fdba8fe8e",
   "metadata": {},
   "source": [
    "#### `nvflare poc prepare-jobs-dir`\n",
    "\n",
    "This is a convenient command can be used to link any directory with federated applications to the `admin`'s `transfer` folder. As we've explained earlier, `transfer` folder of the `admin`'s startup kit can be used to hold federated applications, so that `admin` can submit an application by simplying referring to its folder name, instead of the full path. This command essentially creates a symbolic link for `admin`'s `transfer` folder to point to a directory of available federated applications.\n",
    "\n",
    "Here is the help info for `nvflare poc prepare-jobs-dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b46ffc91-f93e-4c3f-b30e-126899a09aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: nvflare poc prepare-jobs-dir [-h] [-j [JOBS_DIR]] [-debug]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -j [JOBS_DIR], --jobs_dir [JOBS_DIR]\n",
      "                        jobs directory\n",
      "  -debug, --debug       debug is on\n"
     ]
    }
   ],
   "source": [
    "!nvflare poc prepare-jobs-dir -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fcf968-7f4f-4c34-84fe-f0a87cc22307",
   "metadata": {},
   "source": [
    "#### `nvflare poc start` and `nvflare poc stop`\n",
    "\n",
    "As the names indicate, these command starts / stops the FL system. You can either start / stop all participants, or a specific one, for instance, starting / stopping only the `server`.\n",
    "\n",
    "Here is the help info for `nvflare poc start` and `nvflare poc stop`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709d9f82-94fa-48e2-bdd3-b9d40668ba4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: nvflare poc start [-h] [-p [SERVICE]] [-ex [EXCLUDE]] [-gpu [GPU ...]]\n",
      "                         [-debug]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -p [SERVICE], --service [SERVICE]\n",
      "                        participant, Default to all participants\n",
      "  -ex [EXCLUDE], --exclude [EXCLUDE]\n",
      "                        exclude service directory during 'start', default to ,\n",
      "                        i.e. nothing to exclude\n",
      "  -gpu [GPU ...], --gpu [GPU ...]\n",
      "                        gpu device ids will be used as CUDA_VISIBLE_DEVICES.\n",
      "                        used for poc start command\n",
      "  -debug, --debug       debug is on\n"
     ]
    }
   ],
   "source": [
    "!nvflare poc start -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e68c5b6a-c2d5-400f-9633-e775f74b7a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: nvflare poc stop [-h] [-p [SERVICE]] [-ex [EXCLUDE]] [-debug]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -p [SERVICE], --service [SERVICE]\n",
      "                        participant, Default to all participants\n",
      "  -ex [EXCLUDE], --exclude [EXCLUDE]\n",
      "                        exclude service directory during 'stop', default to ,\n",
      "                        i.e. nothing to exclude\n",
      "  -debug, --debug       debug is on\n"
     ]
    }
   ],
   "source": [
    "!nvflare poc stop -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec050b-083e-463e-8517-03570945628f",
   "metadata": {},
   "source": [
    "By default, the PoC mode starts and stops all participants at once. You can use the `-p` or `-ex` flags to include or exclude certain participants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2c3a5-d949-46d0-bbf0-cb6a2c1e2530",
   "metadata": {},
   "source": [
    "#### `nvflare poc clean`\n",
    "\n",
    "This command cleans up and deletes the POC workspaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ee5a1-6040-434b-9651-32f0bbf58b4c",
   "metadata": {},
   "source": [
    "# Example: Starting a PoC System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7e2fe-b723-400e-9965-794d20166643",
   "metadata": {},
   "source": [
    "Let's start a PoC system together, step by step.\n",
    "\n",
    "> **NOTE**:\n",
    "> When running the PoC mode, it's necessary to use a separate terminal since the `nvflare poc start` subcommand will run in the foreground emitting output from the server and any connected clients.\n",
    "> \n",
    "> So, open a new launcher, launch a new terminal, go to this notebooks directory with `cd /flare/notebooks`, and executes all the commands in this section in that terminal.\n",
    "\n",
    "### 1. Create a Workspace\n",
    "\n",
    "The first step is to create a workspace for the PoC. We can use the `nvflare poc prepare` for this. \n",
    "\n",
    "By default, the command creates a workspace in `/tmp/nvflare/poc`. However, we can override the environment variable `NVFLARE_POC_WORKSPACE` to modify the workspace location. \n",
    "\n",
    "As mentioned earlier, the `prepare` subcommand calls internally the provisioning command. Without specify a configuration file or the number of clients, a default template will be used to provision startup kits for 1 server, 2 clients and 1 admin. But since we already have an example configuration file in [`../files/project.yml`/](../files/project.yml/), let go ahead and use it.\n",
    "\n",
    "Execute the following shell commands in the opened terminal:\n",
    "```shell\n",
    "rm -rf $(pwd -P)/\"temp-workspace/\"\n",
    "export NVFLARE_POC_WORKSPACE=$(pwd -P)/\"temp-workspace/\" && nvflare poc prepare -i ../files/project.yml\n",
    "```\n",
    "\n",
    "Running this command will create a new `temp-workspace` folder and generate an `example_project` within which we can find the startup kits for 1 server, 2 clients and 1 admin user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e7c0573-1a5b-4124-9421-ee52280e026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./temp-workspace\u001b[0m\n",
      "├── \u001b[01;34mdata\u001b[0m\n",
      "├── \u001b[01;34mexample_project\u001b[0m\n",
      "│   ├── \u001b[01;34mprod_00\u001b[0m\n",
      "│   │   ├── \u001b[01;34madmin@nvidia.com\u001b[0m\n",
      "│   │   ├── \u001b[01;34mserver1\u001b[0m\n",
      "│   │   ├── \u001b[01;34msite-1\u001b[0m\n",
      "│   │   └── \u001b[01;34msite-2\u001b[0m\n",
      "│   ├── \u001b[01;34mresources\u001b[0m\n",
      "│   │   ├── \u001b[00maws_template.yml\u001b[0m\n",
      "│   │   ├── \u001b[00mazure_template.yml\u001b[0m\n",
      "│   │   └── \u001b[00mmaster_template.yml\u001b[0m\n",
      "│   └── \u001b[01;34mstate\u001b[0m\n",
      "│       └── \u001b[00mcert.json\u001b[0m\n",
      "└── \u001b[00mproject.yml\u001b[0m\n",
      "\n",
      "10 directories, 5 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./temp-workspace -L 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf0e64-25ed-454f-876a-e6ac616ad862",
   "metadata": {},
   "source": [
    "### 2. Start the PoC Mode\n",
    "\n",
    "Next, let's use `nvflare poc start` to start the PoC mode.\n",
    "\n",
    "Noted that `nvflare poc start` starts all participants by default, including the admin.  It's often nice to start th admin separately from the server and clients, so that we can interact with the FL system as admin later. This is exactly what we will do now. To do this, we'll pass the `-ex admin@nvidia.com` argument to exclude the admin from the PoC start-up. We will connect as admin together later.\n",
    "\n",
    "Execute the following command in the same opened terminal:\n",
    "\n",
    "```shell\n",
    "nvflare poc start -ex admin@nvidia.com\n",
    "```\n",
    "\n",
    "After waiting for a bit, you will eventually see some console output. When you see messages similar to the following, showing that the server and clients are all up-and-running, you have successfully started a PoC system!\n",
    "\n",
    "```\n",
    "...\n",
    "2024-11-14 15:40:53,558 - root - INFO - Server started\n",
    "...\n",
    "2024-11-14 15:40:59,995 - FederatedClient - INFO - Successfully registered client:site-1 for project example_project. Token:98da9b03-1fa8-490d-9790-a66c4148eb74 SSID:ebc6125d-0a56-4688-9b08-355fe9e4d61a\n",
    "...\n",
    "2024-11-14 15:40:58,994 - FederatedClient - INFO - Successfully registered client:site-2 for project example_project. Token:9de44414-91ee-41a0-af2f-292a2e5477be SSID:ebc6125d-0a56-4688-9b08-355fe9e4d61a\n",
    "...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a83434f-46d3-4ccd-ac5e-6191b1b9a48a",
   "metadata": {},
   "source": [
    "> **NOTE**:\n",
    "> Please keep this terminal open for now, and do not shutdown the PoC system. We will interact with this PoC system in the next example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ae33e-08f6-4958-ba51-6d37220a3951",
   "metadata": {},
   "source": [
    "# Job Submission and Project Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0947318-fc9a-4f90-a6a0-c391b9d44795",
   "metadata": {},
   "source": [
    "Now we've successfully started an FL system with server and clients in PoC mode, let's learn how to actually submit federated jobs / applications to the server, run the jobs and monitor their running status in NVIDIA FLARE.\n",
    "\n",
    "NVIDIA FLARE offers 3 ways to submit and run federated jobs:\n",
    "1. Using [`FLARE Console`](https://nvflare.readthedocs.io/en/main/real_world_fl/operation.html#operating-nvflare)\n",
    "2. Using [`FLARE API`](https://nvflare.readthedocs.io/en/main/real_world_fl/flare_api.html)\n",
    "3. Using [`FLARE Job CLI`](https://nvflare.readthedocs.io/en/main/user_guide/nvflare_cli/job_cli.html)\n",
    "\n",
    "In this chapter, we focus on the first approach using `FLARE Console`. We will leave `FLARE API` as one of the exercises below for developers who want to dig deeper into the programmatic way of managing FL projects. We will not cover `FLARE Job CLI` in this course, but feel free to check out the [documentation](https://nvflare.readthedocs.io/en/main/user_guide/nvflare_cli/job_cli.html) to learn more about it.\n",
    "\n",
    "The `admin` user that we just provisioned before has the right to submit jobs and manage the FL system. Therefore we need to log into the FL system as the `admin` user. This can be achieved using the `nvflare poc start` command or the start-up script of the `admin` user, as we will see in the example later. After logging into the FL system as `admin`, we will be able to manage the FL system, submit jobs and monitor the project runtime status using `FLARE Console`.\n",
    "\n",
    "[FLARE Console](https://nvflare.readthedocs.io/en/main/real_world_fl/operation.html#operating-nvflare) is a console-like management interface for managing federated projects. FLARE Console offers various commands allowing users to manage and control the state of the FL system, including starting and stopping servers and clients, deploying applications, and monitoring deployment status. With FLARE Console, it is easy to:\n",
    "- Orchestrate the entire federated learning study, allowing users to initiate and manage various components of the system.\n",
    "- Deploy applications to clients and servers from the console, facilitating the execution of collaborative tasks.\n",
    "- Monitor project status: the console provides real-time updates on the status of servers and clients, enabling administrators to track progress and troubleshoot issues.\n",
    "\n",
    "FLARE Console typically runs as a standalone process on a researcher’s workstation or laptop. It interacts only with the server, not directly with FL clients. Administrators can issue commands anywhere using the console through secure network connection to manage experiments effectively.\n",
    "\n",
    "FLARE Console offers a comprehensive list of commands to manage and monitor jobs and the overall FL system. [Here](https://nvflare.readthedocs.io/en/main/real_world_fl/operation.html#admin-command-prompt) is the list of commands available in FLARE Console. Commonly used ones include:\n",
    "- `?`: use the question mark to list all available commands in FLARE Console\n",
    "- `check_status`: check the status of the server or the clients\n",
    "- `info`: list directories for job / application upload & download\n",
    "- `submit_job`: submit a federated job / application that is visible inside the `admin`'s transfer folder\n",
    "- `list_jobs`: list jobs on the server \n",
    "- `download_job`: download results of a job using the job's ID\n",
    "- `shutdown`: shutdown server or specific clients\n",
    "- `bye`: log out of the FLARE Console\n",
    "\n",
    "Besides monitoring system and job runtime, another useful feature in FLARE is integration with popular [Experiment Tracking](https://nvflare.readthedocs.io/en/main/programming_guide/experiment_tracking.html) APIs and tools, for instance, Tensorboard, Weights & Biases and MLFlow. FLARE provides interfaces compatible with these experiment tracking tools, allowing developers to tracking experiment statistics in Client API during client training, and stream these statistics either locally to client workspace or to the server. We will see experiment tracking in the example later. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215193dd-eb57-4525-9e94-fe937e22c291",
   "metadata": {},
   "source": [
    "Now let's put what we've learned into action: in the next section, we are going to submit a real federated application to the server, and monitor it's running status using FLARE Console!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5de8c2-8f2d-4c40-a0a2-7d9472bb49e4",
   "metadata": {},
   "source": [
    "# Example: Federated CIFAR10 Image Classification Using Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7e3c3-2468-44be-9fef-e88bd6050de5",
   "metadata": {},
   "source": [
    "The federated application we are going submit and run is the [**Hello Pytorch**](https://github.com/NVIDIA/NVFlare/tree/main/examples/hello-world/hello-pt) example from NVIDIA FLARE's official example repository.\n",
    "\n",
    "In this application, our goal is to train a simple neural network using Federated Averaging workflow, on the [`CIFAR10` dataset](https://www.cs.toronto.edu/~kriz/cifar.html) to perform image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973befbe-bc68-47ae-9e55-7bfef82d52a9",
   "metadata": {},
   "source": [
    "### Preparation\n",
    "\n",
    "Let's copy the example to our local `exmaples` folder with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f56e6281-4db6-4c5e-a523-d9b38a331709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mexamples/hello-pt\u001b[0m\n",
      "├── \u001b[00mREADME.md\u001b[0m\n",
      "├── \u001b[00mfedavg_script_runner_pt.py\u001b[0m\n",
      "├── \u001b[00mrequirements.txt\u001b[0m\n",
      "└── \u001b[01;34msrc\u001b[0m\n",
      "    ├── \u001b[01;34m__pycache__\u001b[0m\n",
      "    │   └── \u001b[00msimple_network.cpython-310.pyc\u001b[0m\n",
      "    ├── \u001b[00mhello-pt_cifar10_fl.py\u001b[0m\n",
      "    └── \u001b[00msimple_network.py\u001b[0m\n",
      "\n",
      "3 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "!if [ ! -d examples ]; then mkdir examples; fi\n",
    "!if [ ! -d examples/hello-pt ]; then \\\n",
    "    cp -r /NVFlare/examples/hello-world/hello-pt/ examples/hello-pt; fi\n",
    "!tree examples/hello-pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d87be00-d730-4a96-bc0e-34a207c93355",
   "metadata": {},
   "source": [
    "Let's look at this example a little bit, it should remind you want you've learned from the [previous chapter](Chapter_1_Intro_APIs_and_Simulator.ipynb) on FLARE's APIs for developing federated applications. In this Hello-Pytorch example:\n",
    "- The client code is implemented in file [examples/hello-pt/src/hello-pt_cifar10_fl.py](examples/hello-pt/src/hello-pt_cifar10_fl.py). Similar to the Hello FedAvg NumPy example from previous chapter, FLARE client APIs were used to convert a straight-forward centralized Pytorch training script to federated client code, by adding just a couple of lines of API usages.\n",
    "- The server code and the Job creation is in file [examples/hello-pt/fedavg_script_runner_pt.py](examples/hello-pt/fedavg_script_runner_pt.py). As a matter of fact, this example wrapped the FedAvg Controller and FedJob API calls inside a custom FedAvgJob class to make the code cleaner. But the implementation underneath is the same as the previous Hello FedAvg NumPy example.\n",
    "\n",
    "Before moving to the next step, we need to export this example application locally as a Federated Job, so that the `admin` can submit it to the server. We've learned how to do this from the previous chapter. Let's open file [examples/hello-pt/fedavg_script_runner_pt.py](examples/hello-pt/fedavg_script_runner_pt.py), comment the last line:\n",
    "```python\n",
    "job.simulator_run(\"/tmp/nvflare/jobs/workdir\", gpu=\"0\")\n",
    "```\n",
    "And uncomment the line before that:\n",
    "```python\n",
    "job.export_job(\"/tmp/nvflare/jobs/job_config\")\n",
    "```\n",
    "\n",
    "Then, let's run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "251eefbb-50db-469b-9c0b-d9097f8144b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd examples/hello-pt/ && python3 fedavg_script_runner_pt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5ff00-775b-4601-aa22-499b44d36162",
   "metadata": {},
   "source": [
    "After running the command, the federated application should be exported to `/tmp/nvflare/jobs/job_config`, under the name `hello-pt_cifar10_fedavg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a66b1d10-f97b-4de0-afdd-76e032b9673d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/tmp/nvflare/jobs/job_config\u001b[0m\n",
      "└── \u001b[01;34mhello-pt_cifar10_fedavg\u001b[0m\n",
      "    ├── \u001b[01;34mapp_server\u001b[0m\n",
      "    ├── \u001b[01;34mapp_site-1\u001b[0m\n",
      "    ├── \u001b[01;34mapp_site-2\u001b[0m\n",
      "    └── \u001b[00mmeta.json\u001b[0m\n",
      "\n",
      "5 directories, 1 file\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/nvflare/jobs/job_config -L 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658adc7b-5f43-4e0a-b74d-713135666458",
   "metadata": {},
   "source": [
    "Before starting the `admin` user, and submit the application for running, we can optionally link the location of the `hello-pt_cifar10_fedavg` application to the `admin`'s `transfer` folder. Doing so allows the `admin` to use the application's folder name for job submission. This can be achieved using the `nvflare poc prepare-jobs-dir` command. \n",
    "\n",
    "Let's go ahead and execute the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5c75bf6-992c-41b1-9085-d95d3612b9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link job directory from /tmp/nvflare/jobs/job_config to /flare/notebooks/temp-workspace/example_project/prod_00/admin@nvidia.com/transfer\n"
     ]
    }
   ],
   "source": [
    "!nvflare poc prepare-jobs-dir -j /tmp/nvflare/jobs/job_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3af2eb-a6dc-444c-9c2a-0da09306ea54",
   "metadata": {},
   "source": [
    "\n",
    "> **NOTE**:\n",
    "> To successfully run this command, you need to make sure that the previously started PoC system is still up-and-running. To be sure of this, you can also run this command inside the terminal previously opened for starting the PoC system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea95a9-92f5-4a35-bdbc-830ef9eab0c9",
   "metadata": {},
   "source": [
    "After executing this command, we can see that the `transfer` folder now points to the directory where the job `hello-pt_cifar10_fedavg` was exported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "be8e3735-d34c-4b14-a32b-964bbdb439c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lrwxrwxrwx 1 root root 28 Nov 22 15:50 ./temp-workspace/example_project/prod_00/admin@nvidia.com/transfer -> /tmp/nvflare/jobs/job_config\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./temp-workspace/example_project/prod_00/admin@nvidia.com/transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc5057-347f-4709-989d-e1b4e07f7c8e",
   "metadata": {},
   "source": [
    "### Connect to PoC System as `admin` \n",
    "\n",
    "With all preparation done, we are now ready to connect to our previously started PoC FL system as `admin`.\n",
    "\n",
    "> **NOTE**:\n",
    "> Please open a new terminal to execute upcoming shell commands, an interative shell session is needed.\n",
    "\n",
    "To connect to the PoC FL system, you can either use the `nvflare poc start` subcommand, and specify only starting the `admin` user, by executing the following shell command in a new terminal:\n",
    "```shell\n",
    "nvflare poc start -p admin@nvidia.com\n",
    "```\n",
    "Or you can directly invoke the shell script `fl_admin.sh` located in the `admin` user's startup kit, by executing the following command in a new terminal:\n",
    "```shell\n",
    "./temp-workspace/example_project/prod_00/admin@nvidia.com/startup/fl_admin.sh\n",
    "```\n",
    "\n",
    "After connecting as the `admin` user, you will enter the [FLARE Console](https://nvflare.readthedocs.io/en/main/real_world_fl/operation.html#admin-command-prompt).\n",
    "\n",
    "<img src=\"../images/flare-console.png\" alt=\"FLARE Console\" width=50% />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e31524-9bb0-4b91-8048-37ae14985cbe",
   "metadata": {},
   "source": [
    "### Submit Job & Monitor Progress\n",
    "\n",
    "From now on, we will stay inside the FLARE Console for all upcoming operations. \n",
    "\n",
    "All the following operations that we will perform can also be done using FLARE API. We will not walkthrough FLARE API now, but please feel free to refer to the [documentation](https://nvflare.readthedocs.io/en/main/real_world_fl/flare_api.html) to learn more. There is also an exercise on FLARE API at the end of this notebook.\n",
    "\n",
    "To submit a job side the FLARE Console, we can use the `submit_job` command followed by either the absolute path to the federated application, or name of the application if it's been linked to admin's transfer folder. Since we've already linked the application before using `nvflare poc prepare-jobs-dir`, let's go ahead and execute the following command in the FLARE Console:\n",
    "\n",
    "```shell\n",
    "submit_job hello-pt_cifar10_fedavg\n",
    "```\n",
    "\n",
    "After a successful job submission, you should see console messages similar to the following:\n",
    "```\n",
    "Submitted job: bc345b01-b910-4e99-83b6-15f50d44309b\n",
    "Done [65436 usecs] 2024-11-14 17:54:49.201422\n",
    "```\n",
    "\n",
    "As you can see, the job was submitted to the server for running, and a job ID of `bc345b01-b910-4e99-83b6-15f50d44309b` is assigned to keep track of the submitted job. This dynamically generated ID will change each time when you submit a job, even if it's the same job. So you will get a different one than what's shown here.\n",
    "\n",
    "> **NOTE**:\n",
    "> Remember to change the job ID in all the upcoming commands to the ID that you obtained from submitting the job.\n",
    "\n",
    "You will also see console output messages from the terminal which you used to start the PoC system with `nvflare poc start`, indicating that the job has started running.\n",
    "\n",
    "We can query the FL system to inspect the status of running jobs. Execute the following command in the FLARE Console:\n",
    "\n",
    "```shell\n",
    "list_jobs\n",
    "```\n",
    "\n",
    "You will get an output similar to the following:\n",
    "\n",
    "```\n",
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "| JOB ID                               | NAME                    | STATUS  | SUBMIT TIME                      | RUN DURATION   |\n",
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "| bc345b01-b910-4e99-83b6-15f50d44309b | hello-pt_cifar10_fedavg | RUNNING | 2024-11-14T17:54:49.191296+00:00 | 0:00:03.265572 |\n",
    "--------------------------------------------------------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "Notice that the ID display with `list_jobs` corresponds to the assigned job ID after `submit_job`. We can also see that the `STATUS` of the job is `RUNNING`. The `STATUS` will become `FINISHED:COMPLETED` after the job is successfully executed. Feel free to periodically use the `list_jobs` command to check the running status of the job.\n",
    "\n",
    "Another command to make sure that the clients are executing the job is to use `check_status client`. This might return something like this:\n",
    "\n",
    "```\n",
    "------------------------------------------------------------------------\n",
    "| CLIENT | APP_NAME   | JOB_ID                               | STATUS  |\n",
    "------------------------------------------------------------------------\n",
    "| site-2 | app_site-2 | bc345b01-b910-4e99-83b6-15f50d44309b | started |\n",
    "| site-1 | app_site-1 | bc345b01-b910-4e99-83b6-15f50d44309b | started |\n",
    "------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "Indicating that both clients are executing the submitted job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d249abf3-2ec7-4b82-be9f-d9ab0759c017",
   "metadata": {},
   "source": [
    "When a job is running, it will have a corresponding workspace under each participant's folder, for instance, for client `site-1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52c92da6-0475-4085-a64d-150b74411807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 308K\n",
      "drwxr-xr-x 7 root root 4.0K Nov 15 20:09 .\n",
      "drwxr-xr-x 6 root root 4.0K Nov 14 16:51 ..\n",
      "drwxr-xr-x 2 root root 4.0K Nov 14 19:57 .ipynb_checkpoints\n",
      "-rw-r--r-- 1 root root 9.0K Nov 15 19:55 audit.log\n",
      "drwxr-xr-x 3 root root 4.0K Nov 15 18:05 bc345b01-b910-4e99-83b6-15f50d44309b\n",
      "-rw-r--r-- 1 root root 246K Nov 15 19:55 cifar_net.pth\n",
      "-rw-r--r-- 1 root root    4 Nov 14 16:52 daemon_pid.fl\n",
      "drwxr-xr-x 2 root root 4.0K Nov 14 16:51 local\n",
      "-rw-r--r-- 1 root root 5.3K Nov 15 19:55 log.txt\n",
      "-rw-r--r-- 1 root root    4 Nov 14 16:52 pid.fl\n",
      "-rw-r--r-- 1 root root  746 Nov 14 16:51 readme.txt\n",
      "drwxr-xr-x 2 root root 4.0K Nov 14 16:51 startup\n",
      "drwxr-xr-x 2 root root 4.0K Nov 14 16:51 transfer\n"
     ]
    }
   ],
   "source": [
    "!ls -lah ./temp-workspace/example_project/prod_00/site-1/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04be23d8-5b91-4387-b4f5-8d884b4f000c",
   "metadata": {},
   "source": [
    "You will see a folder having it's name same as the job ID (`bc345b01-b910-4e99-83b6-15f50d44309b` in this case), that represents the job's workspace on client `site-1`. You will find the same workspace on `server` and `site-2` as well when the job is still running.\n",
    "\n",
    "After the job is finishes, the server side job workspace will be moved to a secure filesystem location by default. There are ways to configure how and where server-side job workspace are stored after a job finishes, but we will not go into details of this during this course. To be able to retrieve the server's workspace, which contains the final global model and important logs, we need to issue the `download_job` command:\n",
    "\n",
    "```shell\n",
    "download_job bc345b01-b910-4e99-83b6-15f50d44309b\n",
    "```\n",
    "\n",
    "Go ahead and execute this command in the FLARE Console and remember to replace the job ID with yours.\n",
    "\n",
    "After the job downloads successfully, you should see message similar to the following:\n",
    "```\n",
    "> download_job bc345b01-b910-4e99-83b6-15f50d44309b\n",
    "downloading job.zip ...\n",
    "downloaded job.zip (45532 bytes) in 0.015504598617553711 seconds\n",
    "downloading meta.json ...\n",
    "downloaded meta.json (792 bytes) in 0.01458883285522461 seconds\n",
    "downloading workspace.zip ...\n",
    "downloaded workspace.zip (315405 bytes) in 0.01614093780517578 seconds\n",
    "content downloaded to /flare/notebooks/temp-workspace/example_project/prod_00/admin@nvidia.com/startup/../transfer/bc345b01-b910-4e99-83b6-15f50d44309b\n",
    "Done [67753 usecs] 2024-11-15 20:18:18.540653\n",
    "```\n",
    "\n",
    "This command will download the server-side job workspace (whole content of folder `bc345b01-b910-4e99-83b6-15f50d44309b`) to the `transfer` folder of the `admin` user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ec1e7944-8bb0-4d9f-8257-5574983a29c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m./temp-workspace/example_project/prod_00/admin@nvidia.com/transfer/bc345b01-b910-4e99-83b6-15f50d44309b\u001b[0m\n",
      "├── \u001b[01;34mjob\u001b[0m\n",
      "│   └── \u001b[01;34mhello-pt_cifar10_fedavg_extproc\u001b[0m\n",
      "│       ├── \u001b[01;34mapp_server\u001b[0m\n",
      "│       ├── \u001b[01;34mapp_site-1\u001b[0m\n",
      "│       ├── \u001b[01;34mapp_site-2\u001b[0m\n",
      "│       └── \u001b[00mmeta.json\u001b[0m\n",
      "├── \u001b[00mmeta.json\u001b[0m\n",
      "└── \u001b[01;34mworkspace\u001b[0m\n",
      "    ├── \u001b[01;34mapp_server\u001b[0m\n",
      "    │   ├── \u001b[00mFL_global_model.pt\u001b[0m\n",
      "    │   ├── \u001b[01;34mconfig\u001b[0m\n",
      "    │   └── \u001b[01;34mcustom\u001b[0m\n",
      "    ├── \u001b[01;34mcross_site_val\u001b[0m\n",
      "    │   └── \u001b[00mcross_val_results.json\u001b[0m\n",
      "    ├── \u001b[00mfl_app.txt\u001b[0m\n",
      "    ├── \u001b[00mlog.txt\u001b[0m\n",
      "    ├── \u001b[00mmeta.json\u001b[0m\n",
      "    ├── \u001b[00mstats_pool_summary.json\u001b[0m\n",
      "    └── \u001b[01;34mtb_events\u001b[0m\n",
      "        ├── \u001b[01;34msite-1\u001b[0m\n",
      "        └── \u001b[01;34msite-2\u001b[0m\n",
      "\n",
      "14 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree ./temp-workspace/example_project/prod_00/admin@nvidia.com/transfer/bc345b01-b910-4e99-83b6-15f50d44309b -L 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f625248-d1fa-4db9-a013-beeeae72de89",
   "metadata": {},
   "source": [
    "You can see that the server side workspace contains the application itself, the final global model `FL_global_model.pt`, as well as many other log files. Feel free to checkout also client side workspaces: they still remain at client folders after a job finishes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6353e8-f7cd-4d40-a2d6-4fab38014bd2",
   "metadata": {},
   "source": [
    "As we've mentioned earlier, FLARE offers the capability to track experiments with popular tools such as Tensorboard, Weights & Biases and MLFlow. This Hello Pytorch example application actually uses the [`SummaryWriter`](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/client/tracking.py#L23) API inside client code (line 79 of [examples/hello-pt/src/hello-pt_cifar10_fl.py](examples/hello-pt/src/hello-pt_cifar10_fl.py)) to allow metrics logging in the same way as Tensorboard. \n",
    "\n",
    "During client training, these metrics are streamed to the server's workspace as Tensorboard-readable event format. As we've mentioned earlier, if the job is still running, the server side workspace will be inside the server folder. Therefore we can monitor real-time metrics of the training process by launching the `tensorboard` command, and point it to the server folder. \n",
    "\n",
    "After the job finishes and after we've downloaded the server side job workspace to the `admin`'s transfer folder, we can inspect the training metrics using the following tensorboard commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faee0e4f-f13d-4556-829d-065924b12b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a57bf0b-a55d-4512-9a4f-7386016e9d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-ed6377063d19f873\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-ed6377063d19f873\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./temp-workspace/example_project/prod_00/admin@nvidia.com/transfer/bc345b01-b910-4e99-83b6-15f50d44309b/workspace/tb_events/ --bind_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9d846-7739-48bc-b270-a9a2fd5a8070",
   "metadata": {},
   "source": [
    "This example application did not really calculate meaningfull validation metrics during the training process. But you can imagine that within your own real-world application, you could easily log any client side metrics such as accuracy, precision, recall etc., and monitor them in real-time as the training progresses!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ae8e3b-6bae-417a-819e-32e957d1fe8d",
   "metadata": {},
   "source": [
    "**That's it, we have successfully provisioned a PoC system and deployed a real federated application!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a9f698-074f-49e9-a081-d99060bac9c1",
   "metadata": {},
   "source": [
    "Last but not least, do not forget the shut the PoC system down using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b49ec50-e788-4d4b-8ac5-1e8c53f7538d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start shutdown NVFLARE\n",
      "connect to nvflare server\n",
      "checking running jobs\n",
      "shutdown NVFLARE\n",
      "waiting system to shutdown\n"
     ]
    }
   ],
   "source": [
    "!nvflare poc stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39dc4b5-1a0b-406e-8638-7071afcc825a",
   "metadata": {},
   "source": [
    "When you are ready, use the following command to clean up and delete the workspace generated by the PoC system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280fcf02-75b9-4384-8ac3-1b92e5bcc442",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvflare poc clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4855cfad-a694-46cb-af43-f24775f84fcd",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "Let us recap what we've learned in this chapter:\n",
    "- We first learned about FLARE's provisioning process, a crucial part for real-world deployment of federated projects.\n",
    "- We then had an introduction to FLARE's PoC mode, a runtime environment allowing developers to provision an FL system in a sandbox environment on a single PC. We also started a PoC system together. \n",
    "- Next, we walked through the FLARE Console and saw how it can be used to submit federated applications to the FL system for running, manage and monitor job status. We also learned about FLARE's integration with experiment tracking tools such as Tensorboard, Weights & Biases, MLFlow.\n",
    "- Finally, we deployed a real federated application with FLARE Console for CIFAR10 image classification using Pytorch, performed project monitoring with FLARE Console and experiment tracking with Tensorboard.\n",
    "\n",
    "With the knowledge of chapter 2 and chapter 3, you are now able to develop a federated application using FLARE, provision a deployment system and deploy the federated application! There is still some knowledge to gain, if you want your federated application to be fully bullet-proof and scalable in a real-world environment. In the next chapter, we will introduce some of FLARE's advanced features that will hopefully allow you to grasp the last bit of skills towards a full-fledged federated learning developer!\n",
    "\n",
    "Before continuing to the [next chapter](Chapter_4_Advanced_Topics_Use_Cases_and_Additional_Resources.ipynb), here are some exercises to help you grasp the essential of this chapter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08c2df1-a9fc-4b00-ae71-9fe07c3987c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "Modify the project configuration file [../files/project.yml](../files/project.yml) to add another client `site-3`. Provision a PoC system with the new modified configuration file. Then start up the PoC system with only the server (`server1`) and the new client (`site-3`). What would happen if you start `site-3` before the `server`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cdb7a3-a3bd-46f6-8d10-dc6f88e68e42",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Solution to Exercise 1\n",
    "\n",
    "Modify [../files/project.yml](../files/project.yml) by adding the following lines at anywhere is the `participants` section (for exmaple, at the end):\n",
    "```yaml\n",
    "  - name: site-3\n",
    "    type: client\n",
    "    org: nvidia\n",
    "```\n",
    "\n",
    "Provision the PoC system using `nvflare poc prepare`:\n",
    "```shell\n",
    "rm -rf $(pwd -P)/\"temp-workspace/\"\n",
    "export NVFLARE_POC_WORKSPACE=$(pwd -P)/\"temp-workspace/\" && nvflare poc prepare -i ../files/project.yml\n",
    "```\n",
    "\n",
    "Start the PoC system with `nvflare poc start`. Using flag `-p` to start first `server1`, then `site-3`:\n",
    "```shell\n",
    "nvflare poc start -p server1 \n",
    "nvflare poc start -p site-3\n",
    "```\n",
    "\n",
    "If you start the client before the server, the start-up will fail since there is no server endpoint for the client to connect to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce538d1-5548-43f7-8a79-f843851a97d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Exercise 2\n",
    "\n",
    "Restart the PoC system. This time, use FLARE API to establish an admin user connection. After that, submit the same Hello Pytorch application, query job status and download the job when it finishes, all using FLARE API. For references, you can find documentations for FLARE API [here](https://nvflare.readthedocs.io/en/main/real_world_fl/flare_api.html), [here](https://nvflare.readthedocs.io/en/main/real_world_fl/migrating_to_flare_api.html) and [here](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.fuel.flare_api.flare_api.html#module-nvflare.fuel.flare_api.flare_api)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f57a68-a8df-40ba-9414-2e0b185d56f4",
   "metadata": {},
   "source": [
    "## Solution to Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9555c16c-d0e1-4188-ba32-44730c3c1ef5",
   "metadata": {},
   "source": [
    "First, prepare and start up the PoC system (only server and clients) by running the following commands in a new terminal:\n",
    "\n",
    "> **Note**:\n",
    "> Don't forget to remove the section on `site-3` if you have previously modified the project.yml configuration file during Exercise 1.\n",
    "\n",
    "```shell\n",
    "rm -rf $(pwd -P)/\"temp-workspace/\"\n",
    "export NVFLARE_POC_WORKSPACE=$(pwd -P)/\"temp-workspace/\" && nvflare poc prepare -i files/project.yml\n",
    "\n",
    "nvflare poc start -ex admin@nvidia.com\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2e563c-6faf-4c52-ac58-e2e1d9f351b9",
   "metadata": {},
   "source": [
    "To connect to the PoC system as admin user, use the following FLARE APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c11e018a-8e8d-4ce7-90d3-d3831704c57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemInfo\n",
      "server_info:\n",
      "status: stopped, start_time: Sat Nov 23 00:37:44 2024\n",
      "client_info:\n",
      "site-2(last_connect_time: Sat Nov 23 00:38:11 2024)\n",
      "site-1(last_connect_time: Sat Nov 23 00:38:12 2024)\n",
      "job_info:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nvflare.fuel.flare_api.flare_api import new_secure_session\n",
    "\n",
    "project_name = \"example_project\"\n",
    "username = \"admin@nvidia.com\"\n",
    "admin_user_dir = os.path.join(os.getcwd(), \"temp-workspace\", project_name, \"prod_00\", username)\n",
    "\n",
    "sess = new_secure_session(\n",
    "    username=username,\n",
    "    startup_kit_location=admin_user_dir\n",
    ")\n",
    "print(sess.get_system_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0546b9-1d4d-400f-9db6-8e5f6f1c16eb",
   "metadata": {},
   "source": [
    "To submit the `hello-pt_cifar10_fedavg` application, use the following API. Noted here we submit the application using its full path, instead of linking the application's folder to `admin`'s `transfer` folder first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0c44d2b-85ed-4ed2-9b2f-b76841656298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07d7607c-4b1a-471d-a52f-e21b9c953cad was submitted\n"
     ]
    }
   ],
   "source": [
    "path_to_example_job = \"/tmp/nvflare/jobs/job_config/hello-pt_cifar10_fedavg\"\n",
    "job_id = sess.submit_job(path_to_example_job)\n",
    "print(job_id + \" was submitted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe15c489-d484-4996-9b2d-987c4b62d788",
   "metadata": {},
   "source": [
    "Note down the job ID, and use the following API to monitor the live job status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fb621a27",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'hello-pt_cifar10_fedavg', 'resource_spec': {}, 'min_clients': 1, 'deploy_map': {'app_server': ['server'], 'app_site-1': ['site-1'], 'app_site-2': ['site-2']}, 'byoc': True, 'submitter_name': 'admin@nvidia.com', 'submitter_org': 'nvidia', 'submitter_role': 'project_admin', 'job_folder_name': 'hello-pt_cifar10_fedavg', 'job_id': '07d7607c-4b1a-471d-a52f-e21b9c953cad', 'submit_time': 1731709802.0272186, 'submit_time_iso': '2024-11-15T22:30:02.027219+00:00', 'start_time': '2024-11-15 22:30:02.746792', 'duration': 'N/A', 'data_storage_format': 2, 'status': 'RUNNING', 'job_deploy_detail': ['server: OK', 'site-1: OK', 'site-2: OK'], 'schedule_count': 1, 'last_schedule_time': 1731709802.698105, 'schedule_history': ['2024-11-15 22:30:02: scheduled']}\n",
      "{'name': 'hello-pt_cifar10_fedavg', 'resource_spec': {}, 'min_clients': 1, 'deploy_map': {'app_server': ['server'], 'app_site-1': ['site-1'], 'app_site-2': ['site-2']}, 'byoc': True, 'submitter_name': 'admin@nvidia.com', 'submitter_org': 'nvidia', 'submitter_role': 'project_admin', 'job_folder_name': 'hello-pt_cifar10_fedavg', 'job_id': '07d7607c-4b1a-471d-a52f-e21b9c953cad', 'submit_time': 1731709802.0272186, 'submit_time_iso': '2024-11-15T22:30:02.027219+00:00', 'start_time': '2024-11-15 22:30:02.746792', 'duration': 'N/A', 'data_storage_format': 2, 'status': 'RUNNING', 'job_deploy_detail': ['server: OK', 'site-1: OK', 'site-2: OK'], 'schedule_count': 1, 'last_schedule_time': 1731709802.698105, 'schedule_history': ['2024-11-15 22:30:02: scheduled']}\n",
      "{'name': 'hello-pt_cifar10_fedavg', 'resource_spec': {}, 'min_clients': 1, 'deploy_map': {'app_server': ['server'], 'app_site-1': ['site-1'], 'app_site-2': ['site-2']}, 'byoc': True, 'submitter_name': 'admin@nvidia.com', 'submitter_org': 'nvidia', 'submitter_role': 'project_admin', 'job_folder_name': 'hello-pt_cifar10_fedavg', 'job_id': '07d7607c-4b1a-471d-a52f-e21b9c953cad', 'submit_time': 1731709802.0272186, 'submit_time_iso': '2024-11-15T22:30:02.027219+00:00', 'start_time': '2024-11-15 22:30:02.746792', 'duration': 'N/A', 'data_storage_format': 2, 'status': 'RUNNING', 'job_deploy_detail': ['server: OK', 'site-1: OK', 'site-2: OK'], 'schedule_count': 1, 'last_schedule_time': 1731709802.698105, 'schedule_history': ['2024-11-15 22:30:02: scheduled']}\n",
      "......................\n",
      "{'name': 'hello-pt_cifar10_fedavg', 'resource_spec': {}, 'min_clients': 1, 'deploy_map': {'app_server': ['server'], 'app_site-1': ['site-1'], 'app_site-2': ['site-2']}, 'byoc': True, 'submitter_name': 'admin@nvidia.com', 'submitter_org': 'nvidia', 'submitter_role': 'project_admin', 'job_folder_name': 'hello-pt_cifar10_fedavg', 'job_id': '07d7607c-4b1a-471d-a52f-e21b9c953cad', 'submit_time': 1731709802.0272186, 'submit_time_iso': '2024-11-15T22:30:02.027219+00:00', 'start_time': '2024-11-15 22:30:02.746792', 'duration': '0:02:45.010479', 'data_storage_format': 2, 'status': 'FINISHED:COMPLETED', 'job_deploy_detail': ['server: OK', 'site-1: OK', 'site-2: OK'], 'schedule_count': 1, 'last_schedule_time': 1731709802.698105, 'schedule_history': ['2024-11-15 22:30:02: scheduled']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MonitorReturnCode.JOB_FINISHED: 0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nvflare.fuel.flare_api.flare_api import basic_cb_with_print\n",
    "\n",
    "job_id = \"07d7607c-4b1a-471d-a52f-e21b9c953cad\"\n",
    "sess.monitor_job(job_id, cb=basic_cb_with_print, cb_run_counter={\"count\":0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ac11e3-bc1a-4468-8343-e394f929640a",
   "metadata": {},
   "source": [
    "The `cb` argument can be used to provide your own custom callback function for status monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64385603-c2a4-4f67-86d7-0de5de55f77a",
   "metadata": {},
   "source": [
    "You can also perform `list_jobs` with the following API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "70720789-b537-47d3-9dc9-71df6f8fff06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'hello-pt_cifar10_fedavg',\n",
       "  'resource_spec': {},\n",
       "  'min_clients': 1,\n",
       "  'deploy_map': {'app_server': ['server'],\n",
       "   'app_site-1': ['site-1'],\n",
       "   'app_site-2': ['site-2']},\n",
       "  'byoc': True,\n",
       "  'submitter_name': 'admin@nvidia.com',\n",
       "  'submitter_org': 'nvidia',\n",
       "  'submitter_role': 'project_admin',\n",
       "  'job_folder_name': 'hello-pt_cifar10_fedavg',\n",
       "  'job_id': '07d7607c-4b1a-471d-a52f-e21b9c953cad',\n",
       "  'submit_time': 1731709802.0272186,\n",
       "  'submit_time_iso': '2024-11-15T22:30:02.027219+00:00',\n",
       "  'start_time': '2024-11-15 22:30:02.746792',\n",
       "  'duration': '0:02:45.010479',\n",
       "  'data_storage_format': 2,\n",
       "  'status': 'FINISHED:COMPLETED',\n",
       "  'job_deploy_detail': ['server: OK', 'site-1: OK', 'site-2: OK'],\n",
       "  'schedule_count': 1,\n",
       "  'last_schedule_time': 1731709802.698105,\n",
       "  'schedule_history': ['2024-11-15 22:30:02: scheduled']}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.list_jobs(detailed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb6322",
   "metadata": {
    "tags": []
   },
   "source": [
    "When the job finishes, download the server-side job workspace to the `admin`'s `transfer` folder using the following API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15992522",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job download path: /flare/notebooks/temp-workspace/example_project/prod_00/admin@nvidia.com/transfer/07d7607c-4b1a-471d-a52f-e21b9c953cad\n"
     ]
    }
   ],
   "source": [
    "job_download = sess.download_job_result(job_id)\n",
    "print(\"Job download path: \" + job_download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ff99d66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/flare/notebooks/temp-workspace/example_project/prod_00/admin@nvidia.com/transfer/07d7607c-4b1a-471d-a52f-e21b9c953cad\u001b[0m\n",
      "├── \u001b[01;34mjob\u001b[0m\n",
      "│   └── \u001b[01;34mhello-pt_cifar10_fedavg\u001b[0m\n",
      "│       ├── \u001b[01;34mapp_server\u001b[0m\n",
      "│       ├── \u001b[01;34mapp_site-1\u001b[0m\n",
      "│       ├── \u001b[01;34mapp_site-2\u001b[0m\n",
      "│       └── \u001b[00mmeta.json\u001b[0m\n",
      "├── \u001b[00mmeta.json\u001b[0m\n",
      "└── \u001b[01;34mworkspace\u001b[0m\n",
      "    ├── \u001b[01;34mapp_server\u001b[0m\n",
      "    │   ├── \u001b[00mFL_global_model.pt\u001b[0m\n",
      "    │   ├── \u001b[01;34mconfig\u001b[0m\n",
      "    │   └── \u001b[01;34mcustom\u001b[0m\n",
      "    ├── \u001b[01;34mcross_site_val\u001b[0m\n",
      "    │   └── \u001b[00mcross_val_results.json\u001b[0m\n",
      "    ├── \u001b[00mfl_app.txt\u001b[0m\n",
      "    ├── \u001b[00mlog.txt\u001b[0m\n",
      "    ├── \u001b[00mmeta.json\u001b[0m\n",
      "    ├── \u001b[00mstats_pool_summary.json\u001b[0m\n",
      "    └── \u001b[01;34mtb_events\u001b[0m\n",
      "        ├── \u001b[01;34msite-1\u001b[0m\n",
      "        └── \u001b[01;34msite-2\u001b[0m\n",
      "\n",
      "14 directories, 8 files\n"
     ]
    }
   ],
   "source": [
    "!tree {job_download} -L 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bfc891-b213-4d7a-b850-8d2082d075c2",
   "metadata": {},
   "source": [
    "Finally, shutdown the PoC system and close the admin connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379702cf-2019-4a5b-a4bf-f7f9454cd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.shutdown(\"client\")\n",
    "sess.shutdown(\"server\")\n",
    "\n",
    "# Or\n",
    "# print(sess.api.do_command(\"shutdown client\"))\n",
    "# print(sess.api.do_command(\"shutdown server\"))\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
