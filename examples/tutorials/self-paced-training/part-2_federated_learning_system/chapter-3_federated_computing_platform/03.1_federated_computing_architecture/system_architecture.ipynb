{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26d1aff5",
   "metadata": {},
   "source": [
    "# NVIDIA FLARE Overview\n",
    "\n",
    "* Apache License 2.0 to catalyze FL research & development​\n",
    "  \n",
    "* Designed for enterprise production\n",
    "   \n",
    "* Able to run on CPU, GPU, and Multi-GPU\n",
    "\n",
    "* Enables cross-country, distributed, multi-party collaborative learning​\n",
    "\n",
    "* Production scalability with high availability and multi-task execution​\n",
    "\n",
    "* Framework, model, domain, and task agnostic​\n",
    "\n",
    "* Layered, pluggable, customizable federated compute architecture​\n",
    "\n",
    "\n",
    "# NVIDIA FLARE Architecture Overview\n",
    "\n",
    "* Layered, pluggable open architecture​\n",
    "\n",
    "* Each layer’s components are composable and pluggable​\n",
    "\n",
    "* Network: Communication & Messaging layer ​\n",
    "\n",
    "   Drivers: gRPC, HTTP + WebSocket, TCP, any plugin driver​\n",
    "\n",
    "* CellNet: logical end-to-end (cell to cell) network​\n",
    "\n",
    "* Message: reliable streaming message ​\n",
    "\n",
    "* Federated Computing Layer​\n",
    "\n",
    "* Resource-based job scheduling, job monitoring, concurrent job lifecycle management, high-availability management​\n",
    "\n",
    "* Plugin component management ​\n",
    "\n",
    "* Configuration management​\n",
    "\n",
    "* Local event and federated event handling​\n",
    "\n",
    "* Federated Workflow​\n",
    "\n",
    "   SAG, Cyclic, Cross-site Evaluation, Swarm Learning, Federated Analytics​\n",
    "\n",
    "* Federated Learning Algorithms​\n",
    "\n",
    "<img src=\"./flare_overview.png\" alt=\"FLARE Architecture\" width=\"700\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e6ad61",
   "metadata": {},
   "source": [
    "NVIDIA FLARE is built in layers. Each layer is built on top of the next. At the bottom layer is the network communication layer.\n",
    "\n",
    "## FCI - Flare Communication Interface\n",
    "\n",
    "FCI is a logical network framework that supports asynchronous, 2-way communication through multiple transports. It is:\n",
    "\n",
    "* **Pluggable**: It has a pluggable architecture to support different messaging patterns (request-response, broadcast, pub/sub). It can also support different transports through drivers, like TCP, Pipe, HTTP/WS, gRPC.\n",
    "\n",
    "* **Streamable**: Large binary data can be streamed in small chunks to minimize memory usage.\n",
    "\n",
    "* **Full-duplex**: Both sides can send messages to each other without polling, if the transport supports it.\n",
    "\n",
    "* **Multiplex**: Multiple conversations can be conducted over the same connection at the same time using stream IDs.\n",
    "\n",
    "* **Asynchronous**: Can send/receive messages in an asynchronous fashion like fire/forget, listen to messages.\n",
    "\n",
    "* **One-way connection** for remote communications: All TCP-based connections can be initiated from clients so clients have no port exposed.\n",
    "\n",
    "* **Supports IPC**: It can work with communications through pipes or sockets between processes.\n",
    "\n",
    "* **Native heartbeats**: Heartbeats are supported by FCI to keep connections alive.\n",
    "\n",
    "From top to bottom, FCI has the following layers:\n",
    "\n",
    "* **API Layer**: This is the API exposed to application developers, like Communicator and Cellnet.\n",
    "\n",
    "* **Streamable Framed Message (SFM)**: This is the core of FCI and it provides abstraction on top of different communication protocols. It manages endpoints and connections.\n",
    "\n",
    "* **Transport Drivers**: This layer is responsible for sending frames to other endpoints. It treats the frame as opaque bytes.\n",
    "One can use one of driver out of box such as gRPC, TCP, HTTP/Websocket. One can also develop custom driver for alternative protocols. Switch driver will not affect the application layers \n",
    "\n",
    "<img src=\"./fci.png\" alt=\"FLARE Communication Interface\" width=\"300\" height=\"400\">\n",
    "\n",
    "\n",
    "## Federated Job Processing Architecture\n",
    "\n",
    "There are two parent control processes with corresponding job processes on each site. This enables support of concurrent, multi-job processes.\n",
    "\n",
    "<img src=\"./system_architecture.png\" alt=\"FLARE System Architecture\" width=\"700\" height=\"400\">\n",
    "\n",
    "\n",
    "## Event-Based System\n",
    "\n",
    "ALL NVIDIA FLARE's components (FLComponent) has event handling and event firing via the runtime engine. As a result, users can write an FLComponent as a plugin and listen to events and write any customized logic at any layer.\n",
    "\n",
    "\n",
    "## Federated Learning Framework\n",
    "\n",
    "Based on the basic core concepts, we have built many Federated learning workflows including FedAvg, FedOpt, FedProx, Scaffold, cyclic, swarming learning, split learning algorithms with many examples which can be found on the [website](https://nvidia.github.io/NVFlare/) and its [tutorial categories](https://nvidia.github.io/NVFlare/catalog/).\n",
    "\n",
    "## Enterprise Security and Privacy\n",
    "\n",
    "We have many features to support enterprise security as well as support privacy-enhancing technologies (PETs). Please refer to [Part-3 Security and Privacy](../../../part-3_security_and_privacy/part-3_introduction.ipynb).\n",
    "\n",
    "## Simulations\n",
    "\n",
    "We have built different tools for simulation including Python API and CLI. You have seen the Job API and simulator CLI in [Chapter-1](../../../part-1_federated_learning_introduction/Chapter-1_running_federated_learning_applications/01.0_introduction/introduction.ipynb).\n",
    "\n",
    "In [Section 3.2](../03.2_deployment_simulation/simulate_real_world_deployment.ipynb), we will also discuss how to simulate the deployment within a local machine.\n",
    "\n",
    "## Setup and Deployment\n",
    "\n",
    "Setting up the federated computing system is not a trivial task. We have built tools to make this process simpler. We will discuss this in [Chapter 4](../../chapter-4_setup_federated_system/04.0_introduction/introduction.ipynb).\n",
    "\n",
    "## Different type of FLARE APIs\n",
    "\n",
    "At its Core, Flare uses controller and executor to assign tasks and execute tasks for each job. There we have the:\n",
    "\n",
    "### Python APIs\n",
    "\n",
    "* **Controller, Executor API** -- those are the lower-level API that gives the full control and power for any type of federated computing\n",
    "\n",
    "* **ModelController and Client API** -- This is higher level API based on the assumption that for many machine learning and deep learning algorithms, we can use the FLModel data structure to capture the input and output. \n",
    "\n",
    "        ```\n",
    "        class FLModel:\n",
    "            def __init__(\n",
    "                self,\n",
    "                params_type: Union[None, str, ParamsType] = None,\n",
    "                params: Any = None,\n",
    "                optimizer_params: Any = None,\n",
    "                metrics: Optional[Dict] = None,\n",
    "                start_round: Optional[int] = 0,\n",
    "                current_round: Optional[int] = None,\n",
    "                total_rounds: Optional[int] = None,\n",
    "                meta: Optional[Dict] = None,\n",
    "            ):\n",
    "            ...\n",
    "\n",
    "        ```\n",
    "This data structure essentially captures the model (parameter type (Full, Diff), model parameters (weights), optimizer parameters), metrics, metadata. This kind of data structure is understandable by most data scientists.\n",
    "\n",
    "On the Server side, we have ModelController -- Controller uses and consumes FLModel, on the client side we have Client API that receives and sends model updates via FLModel. You have already seen this in previous chapters.\n",
    "\n",
    "\n",
    "* **Job API** -- FLARE Job API is a way to generate job configuration. Although once can direct edit configuration files, one can also use the Job API to construct the needed components and generate the job configuration. The job API can also call job.simulate_run()  -- which is combined step of export job configuration and call simulator run. \n",
    "\n",
    "* **Simulator API** -- one can directly invoke simulator_run() method to start simulation in python\n",
    "\n",
    "\n",
    "* **FLARE API** -- FLARE python API is equivalent to FLARE Console command API. Instead of interacting with FL system via Console command, we can perform most of the command functions via FLARE API. These include connecting to the server, checking status, monitoring jobs, submitting jobs etc.\n",
    "\n",
    "\n",
    "### Command Line Interface\n",
    "\n",
    "FLARE has several CLIs under the  ```nvflare `` command\n",
    "\n",
    "nvflare --version   \n",
    "\n",
    "nvflare poc --- POC command\n",
    "\n",
    "nvflare preflight_check -- check FL system setup to see anything not working and why\n",
    "\n",
    "nvflare provision  -- provision tool\n",
    "\n",
    "nvflare simulator -- simulator CLI\n",
    "\n",
    "nvflare dashboard -- start NVFLARE dashboard, a Web UI to allow participant distribute provisioned startup kit\n",
    "\n",
    "nvflare authz_preview  --- look at different user roles \n",
    "\n",
    "nvflare job --  CLI job command to allow user to create job configuration based on the job templates, list existing templates, and submit job to production and POC.  \n",
    "\n",
    "nvflare config -- this command to allow user to setup default startup dir, poc workspace dir and job template dir locally. This is usefuly for local development with job templates and POC. \n",
    "\n",
    "\n",
    "## Configuration\n",
    "\n",
    "NVFLARE supports several configuration formats: JSON, pyhocon, and YAML. You can see the details in [Configuration Files](https://nvflare.readthedocs.io/en/main/user_guide/configurations.html).\n",
    "\n",
    "You can also leverage the existing [job templates](https://github.com/NVIDIA/NVFlare/tree/main/job_templates): a set of predefined configurations and using [job CLI](https://github.com/NVIDIA/NVFlare/blob/main/examples/tutorials/job_cli.ipynb) to customize to your needs. \n",
    "\n",
    "# Job Template\n",
    "\n",
    "Job templates are a set of existing job configurations with specified structure\n",
    "\n",
    "For example \n",
    "```\n",
    "\n",
    "├── config_fed_client.conf\n",
    "├── config_fed_server.conf\n",
    "├── info.conf\n",
    "├── info.md\n",
    "└── meta.conf\n",
    "\n",
    "```\n",
    "\n",
    "Each job template consists of an \"information card\", info.conf, display card \"info.md\" and job configuration files.\n",
    "\n",
    "The configuration is defined in pyhocon format so we can add comments and explain the details.\n",
    "\n",
    "we can take a look at one example \n",
    "\n",
    "```job_templates/sag_pt/config_fed_client.conf```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1e27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat  ../../../../../../job_templates/sag_pt/config_fed_client.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d0d1dc",
   "metadata": {},
   "source": [
    "With job templates, we can use CLI command to view and modify template during job configuration creatation \n",
    "\n",
    "You can find many details in [job cli tutorial](../../../../job_cli.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44466485",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
