{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy in Federated Learning\n",
    "\n",
    "In addition to operational and physical security, Federated Learning (FL) benefits from various techniques that enhance data privacy and model protection. This chapter explores the privacy-preserving methods available in NVFlare that help mitigate potential risks.\n",
    "\n",
    "One area of consideration in FL is model inversion attacks, where adversaries attempt to reconstruct aspects of training data from model parameters. While deep neural networks (DNNs) contain a vast number of parameters and are trained on large datasets, research has shown that some data patterns can be inferred under certain conditions. However, several well-established defense mechanisms exist to minimize this risk and ensure privacy during model sharing.\n",
    "\n",
    "A widely used approach is Differential Privacy (DP), which introduces carefully calibrated noise to model updates, limiting the ability to extract meaningful information from gradients while preserving model accuracy. DP has been shown to work effectively in FL with minimal impact on performance. Meanwhile, ongoing research continues to refine both attack and defense strategies, further strengthening privacy safeguards.\n",
    "\n",
    "For additional protection, Homomorphic Encryption (HE) can be used to encrypt model updates before sharing. This allows a server to aggregate encrypted weights without accessing their actual values, ensuring that individual client updates remain private. Only the clients, who hold the decryption keys, can retrieve the updated model. While HE introduces some computational overhead, it provides an extra layer of security, particularly in sensitive applications like healthcare, where institutions can collaborate without exposing private data.\n",
    "\n",
    "By integrating DP and HE, NVFlare enhances the security and privacy of federated learning, enabling safe and effective collaboration while maintaining strong data protection measures.\n",
    "\n",
    "1. **NVFlare's Privacy Filters**\n",
    "   * [Privacy Preservation using NVFlare's Filters](../05.1_privacy_filter/privacy_filtering.ipynb)\n",
    "\n",
    "2. **Differential Privacy**\n",
    "\n",
    "    * [Federated Learning with Differential Privacy](../05.2_differential_privacy/privacy_with_differential_privacy.ipynb)\n",
    "\n",
    "3. **Homomorphic Encryption**\n",
    "\n",
    "   * [Federated Learning with Homomorphic Encryption for Secure Aggregation](../05.3_homomorphic_encryption/05.3.1_privacy_with_homormorphic_encryption.ipynb)\n",
    "   * [Secure Federated Kaplan-Meier Analysis via Time-Binning and Homomorphic Encryption](../05.3_homomorphic_encryption/05.3.2_kaplan_meier_survaval_analysis_with_he.ipynb)\n",
    "\n",
    "7. [Recap of the covered topics](../05.4_recap/recap.ipynb)\n",
    "\n",
    "\n",
    "Let's get started learning about [NVFlare's Privacy Filters](../05.1_privacy_filter/privacy_filtering.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
