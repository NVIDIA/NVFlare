{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d4c67e1-2570-40b3-bb16-3bc1074d9bd9",
   "metadata": {},
   "source": [
    "# Summary of Chapter 5\n",
    "\n",
    "We covered several Privacy-Enhancing Technologies, starting with Differential Privacy to avoid the leaking of information about indiviudal training data points, and homomorphic encryption to further secure the training process against potential attacks from a malicous server. The key takeaways are:\n",
    "\n",
    "1. **Privacy Filters**\n",
    "    - **Goal:** Privacy filters in NVFlare help enforce data protection policies by modifying model updates before they are shared, reducing the risk of privacy leakage.\n",
    "    - **How They Work:** These filters can remove, mask, or alter sensitive information in model parameters, preventing unintended exposure of private data.\n",
    "    - **Impact on FL:**\n",
    "        * Allows customization based on organizational policies and regulatory requirements.\n",
    "        * Can be applied to gradients, weights, or metadata to protect specific information.\n",
    "        * Helps organizations balance privacy and model performance by fine-tuning filtering strategies.\n",
    "\n",
    "2. **Differential Privacy (DP)**\n",
    "    - **Goal:** Limits the amount of information an adversary can extract from model updates, reducing the risk of data reconstruction.\n",
    "    - **How It Works:** Adds controlled noise to model gradients or parameters before sharing, ensuring that individual data points cannot be distinguished.\n",
    "    - **Impact on FL:**\n",
    "        * Provides a mathematical guarantee of privacy.\n",
    "        * Minimizes leakage of sensitive information from model updates.\n",
    "        * Has minimal impact on model accuracy when carefully tuned.\n",
    "        * Often combined with other security measures for enhanced protection.\n",
    "\n",
    "3. **Homomorphic Encryption (HE)**\n",
    "    - **Goal:** Secures model updates by allowing computations on encrypted data without decryption, preventing unauthorized access.\n",
    "    - **How It Works:** Clients encrypt their model updates before sharing them with the server, which then aggregates the encrypted updates without decrypting. Only the clients can decrypt the final global model.\n",
    "    - **Impact on FL:**\n",
    "        * Prevents the server from accessing individual client updates.\n",
    "        * Protects against data reconstruction attacks.\n",
    "        * Ensures privacy even in untrusted environments.\n",
    "        * Introduces computational overhead, requiring optimization for real-world deployment.\n",
    "\n",
    "Both DP and HE complement each other in FL: DP protects against inference attacks, while HE ensures confidentiality during model aggregation.\n",
    "\n",
    "Now let's move on to [Chapter 6](../../chapter-6_Security_in_federated_compute_system/06.0_introduction/introduction.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
