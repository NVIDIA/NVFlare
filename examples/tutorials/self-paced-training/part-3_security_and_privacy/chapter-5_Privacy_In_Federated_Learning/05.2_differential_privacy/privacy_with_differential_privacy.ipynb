{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1398ef0a-f189-4d04-a8a9-276a17ab0f8b",
   "metadata": {},
   "source": [
    "# Federated Learning with Differential Privacy\n",
    "\n",
    "Please make sure you set up a virtual environment and follow [example root readme](../../README.md) before starting this notebook.\n",
    "Then, install the requirements.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>NOTE</b> Some of the cells below generate long text output.  We're using <pre>%%capture --no-display --no-stderr cell_output</pre> to suppress this output.  Comment or delete this line in the cells below to restore full output.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002e45c-f58e-4f68-bb5a-9626e084947f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display --no-stderr cell_output\n",
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddd90a1-fe96-4f24-b360-bbe73b24e34a",
   "metadata": {},
   "source": [
    "### Differential Privacy (DP)\n",
    "[Differential Privacy (DP)](https://arxiv.org/abs/1910.00962) [1] is a rigorous mathematical framework designed to provide strong privacy guarantees when handling sensitive data. In the context of Federated Learning (FL), DP plays a crucial role in safeguarding user information by introducing randomness into the training process. Specifically, it ensures privacy by adding carefully calibrated noise to the model updates—such as gradients or weights—before they are transmitted from clients to the central server. This obfuscation mechanism makes it statistically difficult to infer whether any individual data point contributed to a particular update, thereby protecting user-specific information.\n",
    "\n",
    "By integrating DP into FL, even if an adversary gains access to the aggregated updates or models, the added noise prevents them from accurately deducing sensitive details about any individual client's data. Common approaches include \n",
    "\n",
    "1. **Local Differential Privacy (LDP)**, where noise is added directly on the client side before updates are sent\n",
    "2. **Global Differential Privacy (GDP)**, where noise is injected after aggregation at the server.\n",
    "\n",
    "The balance between privacy and model utility is typically managed through a privacy budget (ϵ), which quantifies the trade-off between the level of noise added and the resulting model accuracy.\n",
    "\n",
    "\n",
    "As a first example, we show you how to add **local** DP filters to your FL training in NVFlare. Here, we use the \"Sparse Vector Technique\", i.e. the [SVTPrivacy](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.app_common.filters.svt_privacy.html) protocol, as utilized in [Li et al. 2019](https://arxiv.org/abs/1910.00962) [1] (see [Lyu et al. 2016](https://arxiv.org/abs/1603.01699) [2] for more information). \n",
    "\n",
    "DP is added as an NVFlare `Filter` using the [FedJob API](https://nvflare.readthedocs.io/en/main/programming_guide/fed_job_api.html#fedjob-api) you should have seen in prior chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eca46d7-58e5-4544-a96e-3d52821593ed",
   "metadata": {},
   "source": [
    "#### Sparse Vector Technique\n",
    "\n",
    "The [Sparse Vector Technique](https://arxiv.org/abs/1603.01699) (SVT) enhances privacy by applying noise and thresholding to a randomly selected subset of model weights or updates, $x$. The process consists of two main steps:\n",
    "\n",
    "1. **Noise Addition:** Laplace noise is added to the absolute value of the selected weights:\n",
    "\n",
    "$abs(x)+Lap(s)$\n",
    "\n",
    "2. **Thresholding and Clipping:** The noisy values are clipped within a predefined range $[−γ,γ]$ and shared only if they meet a thresholding condition:\n",
    "\n",
    "$clip(x+Lap(s),γ)$\n",
    "\n",
    "Here, $abs(x)$ represents the absolute value, $Lap(s)$ is noise sampled from the Laplace distribution, $γ$ is the predefined threshold, and $clip(x,γ)$ ensures values remain within the specified range.\n",
    "\n",
    "The experimental results show that there is a tradeoff between model performance and privacy preservation, where stronger privacy guarantees may impact the model performance more severly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0c692a-16dc-4ef9-a432-4b7375a2a7d6",
   "metadata": {},
   "source": [
    "## Run experiments with FL simulator\n",
    "For simplicity, we focus on training a simple federated CNN for CIFAR-10 classification (see its definition in [net.py](src/net.py)). FL simulator is used for running the FL experiments.\n",
    "\n",
    "The experiments are separated into three parts\n",
    "\n",
    "1. Train a model using the FedAvg algorithm with four clients without DP.\n",
    "2. Train the same model using DP added as an NVFlare `Filter`.\n",
    "3. Train the same model using [Opacus'](https://opacus.ai) PrivacyEngine on the client to implementemt local [DP-SGD](https://arxiv.org/abs/1607.00133) [3]. In this case, DP noise is added during each optimization step of the local training and we can skip the additional DP filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3fe64-3915-4c6a-9bed-694d205b0940",
   "metadata": {},
   "source": [
    "#### 0. Download the CIFAR-10 data\n",
    "First, we download the CIFAR-10 dataset to avoid clients overwriting each other's local dataset during this simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609d2de-d033-45a1-b9fa-1ba311bd00e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "DATASET_PATH = \"/tmp/nvflare/data\"\n",
    "torchvision.datasets.CIFAR10(root=DATASET_PATH, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b05b1b-31d6-4e9a-a07f-90cf9fba37b7",
   "metadata": {},
   "source": [
    "## 1. Train without DP\n",
    "#### 1.1 Define a FedJob\n",
    "The `FedJob` is used to define how controllers and executors are placed within a federated job using the `to(object, target)` routine.\n",
    "\n",
    "Here we use a PyTorch `BaseFedJob`, where we can define the job name and the initial global model.\n",
    "The `BaseFedJob` automatically configures components for model persistence, model selection, and TensorBoard streaming for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b59f47-a0c5-4038-abf4-80aefc122c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.net import Net\n",
    "\n",
    "from nvflare.app_common.workflows.fedavg import FedAvg\n",
    "from nvflare.app_opt.pt.job_config.base_fed_job import BaseFedJob\n",
    "from nvflare.job_config.script_runner import ScriptRunner\n",
    "\n",
    "job = BaseFedJob(\n",
    "    name=\"cifar10_fedavg\",\n",
    "    initial_model=Net(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e00fbca-4c8a-4e3f-b258-0f0601291aa4",
   "metadata": {},
   "source": [
    "#### 1.2 Define the Controller Workflow\n",
    "Define the controller workflow and send it to the server. For simplicity, we will run the simulation only for a few round but you can increase it for the models to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283f379-8d85-4a9d-9723-1ca926e10405",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clients = 2\n",
    "\n",
    "controller = FedAvg(\n",
    "    num_clients=n_clients,\n",
    "    num_rounds=3,  # 30 rounds should converge\n",
    ")\n",
    "job.to(controller, \"server\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d8cd1-a6c3-476a-bb7b-3603f434d509",
   "metadata": {},
   "source": [
    "That completes the components that need to be defined on the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a27daa-8d73-4bfb-9400-db9c33e24f7e",
   "metadata": {},
   "source": [
    "#### 1.3 Add clients\n",
    "Next, we can use the `ScriptRunner` and send it to each of the clients to run our training script.\n",
    "\n",
    "Note that our script could have additional input arguments, such as batch size or data path, but we don't use them here for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bfa4b-307f-4880-abbc-6788abe0dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clients):\n",
    "    runner = ScriptRunner(\n",
    "        script=\"src/cifar10_fl.py\"\n",
    "    )\n",
    "    job.to(runner, f\"site-{i+1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a13662-97bf-444b-bfcd-d70f67fc7e80",
   "metadata": {},
   "source": [
    "That's it!\n",
    "\n",
    "#### 1.4 Optionally export the job\n",
    "Now, we could export the job and submit it to a real NVFlare deployment using the [Admin client](https://nvflare.readthedocs.io/en/main/real_world_fl/operation.html) or [FLARE API](https://nvflare.readthedocs.io/en/main/real_world_fl/flare_api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e3d58-a5bc-44f2-997e-2f653b36739e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job.export_job(\"job_configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5135c9-13c2-4e3c-8e53-4047bccf42ec",
   "metadata": {},
   "source": [
    "#### 1.5 Run FL Simulation\n",
    "Finally, we can run our FedJob in simulation using NVFlare's [simulator](https://nvflare.readthedocs.io/en/main/user_guide/nvflare_cli/fl_simulator.html) under the hood.\n",
    "\n",
    "The results will be saved in the specified `workdir`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebc6f36-4d0c-41ac-8540-32dac533043a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job.simulator_run(f\"/tmp/nvflare/{job.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82a3be9-9e58-44ca-9d3f-e85456de7f12",
   "metadata": {},
   "source": [
    "## 2. Add DP as an NVFlare Filter\n",
    "#### 2.1 Run FL Simulation with DP\n",
    "Run the FL simulator with two clients for federated learning with differential privacy. The key now is to add a filter to each client that applies DP before sending the model updates back to the server\n",
    "using the `job.to()` method.\n",
    "\n",
    "Let's create a new FedJob with the DP add through the [SVTPrivacy](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.app_common.filters.html#nvflare.app_common.filters.SVTPrivacy) Filter implementing the **sparse vector technique** for differential privacy [[2]](https://arxiv.org/abs/1603.01699). Note that the epsilon used here is different from the epsilon defining the privacy budget in DP. See [[1]](https://arxiv.org/abs/1910.00962), [[2]](https://arxiv.org/abs/1603.01699) for more details on its usage.\n",
    "\n",
    "> **Note:** Use `filter_type=FilterType.TASK_RESULT` as we are adding the filter on top of the model updates after local training.\n",
    "> \n",
    "> Furthermore, this filter was developed for use with weight differences. So, we use `params_transfer_type=TransferType.DIFF` here when specifying the `ScriptRunner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e6fca-8098-4be4-8d75-6b5e7ab1869d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvflare import FilterType\n",
    "from nvflare.client.config import TransferType\n",
    "from nvflare.app_common.filters import SVTPrivacy\n",
    "\n",
    "# Create BaseFedJob with the initial model\n",
    "job = BaseFedJob(\n",
    "  name=\"cifar10_fedavg_dp\",\n",
    "  initial_model=Net(),\n",
    ")\n",
    "\n",
    "# Define the controller and send to server\n",
    "controller = FedAvg(\n",
    "    num_clients=n_clients,\n",
    "    num_rounds=3,  # 100 rounds should converge\n",
    ")\n",
    "job.to_server(controller)\n",
    "\n",
    "# Add clients\n",
    "for i in range(n_clients):\n",
    "    runner = ScriptRunner(\n",
    "        script=\"src/cifar10_fl.py\",\n",
    "        params_transfer_type=TransferType.DIFF\n",
    "    )\n",
    "    job.to(runner, f\"site-{i+1}\")\n",
    "\n",
    "    # add privacy filter.\n",
    "    dp_filter = SVTPrivacy(fraction=0.9, epsilon=0.1, noise_var=0.1, gamma=1e-5)\n",
    "    job.to(dp_filter, f\"site-{i+1}\", tasks=[\"train\"], filter_type=FilterType.TASK_RESULT)\n",
    "\n",
    "# Optionally export the configuration\n",
    "job.export_job(\"job_configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a661c68e-6b7f-4215-93e3-d4fe55eb5e7e",
   "metadata": {},
   "source": [
    "Next, start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6b911d-a171-49b1-ad2e-b0d73032110c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job.simulator_run(f\"/tmp/nvflare/{job.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf57a13-842b-46e0-9032-69d3a03b3182",
   "metadata": {},
   "source": [
    "> **Note:** you can also try adding or combining the filters with other privacy filters or customize them. For example, use the [PercentilePrivacy](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.app_common.filters.html#nvflare.app_common.filters.PercentilePrivacy) filter based on Shokri and Shmatikov ([Privacy-preserving deep learning, CCS '15](https://dl.acm.org/doi/abs/10.1145/2810103.2813687)) or [ExcludeVars](https://nvflare.readthedocs.io/en/main/apidocs/nvflare.app_common.filters.html#nvflare.app_common.filters.ExcludeVars) filter to exclude variables that shouldn't be shared with the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ef27f-3602-4ccc-8d0e-34137a384c00",
   "metadata": {},
   "source": [
    "## 3. Run DP-SGD with Privacy Budgeting during local training\n",
    "To implement local DP-SGD during client training, we can simply use [Opacus' PrivacyEngine](https://opacus.ai/). For that, we need to modify our training script to add the privacy engine and apply it to our optimizer and data loaders. For example:\n",
    "```\n",
    "# Add PrivacyEngine\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, data_loader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=data_loader,\n",
    "    noise_multiplier=1.1,\n",
    "    max_grad_norm=1.0,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e59246-1ece-40c7-9723-562da0c7fd83",
   "metadata": {},
   "source": [
    "The remaining code is as usual. To enable it, we need to add the `--target_epsilon` argument to our [training script](src/cifar10_fl.py) when using the `ScriptRunner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e28fbf-6150-4481-929d-af40dbd59adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvflare import FilterType\n",
    "from nvflare.client.config import TransferType\n",
    "from nvflare.app_common.filters import SVTPrivacy\n",
    "\n",
    "# Create BaseFedJob with the initial model\n",
    "job = BaseFedJob(\n",
    "  name=\"cifar10_fedavg_dpsgd\",\n",
    "  initial_model=Net(),\n",
    ")\n",
    "\n",
    "# Define the controller and send to server\n",
    "controller = FedAvg(\n",
    "    num_clients=n_clients,\n",
    "    num_rounds=3,  # 100 rounds should converge\n",
    ")\n",
    "job.to_server(controller)\n",
    "\n",
    "# Add clients\n",
    "for i in range(n_clients):\n",
    "    runner = ScriptRunner(\n",
    "        script=\"src/cifar10_fl.py\",\n",
    "        script_args=\"--target_epsilon=50.0\",  # lower epsilon will increase privacy but impact accuracy more\n",
    "        params_transfer_type=TransferType.DIFF\n",
    "    )\n",
    "    job.to(runner, f\"site-{i+1}\")\n",
    "\n",
    "# Optionally export the configuration\n",
    "job.export_job(\"job_configs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4676e7d0-6968-485d-bf77-911bd729586d",
   "metadata": {},
   "source": [
    "Again, we can start the training using the simulator call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b93894b-4fcd-4814-8d59-ac8239a808bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job.simulator_run(f\"/tmp/nvflare/{job.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90c423d-4784-4956-961f-e2ea1ef1b30e",
   "metadata": {},
   "source": [
    "## 4. Visualize the results\n",
    "Finally, you can plot the results by running `tensorboard --logdir /tmp/nvflare` in a new terminal. In this notebook, we only run for a few FL rounds for simplicity. If you uncomment the recommended number of in the FedAvg controller definitions of the cells, you can run the experiments until convergence. As one can observe, the model with DP (red) takes more rounds to achieve a comparable training performance but has less risks of leaking private information compared to the model trained without DP (orange). For more details, on how to apply this filter in a medical imaging use case, see [Li et al. 2019](https://arxiv.org/abs/1910.00962) [1].\n",
    "\n",
    "![TensorBoard Training curve of FedAvg without and with DP](tb_curve_dp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a5e1b-fd7c-44bd-9227-f5a24951dbd4",
   "metadata": {},
   "source": [
    "### Summary\n",
    "This notebook explores two methods for adding Differential Privacy (DP) noise to model training: using an NVFlare `Filter` or integrating Opacus within your local training script. These approaches enhance privacy by reducing the risk of memorizing individual data points. \n",
    "\n",
    "DP remains an active research area, and the optimal choice of parameters depends on your specific problem and risk tolerance. A smaller epsilon provides stronger privacy but introduces more noise, potentially lowering accuracy. This trade-off should be carefully navigated. A recommended technique from [Opacus](https://opacus.ai/tutorials/building_image_classifier#Tips-and-Tricks) is to pre-train on public data before fine-tuning on private data.  \n",
    "\n",
    "Further research into quantifying model memorization and data leakage during training can provide deeper insights into privacy risks. For more details, refer to our paper on [gradient inversion](../../../../../../research/quantifying-data-leakage/README.md) [[4]](https://arxiv.org/abs/2202.06924) which is also implemented in NVFlare.\n",
    "\n",
    "For training large language models with DP, refer to the latest Opacus examples, which can be seamlessly integrated into NVFlare deployments. If you are using TensorFlow, you can achieve similar privacy protections with [TensorFlow Privacy](https://www.tensorflow.org/responsible_ai/privacy/guide)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccafc0dc-ba60-4aa5-a807-0654625ece50",
   "metadata": {},
   "source": [
    "Next, we will learn how to protect the model updates using [homomorphic encryption](../05.3_homomorphic_encryption/05.3.1_privacy_with_homormorphic_encryption.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d9c232-e4ae-4c25-acd9-e0cbe756a423",
   "metadata": {},
   "source": [
    "#### References\n",
    "[1] Li, W., Milletarì, F., Xu, D., Rieke, N., Hancox, J., Zhu, W., Baust, M., Cheng, Y., Ourselin, S., Cardoso, M.J. and Feng, A., 2019, October. Privacy-preserving federated brain tumour segmentation. In International workshop on machine learning in medical imaging (pp. 133-141). Springer, Cham.\n",
    "\n",
    "[2] Lyu, M., Su, D., & Li, N. (2016). Understanding the sparse vector technique for differential privacy. arXiv preprint arXiv:1603.01699.\n",
    "\n",
    "[3] Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., & Zhang, L. (2016, October). Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC conference on computer and communications security (pp. 308-318).\n",
    "\n",
    "[4] Hatamizadeh, A., Yin, H., Molchanov, P., Myronenko, A., Li, W., Dogra, P., ... & Roth, H. R. (2023). Do gradient inversion attacks make federated learning unsafe?. IEEE Transactions on Medical Imaging, 42(7), 2044-2056."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
