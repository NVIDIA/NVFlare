{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e129ede5",
   "metadata": {},
   "source": [
    "# Federated Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf7e391",
   "metadata": {},
   "source": [
    "We like to demo federated experiment tracking via different tools in the following examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec76f4",
   "metadata": {},
   "source": [
    "\n",
    "Example of using [NVIDIA FLARE](https://nvflare.readthedocs.io/en/main/index.html) to train an image classifier using federated averaging ([FedAvg]([FedAvg](https://arxiv.org/abs/1602.05629))) and [PyTorch](https://pytorch.org/) as the deep learning training framework. \n",
    "\n",
    "This example also highlights the streaming capability from the clients to the server with MLFLow \n",
    "\n",
    "> **_NOTE:_** This example uses the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset and will load its data within the trainer code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be1fd6",
   "metadata": {},
   "source": [
    "## 0. Prerequisits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e06c1",
   "metadata": {},
   "source": [
    "Assume you have completed the followings: \n",
    "* create and activate the venv\n",
    "* installed NVFLARE, if not, ollow the [Installation](https://nvflare.readthedocs.io/en/main/getting_started.html#installation) instructions.\n",
    "\n",
    "* git clone the code NVFLARE repository \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2d6b0",
   "metadata": {},
   "source": [
    "## 1. Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04895127",
   "metadata": {},
   "source": [
    "We have two jobs under hello-pt-tb\n",
    "* hello-tb\n",
    "* hello-tb-mix\n",
    "\n",
    "hello-tb -- demonstrates the TBWriter and TBAnalyticsReceiver \n",
    "hello-tb-mix -- demonstrate the same code in previous job (hello-tb), can be displayed in MLFlow and WandB in addition to Tensorboard, by adding MLFLow Receiver. \n",
    "\n",
    "Since these two jobs have the same custom code, the only difference is the configurations, we move the custom shared directory to allow both jobs using the same code. By doing this, you need to specify the PYTHONPATH to include this custom directory. \n",
    "\n",
    "**_note_** if you don't have ```tree``` installed, you can change the beflow command to ``` ls -al hello-pt-tb/*```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429181a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello-pt-tb\n",
      "├── custom\n",
      "│   ├── pt_constants.py\n",
      "│   ├── pt_learner.py\n",
      "│   ├── simple_network.py\n",
      "│   └── test_custom.py\n",
      "├── hello-tb\n",
      "│   ├── app\n",
      "│   │   └── config\n",
      "│   │       ├── config_fed_client.json\n",
      "│   │       └── config_fed_server.json\n",
      "│   └── meta.json\n",
      "├── hello-tb-mix\n",
      "│   ├── app\n",
      "│   │   └── config\n",
      "│   │       ├── config_fed_client.json\n",
      "│   │       └── config_fed_server.json\n",
      "│   └── meta.json\n",
      "└── README.md\n",
      "\n",
      "7 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "tree hello-pt-tb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927c34be",
   "metadata": {},
   "source": [
    "### Experiment Tracking with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56e4870",
   "metadata": {},
   "source": [
    "Go to terminal and install additional requirements\n",
    "\n",
    "```\n",
    "pip3 install torch torchvision tensorboard\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a2c76a",
   "metadata": {},
   "source": [
    "#### Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca0050",
   "metadata": {},
   "source": [
    "**Client Config**\n",
    "```\n",
    " \"components\": [\n",
    "    {\n",
    "      \"id\": \"pt_learner\",\n",
    "      \"path\": \"pt_learner.PTLearner\",\n",
    "      \"args\": {\n",
    "        \"lr\": 0.01,\n",
    "        \"epochs\": 5,\n",
    "        \"analytic_sender_id\": \"log_writer\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"log_writer\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.tb.tb_writer.TBWriter\",\n",
    "      \"args\": {\"event_type\": \"analytix_log_stats\"}\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"event_to_fed\",\n",
    "      \"name\": \"ConvertToFedEvent\",\n",
    "      \"args\": {\"events_to_convert\": [\"analytix_log_stats\"], \"fed_event_prefix\": \"fed.\"}\n",
    "    }\n",
    "  ]\n",
    "  ```\n",
    "  Note PTLearner requires \"analytic_sender_id\", this os the LogWriter Component id, for backward compatibility reason, we did not change the argument name, so existing code doesn't need to change the configurable. \n",
    "  \n",
    "  **Server Config**\n",
    "  \n",
    "  ```\n",
    "   \"components\": [\n",
    "   \n",
    "    ... <omit other components> ...\n",
    "    \n",
    "    {\n",
    "      \"id\": \"tb_analytics_receiver\",\n",
    "      \"name\": \"TBAnalyticsReceiver\",\n",
    "      \"args\": {\"events\": [\"fed.analytix_log_stats\"]}\n",
    "    }\n",
    "  ],\n",
    "  \n",
    "  ```\n",
    "  \n",
    " On the server side, we registered TBAnalyticsReceiver\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb33f760",
   "metadata": {},
   "source": [
    "#### Run the experiment\n",
    "\n",
    "first change directory to examples/experiment_tracking\n",
    "\n",
    "``` \n",
    "cd example/experiment_tracking\n",
    "\n",
    "```\n",
    "then, use nvflare simulator to run the hello-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b5579b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "PYTHONPATH=$PYTHONPATH:$(pwd)/hello-pt-tb/custom\n",
    "nvflare simulator -w /tmp/nvflare/ -n 2 -t 2 hello-pt-tb/hello-tb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeeb5d2",
   "metadata": {},
   "source": [
    "####  View Results\n",
    "\n",
    "**Tensorboard View**\n",
    "\n",
    "from terminal: \n",
    "```\n",
    "tensorboard --logdir=/tmp/nvflare/simulate_job/tb_events\n",
    "TensorFlow installation not found - running with reduced feature set.\n",
    "\n",
    "NOTE: Using experimental fast data loading logic. To disable, pass\n",
    "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
    "    https://github.com/tensorflow/tensorboard/issues/4784\n",
    "\n",
    "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "TensorBoard 2.11.0 at http://localhost:6007/ (Press CTRL+C to quit)\n",
    "\n",
    "```\n",
    "then open broser with  http://localhost:6007/ URL\n",
    "\n",
    "\n",
    "If the server is running on a remote machine, use port forwarding to view the TensorBoard dashboard in a browser.\n",
    "For example:\n",
    "```\n",
    "ssh -L {local_machine_port}:127.0.0.1:6006 user@server_ip)\n",
    "```\n",
    "\n",
    "> **_NOTE:_** For a more in-depth guide about the TensorBoard streaming feature, see [Quickstart (PyTorch with TensorBoard)](https://nvflare.readthedocs.io/en/main/examples/hello_pt_tb.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9672a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "tensorboard --logdir=/tmp/nvflare/simulate_job/tb_events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb4714",
   "metadata": {},
   "source": [
    "## Tensorboard with different receivers\n",
    "\n",
    "Let's assume we would like to use MLFLow or Weights & Biases except we don't want to re-rewrite the code in order to capture the metrics. With NVFLARE, this is really easy, All you need to do is add two the needed receiver. You can add both MLFlow and Weights & Biases receivers if you like. \n",
    "\n",
    "In the following example hello-tb-mix, we just do that. \n",
    "\n",
    "There is no change in the custom code and client configuration. However, we did replace the TBAnalyticReceiver with MLFLow Receiver and WandB Receiver on the server configuration \n",
    "\n",
    "**Server Config**\n",
    "\n",
    "Details of the configurations will be explained in MLFlow example. Here we only to demonstrate you can use them without code changes\n",
    "\n",
    "\n",
    "**_MLflow Recevier_**\n",
    "\n",
    "```\n",
    "\n",
    "   {\n",
    "      \"id\": \"mlflow_receiver\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_receiver.MLFlowReceiver\",\n",
    "      \"args\": {\n",
    "        \"kwargs\": {\n",
    "          \"experiment_name\": \"hello-pt-experiments\"\n",
    "        },\n",
    "        \"artifact_location\": \"artifacts\"\n",
    "      }\n",
    "    },\n",
    "```    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "pip3 install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32a7438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# keep track of original python to avoid mixup in different examples\n",
    "ORIGIN_PATH=$PYTHONPATH\n",
    "echo $ORIGIN_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fefe37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "PYTHONPATH=$ORIGIN_PATH:$( pwd )/hello-pt-tb/custom  \n",
    "echo $PYTHONPATH\n",
    "nvflare simulator -w /tmp/nvflare/ -n 2 -t 2 hello-pt-tb/hello-tb-mix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba6b081",
   "metadata": {},
   "source": [
    "The result for MLFlow experiment run can be found in \n",
    "\n",
    "```\n",
    "/tmp/nvflare/mlruns\n",
    "```\n",
    "\n",
    "How to view the result, we will leave to the MLFLow examples to discuss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7faa6d7c",
   "metadata": {},
   "source": [
    "## 2. MLFlow\n",
    "\n",
    "We have two jobs under hello-pt-mlflow\n",
    "* hello-mlflow\n",
    "* hello-mlflow-mix\n",
    "\n",
    "hello-mlflow -- demonstrates the MLFlowWriter and MLFLowReceiver \n",
    "hello-mlflow-mix -- demonstrate the same code in hello-mlflow, can be displayed in Tensorboard in addition to MLFlow, by adding Tensorboard\n",
    "\n",
    "Since these two jobs have the same custom code, the only difference is the configurations, we move the custom shared directory to allow both jobs using the same code. By doing this, you need to specify the PYTHONPATH to include this custom directory. \n",
    "\n",
    "**_note_** if you don't have ```tree``` installed, you can change the beflow command to ``` ls -al hello-pt-mlflow/*```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11af95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello-pt-mlflow\n",
      "├── custom\n",
      "│   ├── pt_constants.py\n",
      "│   ├── pt_learner.py\n",
      "│   ├── simple_network.py\n",
      "│   └── test_custom.py\n",
      "├── hello-mlflow\n",
      "│   ├── app\n",
      "│   │   └── config\n",
      "│   │       ├── config_fed_client.json\n",
      "│   │       └── config_fed_server.json\n",
      "│   └── meta.json\n",
      "└── hello-mlflow-mix\n",
      "    ├── app\n",
      "    │   └── config\n",
      "    │       ├── config_fed_client.json\n",
      "    │       └── config_fed_server.json\n",
      "    └── meta.json\n",
      "\n",
      "7 directories, 10 files\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "tree hello-pt-mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16f62d5",
   "metadata": {},
   "source": [
    "### Experiment Tracking with MLFlow\n",
    "\n",
    "First Install requirements, skip this step if you have already installed the requirement.\n",
    "```\n",
    "pip3 install mlflow\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "pip3 install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1631ba2",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "**Client Config**\n",
    "\n",
    "```\n",
    "   \"components\": [\n",
    "    {\n",
    "      \"id\": \"pt_learner\",\n",
    "      \"path\": \"pt_learner.PTLearner\",\n",
    "      \"args\": {\n",
    "        \"lr\": 0.01,\n",
    "        \"epochs\": 5,\n",
    "        \"analytic_sender_id\": \"mlflow_writer\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"mlflow_writer\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_writer.MLFlowWriter\",\n",
    "      \"args\": {\"event_type\": \"analytix_log_stats\"}\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"event_to_fed\",\n",
    "      \"name\": \"ConvertToFedEvent\",\n",
    "      \"args\": {\"events_to_convert\": [\"analytix_log_stats\"], \"fed_event_prefix\": \"fed.\"}\n",
    "    }\n",
    "  ]\n",
    "```\n",
    "\n",
    "to use MLflow API syntax, we need to register with MLFlowSender\n",
    "\n",
    "\n",
    "**Server Config**\n",
    "\n",
    "  in addition to the other normal configuration for training, we need to add the following component to handle\n",
    "  the streamed events. \n",
    "  \n",
    "  If the MLfLow tracking server is used, we need to specify the tracking URL, \n",
    "  If the MLflow tracking server is not user, we don't need to specify tracking URL in the argument. \n",
    "  \n",
    "  **with tracking server**\n",
    "  \n",
    "``` \n",
    "  {\n",
    "      \"id\": \"mlflow_receiver\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_receiver.MLFlowReceiver\",\n",
    "      \"args\": {\n",
    "        \"kwargs\": {\n",
    "          \"experiment_name\": \"hello-pt-experiment\",\n",
    "          \"run_name\": \"hello-pt-with-mlflow\",\n",
    "          \"experiment_tags\": {\n",
    "            \"mlflow.note.content\": \"experiment description\"\n",
    "          },\n",
    "          \"run_tags\": {\n",
    "            \"mlflow.note.content\": \"Run description\"\n",
    "          }\n",
    "        },\n",
    "        \"artifact_location\": \"artifacts\",\n",
    "        \"tracking_uri\": \"http://localhost:5001\"\n",
    "      }\n",
    "    } \n",
    "```    \n",
    "   **without tracking server**\n",
    "```   \n",
    "   {\n",
    "      \"id\": \"mlflow_receiver\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_receiver.MLFlowReceiver\",\n",
    "      \"args\": {\n",
    "        \"kwargs\": {\n",
    "          \"experiment_name\": \"hello-pt-experiment\",\n",
    "          \"run_name\": \"hello-pt-with-mlflow\",\n",
    "          \"experiment_tags\": {\n",
    "            \"mlflow.note.content\": \"experiment description\"\n",
    "          },\n",
    "          \"run_tags\": {\n",
    "            \"mlflow.note.content\": \"Run description\"\n",
    "          }\n",
    "        },\n",
    "        \"artifact_location\": \"artifacts\",\n",
    "      }\n",
    "    } \n",
    "```\n",
    "\n",
    "\n",
    "You can set the experiment name in the configuration. You can setup the relative artifacts location ( no need to precreate it). The artifact location is only used for log text. it is not used for parameters and metrics\n",
    "\n",
    "\n",
    "**experiment logging code changes**\n",
    "\n",
    "Different from Tensorboard SummaryWriter API syntax, where add_scalar() or add_scalars() were used, there we the MLFLow Writer follows the MLFlow API with log_params(), log_metrics() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8284b4a2",
   "metadata": {},
   "source": [
    "#### MLFlow Tracking Server\n",
    " \n",
    "MLFLow Tracking Server can be setup and deployed separately. For example, in Azure ML Workspace, the MLFlow tracking server is already setup, all one needs is to find out the tracking URL\n",
    " \n",
    "In this example, we will setup a simple tracking server with SQLite database: \n",
    "\n",
    "```\n",
    "mlflow server --backend-store-uri=sqlite:///mlruns.db  --host localhost --port 5000\n",
    "\n",
    "```\n",
    "the user then can go to http://localhost:5000 to monitoring the experiments during job run\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe3165d",
   "metadata": {},
   "source": [
    "#### Run the experiment\n",
    "\n",
    "Use nvflare simulator to run the hello-mlflow. Recall that we have move the custom code in the a shared location, we need to set PYTHONPATH so the simulator can find it. \n",
    "\n",
    "First cd to `examples/experiment_tracking` directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f08cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "PYTHONPATH=$ORIGIN_PATH:$( pwd )/hello-pt-mlflow/custom  \n",
    "echo $PYTHONPATH\n",
    "\n",
    "nvflare simulator -w /tmp/nvflare/ -n 2 -t 2 hello-pt-mlflow/hello-mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1b9f60",
   "metadata": {},
   "source": [
    "#### Runing experiments without tracking server and tracking URL\n",
    "\n",
    "if we don't specify tracking URL and no tracking server. \n",
    "\n",
    "we can simply run the experiments as before. Meanwhile, we can track the progress by the following command\n",
    "(notice our workspace is point to /tmp/nvflare) \n",
    "\n",
    "\n",
    "**mlflow ui --backend-store-uri=/tmp/nvflare/mlrun**\n",
    "\n",
    "\n",
    "run above from terminal ( it doesn't work running from Notebook)\n",
    "```\n",
    " mlflow ui --backend-store-uri=/tmp/nvflare/mlruns\n",
    " \n",
    "[2023-01-05 15:30:38 -0800] [71735] [INFO] Starting gunicorn 20.1.0\n",
    "[2023-01-05 15:30:38 -0800] [71735] [INFO] Listening at: http://127.0.0.1:5000 (71735)\n",
    "[2023-01-05 15:30:38 -0800] [71735] [INFO] Using worker: sync\n",
    "[2023-01-05 15:30:38 -0800] [71737] [INFO] Booting worker with pid: 71737\n",
    "[2023-01-05 15:30:38 -0800] [71738] [INFO] Booting worker with pid: 71738\n",
    "[2023-01-05 15:30:38 -0800] [71739] [INFO] Booting worker with pid: 71739\n",
    "[2023-01-05 15:30:38 -0800] [71740] [INFO] Booting worker with pid: 71740\n",
    "\n",
    "```\n",
    "\n",
    "then user should open http://127.0.0.1:5000 via browser check the results\n",
    "\n",
    "\n",
    "If the server is running on a remote machine, use port forwarding to view the TensorBoard dashboard in a browser.\n",
    "For example:\n",
    "```\n",
    "ssh -L {local_machine_port}:127.0.0.1:5000 user@server_ip)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc31ce",
   "metadata": {},
   "source": [
    "#### View Result\n",
    "\n",
    "In this example, we specify the Tracking URL, you can review the result in browser http://localhost:5000/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc9481",
   "metadata": {},
   "source": [
    "### MLFlow with different receivers\n",
    "\n",
    "Similar to hello-mlflow job, let's assume we would like to use MLFLow except we don't want to re-rewrite the code in order to capture the metrics. We will need to do is to add the needed receiver. There is no change in the custom code and client configuration. However, we add Tensorboard Receiver. In this exampole, we removed the tracking URI for the MLFLow Receiver. \n",
    "\n",
    "**Server Config**\n",
    "\n",
    "Details of the configurations will be explained in Tensorboard example. Here we only to demonstrate you can use them without code changes\n",
    "\n",
    "Tensorboard Recevier\n",
    "\n",
    "```\n",
    "   {\n",
    "      \"id\": \"tb_analytics_receiver\",\n",
    "      \"name\": \"TBAnalyticsReceiver\",\n",
    "      \"args\": {\"events\": [\"fed.analytix_log_stats\"]}\n",
    "    },\n",
    "    \n",
    "```\n",
    "```\n",
    "  {\n",
    "      \"id\": \"mlflow_receiver_without_tracking_uri\",\n",
    "      \"path\": \"nvflare.app_opt.tracking.mlflow.mlflow_receiver.MLFlowReceiver\",\n",
    "      \"args\": {\n",
    "        \"kwargs\": {\n",
    "          \"experiment_name\": \"hello-pt-experiment\",\n",
    "          \"run_name\": \"hello-pt-with-mlflow\",\n",
    "          \"experiment_tags\": {\n",
    "            \"mlflow.note.content\": \"## **Hello PyTorch experiment with MLFLOW**\",\n",
    "            \"version\": \"v1\",\n",
    "            \"priority\": \"P1\"\n",
    "          },\n",
    "          \"run_tags\": {\n",
    "            \"mlflow.note.content\": \"run description\"\n",
    "          }\n",
    "        },\n",
    "        \"artifact_location\": \"artifacts\"\n",
    "      }\n",
    "    },\n",
    "```\n",
    "\n",
    "Before we run the job, we will need to install the additional requirements ( skip if you have done it)\n",
    "\n",
    "```\n",
    "pip3 install torch torchvision tensorboard \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9c7440",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "pip3 install torch torchvision tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22f695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "PYTHONPATH=$ORIGIN_PATH:$( pwd )/hello-pt-mlflow/custom  \n",
    "echo $PYTHONPATH\n",
    "\n",
    "nvflare simulator -w /tmp/nvflare/ -n 2 -t 2 hello-pt-mlflow/hello-mlflow-mix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36936541",
   "metadata": {},
   "source": [
    "**View Results**\n",
    "\n",
    "Since there is no tracking URI, From terminal: \n",
    "\n",
    "```\n",
    "mlflow ui --backend-store-uri=/tmp/nvflare/mlruns\n",
    "```\n",
    "\n",
    "Then, look at the URL in browser http://localhost:5000/ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9aec11",
   "metadata": {},
   "source": [
    "## Weights and Biases\n",
    "\n",
    "In hello-wandb -- we like to show two parts: \n",
    "\n",
    "1) how to customize the Reciever and LogWriter, so that you can create your own integration for another ML experiment tracking tool.  The  hello-wandb/wandb contains both WandBReceiver and LogWriter. You can read the README to learn about the design consideration for the work. \n",
    "\n",
    "2) hell-wandb/custom directory shows the same hello-pt example, using WandBWriter to log the metrics. \n",
    " \n",
    "\n",
    "**_Note_** Weights and Biases (W&B or WandB) requires registratio on their website. Its free for individual personal and open source use. But not not free for commercial or team use. \n",
    "\n",
    "**Assuming you have the account at W&B and already logins. You can follow the examples**\n",
    "\n",
    "First install the wandb \n",
    "\n",
    "```\n",
    "pip3 install wandb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5775709",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "pip3 install wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "388f558c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello-pt-wandb\n",
      "├── app\n",
      "│   ├── config\n",
      "│   │   ├── config_fed_client.json\n",
      "│   │   └── config_fed_server.json\n",
      "│   └── custom\n",
      "│       ├── pt_constants.py\n",
      "│       ├── pt_learner.py\n",
      "│       ├── simple_network.py\n",
      "│       └── test_custom.py\n",
      "├── meta.json\n",
      "└── wandb\n",
      "    ├── __init__.py\n",
      "    ├── README.md\n",
      "    ├── wandb_receiver.py\n",
      "    └── wandb_writer.py\n",
      "\n",
      "4 directories, 11 files\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "tree hello-pt-wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a6c089",
   "metadata": {},
   "source": [
    "### Experiments with W&B\n",
    "\n",
    "This example, we should WandB Writer and Receiver to track experiments. We can also use mixed receivers such as Tensorboard Receiver and MLFLow Reciever. But the process is the same and there is no need to repeat.\n",
    "\n",
    "**Client Config**\n",
    "\n",
    "```\n",
    " \"components\": [\n",
    "    {\n",
    "      \"id\": \"pt_learner\",\n",
    "      \"path\": \"pt_learner.PTLearner\",\n",
    "      \"args\": {\n",
    "        \"lr\": 0.01,\n",
    "        \"epochs\": 5,\n",
    "        \"analytic_sender_id\": \"log_writer\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"log_writer\",\n",
    "      \"path\": \"wandb_writer.WandBWriter\",\n",
    "      \"args\": {\"event_type\": \"analytix_log_stats\"}\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"event_to_fed\",\n",
    "      \"name\": \"ConvertToFedEvent\",\n",
    "      \"args\": {\"events_to_convert\": [\"analytix_log_stats\"], \"fed_event_prefix\": \"fed.\"}\n",
    "    }\n",
    "  ]\n",
    "```\n",
    "\n",
    "Here we use W&B log() API syntax, using WandBWriter. \n",
    "\n",
    "\n",
    "\n",
    "**Server Config**\n",
    " \n",
    "WandB Receiver\n",
    "\n",
    "```\n",
    "    {\n",
    "      \"id\": \"wandb_receiver\",\n",
    "      \"path\": \"wandb_receiver.WandBReceiver\",\n",
    "      \"args\": {\n",
    "          \"mode\": \"offline\",\n",
    "\n",
    "          \"config\": {\n",
    "            \"architecture\": \"CNN\",\n",
    "            \"dataset_id\": \"CIFAR10\",\n",
    "            \"momentum\": 0.9,\n",
    "            \"optimizer\": \"SGD\",\n",
    "            \"learning rate\": 0.01,\n",
    "            \"epochs\": 5\n",
    "          },\n",
    "\n",
    "          \"kwargs\" :  {\n",
    "            \"project\": \"hello-pt-experiment\",\n",
    "            \"notes\": \"descripton\",\n",
    "            \"tags\": [\"baseline\", \"paper1\"],\n",
    "            \"group\": \"hello-pt\",\n",
    "            \"job_type\": \"train-validate\"\n",
    "          }\n",
    "      }\n",
    "    }\n",
    "```    \n",
    "\n",
    "In Weights and Biases, you can specify the init() project anr run related information specified in **kwargs**\" argument. the key of the argument must much the WandB init() API. Here you can add project, notes, tags etc. \n",
    "\n",
    "You can also set the configuration parameters related to the experiments (whatever you think its important)\n",
    "\n",
    "\n",
    "The **\"mode\"** can be either: **\"online\"**, **\"offline\"** or **\"disabled\"** ( test only). \n",
    "\n",
    "If the mode is set to be online, the metrics will directly send to Weights & Biases hosted website, assuming you have the open the account and logged in. \n",
    "\n",
    "If the mode is set to be offline, the metrics will be written to the file and sync to the W&B Website. \n",
    "\n",
    "Now before run the job-mix example, we need to install additional requirements for mlflow and WandB, then run the job. ( skip this step if you already have them installed) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70cb537",
   "metadata": {},
   "source": [
    "Change the directory to ```example/experiment_tracking``` directory. Now run the example via simulator\n",
    "\n",
    "Make sure the PYTHONPATH doesn contains other the same classes from previous example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "PYTHONPATH=$ORIGIN_PATH:$(pwd)/hello-pt-wandb/wand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9059ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "nvflare simulator -w /tmp/nvflare/ -n 2 -t 2 hello-pt-wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d00295",
   "metadata": {},
   "source": [
    "When finish success in the **offline** mode, you will seen something like this\n",
    "```\n",
    "wandb: Waiting for W&B process to finish... (success).\n",
    "wandb: Waiting for W&B process to finish... (success).\n",
    "wandb: \n",
    "wandb: Run history:\n",
    "wandb:          train_loss █▆▆▆▄▄▃▅▄▄▄▆▅▃▄▄▄▃▄▃▄▄▄▄▃▃▂▄▂▂▃▄▂▄▁▁▃▂▂▃\n",
    "wandb: validation_accuracy ▁\n",
    "wandb: \n",
    "wandb: Run summary:\n",
    "wandb:          train_loss 0.5315\n",
    "wandb: validation_accuracy 0.4926\n",
    "wandb: \n",
    "wandb: \n",
    "wandb: Run history:\n",
    "wandb:          train_loss █▇▆▅▄▅▅▄▅▅▃▄▅▅▂▅▃▃▃▃▄▃▃▄▃▃▁▃▄▃▄▃▄▂▂▁▁▃▂▂\n",
    "wandb: validation_accuracy ▁\n",
    "wandb: \n",
    "wandb: Run summary:\n",
    "wandb:          train_loss 0.90587\n",
    "wandb: validation_accuracy 0.4888\n",
    "wandb: \n",
    "wandb: You can sync this run to the cloud by running:\n",
    "wandb: wandb sync /tmp/nvflare/wandb/offline-run-20230108_214713-3m98v39x\n",
    "wandb: You can sync this run to the cloud by running:\n",
    "wandb: wandb sync /tmp/nvflare/wandb/offline-run-20230108_214713-tfyfivqt\n",
    "wandb: Find logs at: ./wandb/offline-run-20230108_214713-tfyfivqt/logs\n",
    "wandb: Find logs at: ./wandb/offline-run-20230108_214713-3m98v39x/logs\n",
    "```\n",
    "\n",
    "You can follow the command to sync result to W&B with \n",
    "\n",
    "```\n",
    "wandb sync <path>\n",
    "```\n",
    "then you can login to the W&B website to view the result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534d7879",
   "metadata": {},
   "source": [
    "#### Trouble shoorting\n",
    "\n",
    "Since the WandB run for each client is implemented as multiprocess, with multiprocess, the common issues as noted in [WandB documentation](https://docs.wandb.ai/guides/track/advanced/distributed-training)\n",
    "* *_Hanging at the beginning of training_* \n",
    "* *_Hanging at the end of training_*\n",
    "\n",
    "One way to check is simply check nvflare simular process is running or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "ps -eaf | grep nvflare "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e9e4a4",
   "metadata": {},
   "source": [
    "kill the remain jobs. In some cases, we found it helpful by simply remove the WandB out directory to clean up from previous run. For example\n",
    "\n",
    "```\n",
    "rm /tmp/nvflare/wandb\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574be5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
