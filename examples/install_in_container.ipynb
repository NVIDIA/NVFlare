{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b3ff58-6ab9-436a-a457-1bc12d29b7a8",
   "metadata": {},
   "source": [
    "# Containerized Deployment with Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf3f32f-8f79-4969-ba96-37c7b9c55241",
   "metadata": {},
   "source": [
    "Running NVIDIA FLARE in a Docker container is sometimes a convenient way to ensure a uniform OS and software environment across client and server systems. This can be used as an alternative to the bare-metal Python virtual environment described above and will use a similar installation to simplify transitioning between a bare metal and containerized environment.\n",
    "\n",
    "To get started with a containerized deployment, you will first need to install a supported container runtime and the NVIDIA Container Toolkit to enable support for GPUs. System requirements and instructions for this can be found in the NVIDIA Container Toolkit Install Guide <https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html>.\n",
    "\n",
    "A simple Dockerfile is used to capture the base requirements and dependencies. In this case, weâ€™re building an environment that will support PyTorch-based workflows, in particular the Hello PyTorch with Tensorboard Streaming example. The base for this build is the NGC PyTorch container. On this base image, we will install the necessary dependencies and clone the NVIDIA FLARE GitHub source code into the root workspace directory. To create a Dockerfile, create a file named Dockerfile using any text editor and include the following:\n",
    "\n",
    "```\n",
    "ARG PYTORCH_IMAGE=nvcr.io/nvidia/pytorch:23.02-py3\n",
    "FROM ${PYTORCH_IMAGE}\n",
    "\n",
    "RUN python3 -m pip install -U pip\n",
    "RUN python3 -m pip install -U setuptools\n",
    "RUN python3 -m pip install torch torchvision tensorboard nvflare\n",
    "\n",
    "WORKDIR /workspace/\n",
    "RUN git clone https://github.com/NVIDIA/NVFlare.git\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "We can then build the new container by running docker build in the directory containing this Dockerfile, for example tagging it nvflare-pt:\n",
    "\n",
    "docker build -t nvflare-pt .\n",
    "This will result in a docker image, nvflare-pt:latest. You can run this container with Docker, in this example mounting a local my-workspace directory into the container for use as a persistent workspace:\n",
    "\n",
    "```\n",
    "mkdir my-workspace\n",
    "docker run --rm -it --gpus all \\\n",
    "    --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 \\\n",
    "    -w $(pwd -P)/my-workspace:/workspace/my-workspace \\\n",
    "    nvflare-pt:latest\n",
    "```    \n",
    "Once the container is running, you can also exec into the container, for example if you need another terminal to start additional FLARE clients. First find the CONTAINER ID using docker ps, and then use that ID to exec into the container:\n",
    "```\n",
    "docker ps  # use the CONTAINER ID in the output\n",
    "docker exec -it <CONTAINER ID> /bin/bash\n",
    "```\n",
    "This container can be used to run the FL Simulator or any FL server or client. When using the FL Simulator (described in the next section), you can simply mount in any directories needed for your FLARE application code, and run the Simulator within the Docker container with all dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34fe69b-aa84-45e4-94a4-b402574decb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
