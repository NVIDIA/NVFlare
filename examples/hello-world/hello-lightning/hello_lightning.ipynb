{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5c3d67-a6ea-4f59-84d2-effc3ef016e1",
   "metadata": {},
   "source": [
    "# Getting Started with NVFlare (Lightning)\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NVIDIA/NVFlare/blob/main/examples/getting_started/pt/nvflare_lightning_getting_started.ipynb)\n",
    "\n",
    "NVFlare is an open-source framework that allows researchers and\n",
    "data scientists to seamlessly move their machine learning and deep\n",
    "learning workflows into a federated paradigm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf2b4a8-ed42-421d-8898-c0c93f9d8a09",
   "metadata": {},
   "source": [
    "## Basic Concepts\n",
    "At the heart of NVFlare lies the concept of collaboration through \"tasks.\" An FL controller assigns tasks (e.g., training on local data) to one or more FL clients and processes returned results (e.g., model weight updates). Based on these results and other factors (e.g., a pre-configured number of training rounds), the controller may assign additional tasks.\n",
    "\n",
    "<img src=\"../../../docs/resources/controller_executor_no_filter.png\" alt=\"NVIDIA FLARE Controller and Executor\" width=75% height=75% />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907933a8-20fd-4aa7-a3bf-3f5b5829a544",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc38cbf",
   "metadata": {},
   "source": [
    "Install NVFlare and dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bba668-72ac-4e69-aaed-8d4254f547c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install nvflare~=2.7.0rc pytorch_lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76354477",
   "metadata": {},
   "source": [
    "If running in Google Colab, download the source code for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869ef70",
   "metadata": {
    "tags": [
     "skip-execution",
     "skip",
     "colab"
    ]
   },
   "outputs": [],
   "source": [
    "%pip install --ignore-installed blinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc3b26a",
   "metadata": {
    "tags": [
     "skip-execution",
     "skip",
     "colab"
    ]
   },
   "outputs": [],
   "source": [
    "! npx degit -f NVIDIA/NVFlare/examples/advanced/fedavg-with-early-stopping . -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cb248-dc6a-48d1-880d-33c4324d9723",
   "metadata": {},
   "source": [
    "## Federated Averaging with NVFlare\n",
    "Given the flexible controller and executor concepts, it is easy to implement different computing and communication patterns with NVFlare, such as [FedAvg](https://proceedings.mlr.press/v54/mcmahan17a?ref=https://githubhelp.com) and [cyclic weight transfer](https://academic.oup.com/jamia/article/25/8/945/4956468).\n",
    "\n",
    "The controller's `run()` routine is responsible for assigning tasks and processing task results from the executors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f84fb1-9dd3-4c72-a727-c4614260f02f",
   "metadata": {},
   "source": [
    "### Server Code\n",
    "First, we provide a simple implementation of the [FedAvg](https://proceedings.mlr.press/v54/mcmahan17a?ref=https://githubhelp.com) algorithm with NVFlare. The `run()` routine implements the main algorithmic logic. Subroutines like `sample_clients()` and `send_model_and_wait()` utilize the communicator object native to each controller to get the list of available clients, distribute the current global model to the clients, and collect their results.\n",
    "\n",
    "The FedAvg controller implements these main steps:\n",
    "1. The FL server initializes a model using `self.load_model()`.\n",
    "2. For each round (global iteration):\n",
    "    - The FL server samples available clients using `self.sample_clients()`.\n",
    "    - The FL server sends the global model to clients and waits for their updates using `self.send_model_and_wait()`.\n",
    "    - The FL server aggregates all the `results` and produces a new global model using `self.update_model()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a13d5-1130-44e6-8818-70e30de401e6",
   "metadata": {},
   "source": [
    "```python\n",
    "class FedAvg(BaseFedAvg):\n",
    "    def run(self) -> None:\n",
    "        self.info(\"Start FedAvg.\")\n",
    "\n",
    "        model = self.load_model()\n",
    "        model.start_round = self.start_round\n",
    "        model.total_rounds = self.num_rounds\n",
    "\n",
    "        for self.current_round in range(self.start_round, self.start_round + self.num_rounds):\n",
    "            self.info(f\"Round {self.current_round} started.\")\n",
    "            model.current_round = self.current_round\n",
    "\n",
    "            clients = self.sample_clients(self.num_clients)\n",
    "\n",
    "            results = self.send_model_and_wait(targets=clients, data=model)\n",
    "\n",
    "            aggregate_results = self.aggregate(results)\n",
    "\n",
    "            model = self.update_model(model, aggregate_results)\n",
    "\n",
    "            self.save_model(model)\n",
    "\n",
    "        self.info(\"Finished FedAvg.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b6476-089a-4e9d-825b-07107bd5d84a",
   "metadata": {},
   "source": [
    "### Client Code \n",
    "Given a CIFAR10 [PyTorch Lightning](https://lightning.ai/) code example with the network wrapped in a `LitNet` LightningModule class (see [model.py](./model.py)), we will adapt this centralized training code to run in a federated setting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c551053-5460-4d83-8578-796074170342",
   "metadata": {},
   "source": [
    "On the client side, the training workflow is as follows:\n",
    "1. Receive the model from the FL server.\n",
    "2. Perform local training on the received global model and/or evaluate the model for model selection.\n",
    "3. Send the updated model back to the FL server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02bfc2a-783c-494f-9427-c38f40a2e870",
   "metadata": {},
   "source": [
    "NVFlare's Client Lightning API makes this adaptation simple with the `patch()` function:\n",
    "- `flare.patch(trainer)`: Patches the Lightning trainer. After calling `flare.patch()`, methods like `trainer.fit()` and `trainer.validate()` will automatically retrieve the global model from the FL server and send back the updated model after training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115ee07-d848-4a7c-99ad-64e20ab7093c",
   "metadata": {},
   "source": [
    "With this method, developers can adapt their centralized training code for federated learning with these simple changes:\n",
    "```python\n",
    "    # (1) import nvflare lightning client API\n",
    "    import nvflare.client.lightning as flare\n",
    "\n",
    "    # (2) patch the lightning trainer\n",
    "    flare.patch(trainer)\n",
    "\n",
    "    while flare.is_running():\n",
    "        \n",
    "        # Note that we can optionally receive the FLModel from NVFLARE.\n",
    "        # We don't need to pass this input_model to the trainer because after flare.patch(), the trainer.fit()/validate() methods will get the global model internally\n",
    "        input_model = flare.receive()\n",
    "\n",
    "        trainer.validate(...)\n",
    "\n",
    "        trainer.fit(...)\n",
    "\n",
    "        trainer.test(...)\n",
    "\n",
    "        trainer.predict(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67432f44-4144-4347-8d74-e7f57e065a14",
   "metadata": {},
   "source": [
    "The full client training script is saved in [client.py](./client.py), which performs CNN training on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da34414-bac4-4352-8077-ab7ade998eec",
   "metadata": {},
   "source": [
    "## Run an NVFlare Job\n",
    "Now that we have defined the FedAvg controller to run our federated compute workflow on the FL server and our client training script to receive global models, run local training, and send results back to the FL server, we can put everything together using NVFlare's Job Recipe API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cedaf75-3a4a-4843-8017-7716b53149a2",
   "metadata": {},
   "source": [
    "#### 1. Define the Initial Model\n",
    "First, we define the global model used to initialize training on the FL server. See [model.py](./model.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c23d0-afff-4d93-901f-c6b565a0b67e",
   "metadata": {},
   "source": [
    "```python\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LitNet(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = Net()\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES)\n",
    "        self.valid_acc = Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES)\n",
    "        # (optional) pass additional information via self.__fl_meta__\n",
    "        self.__fl_meta__ = {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, labels = batch\n",
    "        outputs = self(x)\n",
    "        loss = criterion(outputs, labels)\n",
    "        self.train_acc(outputs, labels)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"train_acc\", self.train_acc, on_step=True, on_epoch=False)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, labels = batch\n",
    "        outputs = self(x)\n",
    "        loss = criterion(outputs, labels)\n",
    "        self.valid_acc(outputs, labels)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss)\n",
    "            self.log(f\"{stage}_acc\", self.valid_acc, on_step=True, on_epoch=True)\n",
    "        return outputs\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def predict_step(self, batch: Any, batch_idx: int, dataloader_idx: int = 0) -> Any:\n",
    "        return self.evaluate(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.001, momentum=0.9)\n",
    "        return {\"optimizer\": optimizer}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b70da5d-ba8b-4e65-b47f-44bb9bddae4d",
   "metadata": {},
   "source": [
    "#### 2. Job Recipe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21270719-ae55-4769-b052-57f2b3e9a738",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "from nvflare.app_opt.pt.recipes.fedavg import FedAvgRecipe\n",
    "from nvflare.recipe.sim_env import SimEnv\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "DATASET_ROOT = \"/tmp/nvflare/data\"\n",
    "\n",
    "# Download data\n",
    "datasets.CIFAR10(root=DATASET_ROOT, train=True, download=True)\n",
    "datasets.CIFAR10(root=DATASET_ROOT, train=False, download=True)\n",
    "n_clients  = 2\n",
    "num_rounds = 2\n",
    "batch_size = 24\n",
    "\n",
    "recipe = FedAvgRecipe(\n",
    "        min_clients=n_clients,\n",
    "        num_rounds=num_rounds,\n",
    "        initial_model=LitNet(),\n",
    "        train_script=\"client.py\",\n",
    "        train_args=f\"--batch_size {batch_size}\",\n",
    ")\n",
    "\n",
    "env = SimEnv(num_clients=n_clients, num_threads=n_clients)\n",
    "recipe.execute(env=env)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac3f0a8-06bb-4bea-89d3-4a5fc5b76c63",
   "metadata": {},
   "source": [
    "#### 3. Execute Recipe\n",
    "You can execute the same recipe in different environments: `SimEnv`, `PoCEnv`, or `ProdEnv`. Here we will run it in the simulation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13068ab7-35cf-49e7-91ed-10993049ef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python job.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
