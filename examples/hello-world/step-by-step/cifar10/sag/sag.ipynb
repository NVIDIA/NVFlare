{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a554d44-78e0-4078-bf85-bfa2d0551cc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FedAvg algorithm\n",
    "<a id = \"title\"></a>\n",
    "\n",
    "In this example, we will demonstrate the FedAvg algorithm using the CIFAR10 dataset.\n",
    "\n",
    "Both Job life-cycle and training workflow are controlled on the server side; we will just use the existing available SAG controller available in NVFLARE.\n",
    "\n",
    "For client-side training code, we will leverage the new DL to FL Client API.\n",
    "\n",
    "First, let's look at the FedAvg Algorithm and SAG Workflow.\n",
    "\n",
    "\n",
    "## Scatter and Gather (SAG)\n",
    "\n",
    "FLARE's Scatter and Gather workflow is similar to the Message Passing Interface (MPI)'s MPI Broadcast + MPI Gather. [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) is a standardized and portable message-passing standard designed to function on parallel computing architectures. MPI consists of some [collective communication routines](https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/), such as MPI Broadcast, MPI Scatter, and MPI Gather.\n",
    "\n",
    "<img src=\"mpi_scatter.png\" alt=\"scatter\" width=25% height=20% /><img src=\"mpi_gather.png\" alt=\"gather\" width=25% height=20% />\n",
    "\n",
    "\n",
    "\n",
    "## FedAvg with SAG\n",
    "We use [SAG workflow](https://nvflare.readthedocs.io/en/main/programming_guide/controllers/scatter_and_gather_workflow.html) to implement the FedAvg algorithm. You can see one round of training in such workflow.\n",
    "\n",
    "<img src=\"fed_avg_one_round.png\" alt=\"FedAvg\" width=35% height=30% />\n",
    "\n",
    "<a id = \"sag\"></a>\n",
    "<img src=\"fed_avg.png\" alt=\"FedAvg\" width=50% height=45% /> <img src=\"sag.png\" alt=\"Scatter and Gather\" width=40% height=40% />\n",
    "\n",
    "The aggregation of FedAvg is done on the server side, its weighted on the number of training steps on each client\n",
    " \n",
    "## Convert training code to federated learning training code\n",
    "<a id = \"code\"></a>\n",
    "We will use the original [Training a Classifer](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) example\n",
    "from pytorch as the code base.\n",
    "\n",
    "The original code with some clean-up (remove comments etc.) can be found in [here](../code/dl/train.py)\n",
    "\n",
    "\n",
    "With the NVFLARE DL to FL Client APIs, we need to transform the existing pytorch classifer training code into Federated Classifer training code with few lines of code changes. The already converted code can be found in **[here](../code/fl/train.py)**\n",
    "\n",
    "For detailed discussion how to convert training code into federated learning training code using Client API, you can also checked out the examples [here](https://github.com/NVIDIA/NVFlare/blob/main/examples/hello-world/ml-to-fl/README.md)\n",
    "\n",
    "The key changes are the following steps: \n",
    "\n",
    "```\n",
    "    #  import nvflare client API\n",
    "    import nvflare.client as flare\n",
    "\n",
    "    #  initializes NVFlare client API\n",
    "    flare.init()\n",
    "\n",
    "    # gets FLModel from NVFlare\n",
    "    input_model = flare.receive()\n",
    "\n",
    "    # loads model from NVFlare\n",
    "    net.load_state_dict(input_model.params)\n",
    "\n",
    "    # evaluate on received model\n",
    "    accuracy = evaluate(input_model.params)\n",
    "    \n",
    "    # construct trained FL model\n",
    "    output_model = flare.FLModel(\n",
    "        params=net.cpu().state_dict(),\n",
    "        metrics={\"accuracy\": accuracy},\n",
    "        meta={\"NUM_STEPS_CURRENT_ROUND\": steps},\n",
    "    )\n",
    "    \n",
    "    # send model back to NVFlare\n",
    "    flare.send(output_model)\n",
    "```\n",
    "\n",
    "If you are using pytorch-lightning, the changes are much smaller, 1-line import , 1-line change applies to trainer, 1-line global model evaluation. see [cifar10_lightning_examples](https://github.com/NVIDIA/NVFlare/blob/main/examples/hello-world/ml-to-fl/pt/cifar10_lightning_fl.py) \n",
    "# Prepare Data\n",
    "<a id = \"data\"></a>\n",
    "\n",
    "Let's get the data first. Follow the instruction of cifar10, we can download the data with following scripts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e487166-1e47-47db-83cc-f482735b76a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CIFAR10_ROOT = \"/tmp/nvflare/data/cifar10\"\n",
    "\n",
    "! python ../data/download.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667a9be-a911-4485-a3cb-cc2067852545",
   "metadata": {},
   "source": [
    "## Job Folder and Configurations\n",
    "<a id = \"job\"></a>\n",
    "\n",
    "NVFlare needs a job folder to run.\n",
    "We can use NVFLARE Job API to set up the configurations for the server and clients.\n",
    "\n",
    "Let's first copy the required files over:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083d183",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp ../code/fl/train.py train.py\n",
    "! cp ../code/fl/net.py net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c041d",
   "metadata": {},
   "source": [
    "The following code is constructing an FedAvgJob with ScriptRunner with 2 clients, 5 rounds and batch size is 6. Feel free to edit any of these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d4443-a274-4176-a5ac-b36008178d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from net import Net\n",
    "\n",
    "from nvflare.app_opt.pt.job_config.fed_avg import FedAvgJob\n",
    "from nvflare.job_config.script_runner import ScriptRunner\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_clients = 2\n",
    "    num_rounds = 5\n",
    "    train_script = \"train.py\"\n",
    "\n",
    "    job = FedAvgJob(\n",
    "        name=\"cifar10_fedavg\",\n",
    "        n_clients=n_clients,\n",
    "        num_rounds=num_rounds,\n",
    "        initial_model=Net()\n",
    "    )\n",
    "\n",
    "    # Add clients\n",
    "    for i in range(n_clients):\n",
    "        runner = ScriptRunner(\n",
    "            script=train_script, script_args=\"--batch_size 6\"\n",
    "        )\n",
    "        job.to(runner, f\"site-{i+1}\")\n",
    "\n",
    "    job.export_job(\"/tmp/nvflare/jobs\")\n",
    "    job.simulator_run(\"/tmp/nvflare/jobs/workdir\", gpu=\"0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05147756-57b3-4976-bddf-794a1cd0fd4d",
   "metadata": {},
   "source": [
    "## Run Job\n",
    "\n",
    "The previous cell exports the job config and executes the job in NVFlare simulator.\n",
    "\n",
    "If you want to run in production system, you will need to submit this exported job folder to nvflare system.\n",
    "\n",
    "We can check the content of a job folder using tree command or ls -all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178536c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree /tmp/nvflare/jobs/cifar10_fedavg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055bde7-432d-4e6b-9163-b5ab7ede7b73",
   "metadata": {},
   "source": [
    "\n",
    "The next 5 examples will be using the same FedAvg workflow, but will demonstrate different execution APIs and feature.\n",
    "In the next example [sag_deploy_map](../sag_deploy_map/sag_deploy_map.ipynb), we will learn about the deploy_map configuration for deployment of apps to different sites."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
