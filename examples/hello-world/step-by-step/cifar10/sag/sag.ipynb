{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a554d44-78e0-4078-bf85-bfa2d0551cc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FedAvg Algorithm with SAG (Scatter & Gather) workflow\n",
    "<a id = \"title\"></a>\n",
    "\n",
    "In this example, we will demonstrate the SAG workflow with FedAvg using the CIFAR10 dataset.\n",
    "\n",
    "Both Job Lifecycle and training workflow are controlled on the server side; we will just use the existing available SAG controller available in NVFLARE.\n",
    "\n",
    "For client-side training code, we will leverage the new DL to FL Client API.\n",
    "\n",
    "First, let's look at the FedAvg Algorithm and SAG Workflow.\n",
    "\n",
    "\n",
    "## Scatter and Gather (SAG)\n",
    "\n",
    "FLARE's Scatter and Gather workflow is similar to the Message Passing Interface (MPI)'s MPI Broadcast + MPI Gather. [MPI](https://en.wikipedia.org/wiki/Message_Passing_Interface) is a standardized and portable message-passing standard designed to function on parallel computing architectures. MPI consists of some [collective communication routines](https://mpitutorial.com/tutorials/mpi-broadcast-and-collective-communication/), such as MPI Broadcast, MPI Scatter, and MPI Gather.\n",
    "\n",
    "<img src=\"mpi_scatter.png\" alt=\"scatter\" width=25% height=20% /><img src=\"mpi_gather.png\" alt=\"gather\" width=25% height=20% />\n",
    "\n",
    "\n",
    "\n",
    "## FedAvg with SAG\n",
    "We use [SAG workflow](https://nvflare.readthedocs.io/en/main/programming_guide/controllers/scatter_and_gather_workflow.html) to implement the FedAvg algorithm. You can see one round of training in such workflow.\n",
    "\n",
    "<img src=\"fed_avg_one_round.png\" alt=\"FedAvg\" width=35% height=30% />\n",
    "\n",
    "<a id = \"sag\"></a>\n",
    "<img src=\"fed_avg.png\" alt=\"FedAvg\" width=50% height=45% /> <img src=\"sag.png\" alt=\"Scatter and Gather\" width=40% height=40% />\n",
    "\n",
    "The FedAvg aggregation is done on the server side, its weighted on the number of training steps on each client\n",
    " \n",
    "## Convert training code to federated learning training code\n",
    "<a id = \"code\"></a>\n",
    "We will use the original [Training a Classifer](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) example\n",
    "in pytorch as the code base. The cleanup code (remove comments etc.) can be found in [here](../code/dl/train.py)\n",
    "\n",
    "\n",
    "With the NVFLARE DL to FL Client APIs, we need to transform the existing pytorch classifer training code into Federated Classifer training code with few lines of code changes. The already converted code can be found in **[here](../code/fl/train.py)**\n",
    "\n",
    "For detailed discussion how to convert training code into federated learning training code using Client API, you can also checked out the examples [here](https://github.com/NVIDIA/NVFlare/blob/main/examples/hello-world/ml-to-fl/README.md) and code \n",
    "\n",
    "The key changes are the following steps: \n",
    "\n",
    "```\n",
    "    #  import nvflare client API\n",
    "    import nvflare.client as flare\n",
    "\n",
    "    #  initializes NVFlare client API\n",
    "    flare.init()\n",
    "\n",
    "    # gets FLModel from NVFlare\n",
    "    input_model = flare.receive()\n",
    "\n",
    "    # loads model from NVFlare\n",
    "    net.load_state_dict(input_model.params)\n",
    "\n",
    "    # evaluate on received model\n",
    "    accuracy = evaluate(input_model.params)\n",
    "    \n",
    "    # construct trained FL model\n",
    "    output_model = flare.FLModel(\n",
    "        params=net.cpu().state_dict(),\n",
    "        metrics={\"accuracy\": accuracy},\n",
    "        meta={\"NUM_STEPS_CURRENT_ROUND\": steps},\n",
    "    )\n",
    "    \n",
    "    # send model back to NVFlare\n",
    "    flare.send(output_model)\n",
    "```\n",
    "\n",
    "If you are using pytorch-lightning, the changes are much smaller, 1-line import , 1-line change applies to trainer, 1-line global model evaluation. see [cifar10_lightning_examples](https://github.com/NVIDIA/NVFlare/blob/main/examples/hello-world/ml-to-fl/pt/cifar10_lightning_fl.py) \n",
    "# Prepare Data\n",
    "<a id = \"data\"></a>\n",
    "\n",
    "Let's get the data first. Follow the instruction of cifar10, we can download the data with following scripts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e487166-1e47-47db-83cc-f482735b76a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CIFAR10_ROOT = \"/tmp/nvflare/data/cifar10\"\n",
    "\n",
    "! python ../data/download.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6667a9be-a911-4485-a3cb-cc2067852545",
   "metadata": {},
   "source": [
    "## Job Folder and Configurations\n",
    "<a id = \"job\"></a>\n",
    " \n",
    "Now we need to set up the configurations for the server and clients and construct the Job folder NVFLARE needs to run. We can do this using NVFLARE job CLI. You can study the [Job CLI tutorials](https://github.com/NVIDIA/NVFlare/blob/main/examples/tutorials/job_cli.ipynb) later with all the details. But for now, you can just use the following commands to find out the available job templates.\n",
    "\n",
    "We need to set the job templates directory so the job CLI commands can find the job templates. If you have already set `NVFLARE_HOME` to `<NVFLARE git clone directory>`, then you can skip the following step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3d4443-a274-4176-a5ac-b36008178d63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare config -jt ../../../../../job_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7608e860-ba67-4c5c-8219-acf5d52860f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare job list_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c354739-e464-4bca-afbe-eb04baa5751d",
   "metadata": {},
   "source": [
    "* Create job folder and initial configs\n",
    "\n",
    "There are several FedAvg using pytorch job templates. We would like to use the Client API job templates\n",
    "\n",
    "* sag_pt               \n",
    "* sag_pt_deploy_map    \n",
    "* sag_pt_he            \n",
    "* sag_pt_in_proc       \n",
    "* sag_pt_mlflow        \n",
    "\n",
    "\n",
    "Since we like to start with simpliest without worry about MLFlo, homomorphic encryption or other advanced configurations, we now have\n",
    " \n",
    "* sag_pt               \n",
    "* sag_pt_in_proc       \n",
    "\n",
    "both job templates can be used.\n",
    "\n",
    "* **'sag_pt_in_proc'** job template will run the training code in the **same process** as the NVFlare job process. \n",
    "* **'sag_pt'** job template will run the training code in a **different process** as the NVFlare job process. \n",
    "\n",
    "\n",
    "In the current example, we will use **'sag_pt_in_proc'**. \n",
    "\n",
    "Let's create a job folder with this template initially without specifying the code location, just to see what needs to be changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948b39c7-6744-48b4-b73c-47d64c945102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare job create -j /tmp/nvflare/jobs/cifar10_sag_pt -w sag_pt_in_proc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a78d674-5d4f-4d2a-8c70-90e538f86727",
   "metadata": {},
   "source": [
    "Lets also looks at the server and client configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b6688e-1072-45b2-b6e1-cd240807d9ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/jobs/cifar10_sag_pt/app/config/config_fed_server.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174903e1-824d-4a7b-98c4-e190d357a431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/jobs/cifar10_sag_pt/app/config/config_fed_client.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb79b5c-f97f-472f-91a5-5a5175fb9759",
   "metadata": {},
   "source": [
    "* Create a job folder with all the configurations.\n",
    "\n",
    "Let's change the `num_rounds` to 5, `script` to `train.py`, and `min_clients` to 2 in `meta.conf`. We also want to change the arguments for `train.py`: `dataset_path=CIFAR10_ROOT`, `batch_size=6`, `num_workers=2`. Note that the `dataset_path` is not actually changed, but we just want to show you that it could be changed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10603e48-ef80-46fc-9163-e287ce43c803",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare job create -j /tmp/nvflare/jobs/cifar10_sag_pt -w sag_pt_in_proc \\\n",
    "-f meta.conf min_clients=2 \\\n",
    "-f config_fed_client.conf app_script=train_with_mlflow.py app_config=\"--batch_size 4 --dataset_path {CIFAR10_ROOT} --num_workers 2\" \\\n",
    "-f config_fed_server.conf num_rounds=2 \\\n",
    "-sd ../code/fl \\\n",
    "-force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2208467c-8cf7-4446-963e-af92f79bfad5",
   "metadata": {},
   "source": [
    "OK, we are ready to run the job, let's look at the job folder, use \"ls -al\" if you don't have \"tree\" installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf68fbb-eefc-4268-b341-2558dfe35c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tree /tmp/nvflare/jobs/cifar10_sag_pt  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05147756-57b3-4976-bddf-794a1cd0fd4d",
   "metadata": {},
   "source": [
    "## Run Job\n",
    "We can use simulator to run the job directly. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bd4d00-810f-416c-a2e1-e469a9697e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare simulator /tmp/nvflare/jobs/cifar10_sag_pt  -w /tmp/nvflare/jobs/cifar10_sag_pt_workspace -t 2 -n 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b055bde7-432d-4e6b-9163-b5ab7ede7b73",
   "metadata": {},
   "source": [
    "The job should be running in the simulator mode. We are done with the training. \n",
    "\n",
    "The next 5 examples will use the same ScatterAndGather workflow, but will demonstrate different execution APIs and feature.\n",
    "In the next example [sag_deploy_map](../sag_deploy_map/sag_deploy_map.ipynb), we will learn about the deploy_map configuration for deployment of apps to different sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b430b-a65b-4b1e-8793-9b3befcfcfd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tree  /tmp/nvflare/jobs/cifar10_sag_pt_workspace/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50594df7-b4c9-4e5e-944a-403b5a105c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mlflow ui --port 5000 --backend-store-uri /tmp/nvflare/jobs/cifar10_sag_pt_workspace/server/simulate_job/mlruns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2b6628-61af-4bc8-84d4-a9876a27c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard  --logdir=/tmp/nvflare/jobs/cifar10_sag_pt_workspace/server/simulate_job/tb_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ad11c3-6ef7-46cd-8778-0090505b14e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
