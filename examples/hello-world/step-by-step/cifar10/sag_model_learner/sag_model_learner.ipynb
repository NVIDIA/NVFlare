{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514c47e2-420d-4af4-9bf0-cac337c51c39",
   "metadata": {},
   "source": [
    "# FedAvg with SAG workflow using Model Learner\n",
    "\n",
    "In this example, we will demonstrate the FegAvg SAG workflow using the CIFAR10 dataset using the ModelLearner API. \n",
    "\n",
    "While the previous example [FedAvg with SAG workflow](../sag/sag.ipynb#title) utilized the Client API, here we will demonstrate how to convert the original training code into a ModelLearner trainer, showcase its capabilities, and recommend the best use cases.\n",
    "\n",
    "For an overview on Federated Averaging and SAG, see the section from the previous example: [Understanding FedAvg and SAG](../sag/sag.ipynb#sag)\n",
    "\n",
    "## ModelLearner\n",
    "\n",
    "The main goal of the ModelLearner is to make it easier to write learning logic by minimizing FLARE specific concepts that the user is exposed to. The ModelLearner defines familiar learning functions for training and validation, and uses the FLModel object for transferring learning information.\n",
    "\n",
    "Key Concepts:\n",
    "- Learning\n",
    "    - `FLModel` object defines structure to contain essential information about the learning task, such as `params`, `metrics`, `meta`, etc.\n",
    "    - learning logic implemented in `train()` and `validate` methods, which both receive and send an `FLModel` object\n",
    "    - return requested model via `get_model()`\n",
    "- Lifecycle\n",
    "    - `initialize` for logic before learning job start and `finalize` for once learning job is finished\n",
    "    - abort gracefully with `abort()` or `is_aborted()`\n",
    "- Convenience \n",
    "    - various logging methods such as `info`, `debug`, `error`, etc.\n",
    "    - contextual information availabled in learner\n",
    "\n",
    "\n",
    "Here are the full definitions of the APIs for the [ModelLearner](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/abstract/model_learner.py) and [FLModel](https://github.com/NVIDIA/NVFlare/blob/main/nvflare/app_common/abstract/fl_model.py).\n",
    "\n",
    "### When to use ModelLearner\n",
    "\n",
    "The ModelLearner is best used when working with standard machine learning code that can fit well into the train and validate methods and can be easily adapated to the ModelLearner structure. This allows for the separation of FLARE specific communication constructs from the machine learning specific tasks, and provides the FLModel object for data transfer. \n",
    "\n",
    "On the otherhand, if the user would rather not adapt the code structure, we recommend using the [Client API](https://github.com/NVIDIA/NVFlare/blob/main/examples/hello-world/ml-to-fl/README.md) for even simpler conversion to FL code at the cost of losing some convenience functionalities.\n",
    "\n",
    "Finally, if the user wishes to implement something more specific that is not supported by the ModelLearner, we recommend writing an Executor which gives greater freedom for defining logic and tasks. The main tradeoff is this requires the use of more FLARE concepts such as FLContext, Shareable, DXO, etc.\n",
    "\n",
    "\n",
    "## Converting DL training code to FL ModelLearner training code\n",
    "We will use the original [Training a Classifer](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) example\n",
    "in PyTorch as our base [DL code](../code/dl/train.py).\n",
    "\n",
    "With the FLARE ModelLearner API, we need to transform the existing PyTorch classifer into a Federated classifer by restructuring our code to subclass ModelLearner, and implementing the required methods. The converted code can be found at [FL ModelLearner code](../code/fl/model_learner.py).\n",
    "\n",
    "Key Changes:\n",
    "- Subclass ModelLearner with appropriate init args\n",
    "- Encapsulate the original DL train and validate code inside `local_train()` and `local_validate()` and the dataset and PyTorch training utilities in `initialize()`\n",
    "- Implement the `train()` and `validate()` methods by wrapping the local learning methods and processing and returning `FLModel`\n",
    "- Implement `get_model()` method to load and return best local model, so it can then be sent to other sites for validation (via the cross-site evaluation workflow)\n",
    "\n",
    "```\n",
    "def get_model(self, model_name: str) -> Union[str, FLModel]:\n",
    "    # Retrieve the best local model saved during training.\n",
    "    if model_name == ModelName.BEST_MODEL:\n",
    "        try:\n",
    "            model_data = torch.load(self.model_path, map_location=\"cpu\")\n",
    "            np_model_data = {k: v.cpu().numpy() for k, v in model_data.items()}\n",
    "\n",
    "            return FLModel(params_type=ParamsType.FULL, params=np_model_data)\n",
    "        except Exception as e:\n",
    "            raise ValueError(\"Unable to load best model\") from e\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_type: {model_name}\")  # Raised errors are caught in LearnerExecutor class.\n",
    "\n",
    "def train(self, model: FLModel) -> Union[str, FLModel]:\n",
    "    self.info(f\"Current/Total Round: {self.current_round + 1}/{self.total_rounds}\")\n",
    "    self.info(f\"Client identity: {self.site_name}\")\n",
    "\n",
    "    pt_input_params = {k: torch.as_tensor(v) for k, v in model.params.items()}\n",
    "    self._local_train(pt_input_params)\n",
    "\n",
    "    pt_output_params = {k: torch.as_tensor(v) for k, v in self.net.cpu().state_dict().items()}\n",
    "    accuracy = self._local_validate(pt_output_params)\n",
    "\n",
    "    if accuracy > self.best_acc:\n",
    "        self.best_acc = accuracy\n",
    "        torch.save(self.net.state_dict(), self.model_path)\n",
    "\n",
    "    np_output_params = {k: v.cpu().numpy() for k, v in self.net.cpu().state_dict().items()}\n",
    "    return FLModel(\n",
    "        params=np_output_params,\n",
    "        metrics={\"accuracy\": accuracy},\n",
    "        meta={\"NUM_STEPS_CURRENT_ROUND\": 2 * len(self.trainloader)},\n",
    "    )\n",
    "\n",
    "def validate(self, model: FLModel) -> Union[str, FLModel]:\n",
    "    pt_params = {k: torch.as_tensor(v) for k, v in model.params.items()}\n",
    "    val_accuracy = self._local_validate(pt_params)\n",
    "\n",
    "    return FLModel(metrics={\"val_accuracy\": val_accuracy})\n",
    "\n",
    "...\n",
    "    \n",
    "```\n",
    "\n",
    "## Job Configuration\n",
    "\n",
    "Now we must install the ModelLearner to the training client. We use the predefined `ModelLearnerExecutor`, which handles setting up the Learner and executing the tasks using the ModelLearner methods. In the client configuration, the `learner_id` of the `ModelLearnerExecutor` is mapped to the `id` of the ModelLearner trainer component that we implemented.\n",
    "\n",
    "Let's use the Job CLI to create the job from a ModelLearner template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de430380",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvflare job create -j /tmp/nvflare/jobs/sag_pt_model_learner -w sag_pt_model_learner -sd ../code/fl -force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8e88f",
   "metadata": {},
   "source": [
    "We can take a look at the server and client configurations and make any changes as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /tmp/nvflare/jobs/sag_pt_model_learner/app/config/config_fed_server.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /tmp/nvflare/jobs/sag_pt_model_learner/app/config/config_fed_client.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf82dac",
   "metadata": {},
   "source": [
    "Ensure that our ModelLearner trainer code is correctly installed with the ModelLearnerExecutor. Also since the ModelLearnerExecutor supports the train, validate, and submit_model tasks, we can use the CrossSiteModelEval workflow (see [CSE example](../cse/cse.ipynb)) in the server configuration in addition to the ScatterAndGather workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc8869",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f63bf0f",
   "metadata": {},
   "source": [
    "Make sure the CIFAR10 dataset is downloaded with the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17323f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../data/download.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f3c9f-8185-47d3-8658-40f7b16699c5",
   "metadata": {},
   "source": [
    "## Run the Job\n",
    "\n",
    "Now we can run the job with the simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70738539-3df6-4779-831f-0a1375d6aabf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare simulator /tmp/nvflare/jobs/sag_pt_model_learner -w /tmp/nvflare/sag_pt_model_learner -t 2 -n 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48271064",
   "metadata": {},
   "source": [
    "As an additional resource, also see the [CIFAR10 examples](../../../../advanced/cifar10/README.md) for a comprehensive implementation of a PyTorch ModelLearner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef3134",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
