{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514c47e2-420d-4af4-9bf0-cac337c51c39",
   "metadata": {},
   "source": [
    "# Cross-Site Evaluation (with SAG)\n",
    "\n",
    "In this example, we will demonstrate the Cross-Site Evaluation workflow using the Client API and the CIFAR10 dataset. In order to first produce models to perform cross-evaluation with, we run the [SAG workflow](../sag/sag.ipynb) beforehand.\n",
    "\n",
    "## Cross-Site Evaluation Workflow\n",
    "\n",
    "<img src=\"figs/cse.png\" alt=\"cse\" width=35% height=35% />\n",
    "\n",
    "(Note: the diagram above illustrates evaluation on client-1's model, however in the workflow all participating clients will have their models evaluated by all other participating clients)\n",
    "\n",
    "The `CrossSiteModelEval` workflow uses the data from clients to run evaluation with the models of other clients. Data is not shared, rather the collection of models is distributed by the server to each client site to run local validation. The server’s global model is also distributed to each client for evaluation on the client’s local dataset for global model evaluation. Finally, validation results are collected by the server to construct an all-to-all matrix of model performance vs. client dataset, and the `ValidationJsonGenerator` is used to write the results to a JSON file on the server.\n",
    "\n",
    "Required tasks: \n",
    "- `validate` to perform validation on model using local dataset\n",
    "- `submit_model` to obtain the client model for validation\n",
    "\n",
    "## Converting DL training code to FL training code with Multi-Task Support\n",
    "<a id = \"code\"></a>\n",
    "\n",
    "We will be using the [Client API FL code](../code/fl/train.py) trainer converted from the original [Training a Classifer](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) example.\n",
    "\n",
    "Key changes when writing a FL code to support multiple tasks:\n",
    "- When using the default `launch_once` parameter of `SubprocessLauncher`, we encapsulate our code in `while flare.is_running():` loop so we can call `flare.receive()` and perform various tasks. This is useful when launching everytime would be inefficient, such as when having to perform data setup every time.\n",
    "- We use `flare.is_train()`, `flare.is_evaluate()`, and `flare.is_submit_model()` for implementing the `train`, `validate`, and `submit_model` tasks depending on the mode.\n",
    "\n",
    "```\n",
    "    # (3) run continously when launch_once=true\n",
    "    while flare.is_running():\n",
    "\n",
    "        # (4) receive FLModel from NVFlare\n",
    "        input_model = flare.receive()\n",
    "\n",
    "        # (5) performing train task on received model\n",
    "        if flare.is_train():\n",
    "            ...\n",
    "        # (6) performing evaluate task on received model\n",
    "        elif flare.is_evaluate():\n",
    "            ...\n",
    "        # (7) performing submit_model task to obtain best local model\n",
    "        elif flare.is_submit_model():\n",
    "            ...\n",
    "```\n",
    "\n",
    "See [Converting to FL code using Client API](../sag/sag.ipynb#code) for more details on using the Client API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc8869",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f63bf0f",
   "metadata": {},
   "source": [
    "Make sure the CIFAR10 dataset is downloaded with the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17323f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../data/download.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0995ed8",
   "metadata": {},
   "source": [
    "## Job Configuration\n",
    "\n",
    "Now we must configure our client api trainer along with the Server-Controlled Cross-site Evaluation workflows.\n",
    "\n",
    "The client configuration for the trainer with the Client API is standard with the PTClientAPILauncherExecutor, SubprocessLauncher, and our defined app script that supports the `train`, `validate`, and `submit_model` tasks. \n",
    "\n",
    "In the server configuration, after `ScatterAndGather` we add the `CrossSiteModelEval` workflow, which uses the `validate` and `submit_model` tasks and requires a model locator. Under components the `PTFileModelLocator` is used to locate the models inventory saved during training, and an optional `IntimeModelSelector` is used to select the best global model to save based on the validation scores from the clients. Finally, the `ValidationJsonGenerator` generates `cross_val_results.json` which contains the accuracy of each validate model.\n",
    "\n",
    "Let's copy the required files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e991613",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "! cp ../code/fl/train.py train.py\n",
    "! cp ../code/fl/net.py net.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f73c909",
   "metadata": {},
   "source": [
    "Let's use the Job API to create a job and run using the simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f50029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import Net\n",
    "from nvflare import FedJob\n",
    "from nvflare.app_common.workflows.fedavg import FedAvg\n",
    "from nvflare.app_common.workflows.cross_site_eval import CrossSiteEval\n",
    "from nvflare.app_opt.pt.job_config.model import PTModel\n",
    "from nvflare.job_config.script_runner import FrameworkType, ScriptRunner\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    n_clients = 2\n",
    "    num_rounds = 1\n",
    "    train_script = \"train.py\"\n",
    "\n",
    "    job = FedJob(name=\"cse\")\n",
    "    \n",
    "    # Define the initial global model and send to server\n",
    "    comp_ids = job.to(PTModel(Net()), \"server\")\n",
    "    \n",
    "    # Define the controller workflow and send to server\n",
    "    controller = FedAvg(\n",
    "        num_clients=n_clients,\n",
    "        num_rounds=num_rounds,\n",
    "        persistor_id=comp_ids[\"persistor_id\"]\n",
    "    )\n",
    "    job.to(controller, \"server\")\n",
    "\n",
    "    # Define the controller workflow and send to server\n",
    "    controller = CrossSiteEval(\n",
    "        persistor_id=comp_ids[\"persistor_id\"]\n",
    "    )\n",
    "    job.to(controller, \"server\")\n",
    "\n",
    "    # Add clients\n",
    "    for i in range(n_clients):\n",
    "        runner = ScriptRunner(\n",
    "            script=train_script,\n",
    "            script_args=\"--local_epochs 1 --batch_size 32\",\n",
    "            launch_external_process=True            \n",
    "        )\n",
    "        job.to(runner, f\"site-{i+1}\")\n",
    "\n",
    "    job.export_job(\"/tmp/nvflare/jobs\")\n",
    "    job.simulator_run(\"/tmp/nvflare/jobs/workdir\", gpu=\"0\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f3c9f-8185-47d3-8658-40f7b16699c5",
   "metadata": {},
   "source": [
    "## Run Job\n",
    "\n",
    "The previous cell exports the job config and executes the job in NVFlare simulator.\n",
    "\n",
    "If you want to run in production system, you will need to submit this exported job folder to nvflare system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af985f4",
   "metadata": {},
   "source": [
    "To view the validation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /tmp/nvflare/jobs/workdir/server/simulate_job/cross_site_val/cross_val_results.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48271064",
   "metadata": {},
   "source": [
    "For additional resources, see other examples for SAG with CSE using the [Executor](../sag_executor/sag_executor.ipynb).\n",
    "\n",
    "[Hello-Numpy](https://github.com/NVIDIA/NVFlare/tree/main/examples/hello-world/hello-numpy-cross-val) also demonstrates how to run cross-site evaluation using the previous training results.\n",
    "\n",
    "Next we will look at the [cyclic](../cyclic/cyclic.ipynb) example, which shows the cyclic workflow for the Cyclic Weight Transfer algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef3134",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
