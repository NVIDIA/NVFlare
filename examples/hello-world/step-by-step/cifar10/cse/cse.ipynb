{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514c47e2-420d-4af4-9bf0-cac337c51c39",
   "metadata": {},
   "source": [
    "# Cross-Site Evaluation (with SAG)\n",
    "\n",
    "In this example, we will demonstrate the Cross-Site Evaluation workflow using the Client API and the CIFAR10 dataset. In order to first produce models to perform cross-evaluation with, we run the [SAG workflow](../sag/sag.ipynb) beforehand.\n",
    "\n",
    "## Cross-Site Evaluation Workflow\n",
    "\n",
    "<img src=\"figs/cse.png\" alt=\"cse\" width=35% height=35% />\n",
    "\n",
    "(Note: the diagram above illustrates evaluation on client-1's model, however in the workflow all participating clients will have their models evaluated by all other participating clients)\n",
    "\n",
    "The `CrossSiteModelEval` workflow uses the data from clients to run evaluation with the models of other clients. Data is not shared, rather the collection of models is distributed by the server to each client site to run local validation. The server’s global model is also distributed to each client for evaluation on the client’s local dataset for global model evaluation. Finally, validation results are collected by the server to construct an all-to-all matrix of model performance vs. client dataset, and the `ValidationJsonGenerator` is used to write the results to a JSON file on the server.\n",
    "\n",
    "Required tasks: \n",
    "- `validate` to perform validation on model using local dataset\n",
    "- `submit_model` to obtain the client model for validation\n",
    "\n",
    "## Converting DL training code to FL training code with Multi-Task Support\n",
    "<a id = \"code\"></a>\n",
    "\n",
    "We will be using the [Client API FL code](../code/fl/train.py) trainer converted from the original [Training a Classifer](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) example.\n",
    "\n",
    "Key changes when writing a FL code to support multiple tasks:\n",
    "- When using the default `launch_once` parameter of `SubprocessLauncher`, we encapsulate our code in `while flare.is_running():` loop so we can call `flare.receive()` and perform various tasks. This is useful when launching everytime would be inefficient, such as when having to perform data setup every time.\n",
    "- We use `flare.is_train()`, `flare.is_evaluate()`, and `flare.is_submit_model()` for implementing the `train`, `validate`, and `submit_model` tasks depending on the mode.\n",
    "\n",
    "```\n",
    "    # (3) run continously when launch_once=true\n",
    "    while flare.is_running():\n",
    "\n",
    "        # (4) receive FLModel from NVFlare\n",
    "        input_model = flare.receive()\n",
    "\n",
    "        # (5) performing train task on received model\n",
    "        if flare.is_train():\n",
    "            ...\n",
    "        # (6) performing evaluate task on received model\n",
    "        elif flare.is_evaluate():\n",
    "            ...\n",
    "        # (7) performing submit_model task to obtain best local model\n",
    "        elif flare.is_submit_model():\n",
    "            ...\n",
    "```\n",
    "\n",
    "See [Converting to FL code using Client API](../sag/sag.ipynb#code) for more details on using the Client API.\n",
    "\n",
    "## Job Configuration\n",
    "\n",
    "Now we must configure our client api trainer along with the Server-Controlled Cross-site Evaluation workflows.\n",
    "\n",
    "The client configuration for the trainer with the Client API is standard with the PTClientAPILauncherExecutor, SubprocessLauncher, and our defined app script that supports the `train`, `validate`, and `submit_model` tasks. \n",
    "\n",
    "In the server configuration, after `ScatterAndGather` we add the `CrossSiteModelEval` workflow, which uses the `validate` and `submit_model` tasks and requires a model locator. Under components the `PTFileModelLocator` is used to locate the models inventory saved during training, and an optional `IntimeModelSelector` is used to select the best global model to save based on the validation scores from the clients. Finally, the `ValidationJsonGenerator` generates `cross_val_results.json` which contains the accuracy of each validate model.\n",
    "\n",
    "Let's use the Job CLI to create the job from a SAG and Cross-site Evaluation Client API template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0383e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvflare config -jt ../../../../../job_templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de430380",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvflare job create -j /tmp/nvflare/jobs/sag_cse_pt -w sag_cse_pt -sd ../code/fl -force"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd8e88f",
   "metadata": {},
   "source": [
    "We can take a look at the server and client configurations and make any changes as desired:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /tmp/nvflare/jobs/sag_cse_pt/app/config/config_fed_server.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d223847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /tmp/nvflare/jobs/sag_cse_pt/app/config/config_fed_client.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cc8869",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f63bf0f",
   "metadata": {},
   "source": [
    "Make sure the CIFAR10 dataset is downloaded with the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17323f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python ../data/download.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f3c9f-8185-47d3-8658-40f7b16699c5",
   "metadata": {},
   "source": [
    "## Run the Job\n",
    "\n",
    "Now we can run the job with the simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70738539-3df6-4779-831f-0a1375d6aabf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare simulator /tmp/nvflare/jobs/sag_cse_pt -w /tmp/nvflare/sag_cse_pt_workspace -t 2 -n 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af985f4",
   "metadata": {},
   "source": [
    "To view the validation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2dc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cat /tmp/nvflare/sag_cse_pt_workspace/simulate_job/cross_site_val/cross_val_results.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48271064",
   "metadata": {},
   "source": [
    "For additional resources, see other examples for SAG with CSE using the [ModelLearner](../sag_model_learner/sag_model_learner.ipynb), [Executor](../sag_executor/sag_executor.ipynb), and [Hello-Numpy](https://github.com/NVIDIA/NVFlare/tree/main/examples/hello-world/hello-numpy-cross-val).\n",
    "\n",
    "Also the ability to run Cross-site Evaluation without having to re-run training will be added in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bef3134",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
