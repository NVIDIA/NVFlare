{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cb3afa",
   "metadata": {},
   "source": [
    "# Calculate CIFAR10 Image Histogram\n",
    "\n",
    "Before one training the image classifer, the pytorch example follows the following steps: \n",
    "\n",
    "* **Prepare Data**\n",
    "    * Load and normalize the CIFAR10 training and test datasets using torchvision\n",
    "    \n",
    "* **Training**\n",
    "    * Define a Convolutional Neural Network\n",
    "    * Define a loss function\n",
    "    * Train the network on the training data\n",
    "    * Test the network on the test data\n",
    "    \n",
    "We will add another step to calculate the data historgram and compare the local (site) histogram and global historgrams. So the above steps become\n",
    "\n",
    "\n",
    "* **Prepare Data**\n",
    "    * Load and normalize the CIFAR10 training and test datasets using torchvision\n",
    "\n",
    "* **Data Statistics**\n",
    "    * Calculate data stastics: image intensity histograms\n",
    "    \n",
    "* **Training**\n",
    "    * Define a Convolutional Neural Network\n",
    "    * Define a loss function\n",
    "    * Train the network on the training data\n",
    "    * Test the network on the test data\n",
    "\n",
    "\n",
    "\n",
    "## Setup NVFLARE\n",
    "\n",
    "Follow [Getting Started](https://nvflare.readthedocs.io/en/main/getting_started.html) to set up a virtual environment and install NVFLARE.\n",
    "\n",
    "You can also follow this [notebook](https://github.com/NVIDIA/NVFlare/blob/main/examples/nvflare_setup.ipynb) to get set up.\n",
    "\n",
    "> Make sure you have installed nvflare from **terminal** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a17f22-5667-4f99-b4f6-d49116db74b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install requirements\n",
    "assuming the current directory is 'cifar10/stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92caefd5-9438-46b1-b46f-12b929ff11a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8969bf-d010-42b5-a807-0808922402d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065b351-baac-4f84-aa15-3d875f86cb93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Data\n",
    "\n",
    "Generally, when you have to deal with image, text, audio or video data, you can use standard python packages that load data into a numpy array. Then you can convert this array into a torch.*Tensor. Torch provied a package called torchvision, that has data loaders for common datasets such as ImageNet, CIFAR10, MNIST, etc. and data transformers for images, viz., torchvision.datasets and torch.utils.data.DataLoader.\n",
    "\n",
    "For CIFAR10 dataset, it has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. \n",
    "The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "![cifar10](../data/cifar10.png)\n",
    "\n",
    "\n",
    "The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1].\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e64769-17f1-4805-9399-1c141e050065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "CIFAR10_ROOT = \"/tmp/nvflare/data/cifar10\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 6\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562f713-5892-43c7-a3d6-d277c337b5ea",
   "metadata": {},
   "source": [
    "Once you have extract the data from zip file, you can check the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc68ebf-6071-479d-8cc1-15439bedea02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls -al {CIFAR10_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a2400-6c8c-4a19-af1b-3cc8ad526a77",
   "metadata": {},
   "source": [
    "Lets explore the data, first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c68fd-12a8-40f4-8794-55bcde83606d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
    "\n",
    "# dimension and shapes\n",
    " \n",
    "# Display image and label.\n",
    "print(f\"\\nFeature batch shape: {images.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()} \\n\")\n",
    "\n",
    "\n",
    "print(\"train datasize =\", len(trainset))\n",
    "print(\"test datasize =\", len(testset))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498d3c6-1e6e-4501-a013-a2430f8d2d24",
   "metadata": {},
   "source": [
    "We can see the images has shape of [batch, channel, rows, cols] = [6,3,32,32]\n",
    "\n",
    "## Download data in script\n",
    "We have prepared python script to download the data as well. \n",
    "```\n",
    "python ../data/download.py  --dataset_path <data_path>\n",
    "```\n",
    "if dataset_path is not specified, it default to CIFAR10_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19223a-25bf-42c3-8109-2aaf80b298f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python ../data/download.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda6b93-1723-4ba5-a123-f7a782bde545",
   "metadata": {},
   "source": [
    "## Create Local Image Intensity Histogram Calculator\n",
    "\n",
    "We ignored all other statistics calculations (mean, stddev etc. as they don't apply). all methods have default implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfefa1-7ad7-4602-8370-2c621dd8eb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "from nvflare.apis.fl_context import FLContext\n",
    "from nvflare.app_common.abstract.statistics_spec import Bin, DataType, Feature, Histogram, HistogramType, Statistics\n",
    " \n",
    "# the dataset path    \n",
    "CIFAR10_ROOT = \"/tmp/nvflare/data/cifar10\"\n",
    "\n",
    "\n",
    "class ImageStatistics(Statistics):\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_root: str = CIFAR10_ROOT, \n",
    "                 batch_size: int = 4):\n",
    "        \"\"\"local image intensity calculator.\n",
    "\n",
    "        Args:\n",
    "            dataset_path: directory with local image data.\n",
    "         Returns:\n",
    "            Histogram of local statistics`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset_path = data_root\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # there are three color channels : RGB, each corresponding to each channel index\n",
    "        # we are going treat each channel as one feature, the feature Ids are corresponding to tensor channel index. \n",
    "        # The feature name is named \"red\", \"gree\", \"blue\" (RGB). \n",
    "        \n",
    "        self.features_ids = { \"red\": 0, \"green\": 1,\"blue\": 2}\n",
    "        self.image_features  = [Feature(\"red\", DataType.FLOAT),\n",
    "                                Feature(\"green\", DataType.FLOAT),\n",
    "                                Feature(\"blue\", DataType.FLOAT)]\n",
    "        self.dataset_lengths = {}\n",
    "        self.loaders = {}\n",
    "\n",
    "        self.client_name = None\n",
    "        self.fl_ctx = None\n",
    "\n",
    "\n",
    "\n",
    "    def initialize(self, fl_ctx: FLContext):\n",
    "\n",
    "        # FLContext is context information for the client side NVFLARE engine. \n",
    "        # it includes many runtime information. \n",
    "        # Here we only interested in client site name. \n",
    "        # fl_ctx.get_identity_name() will return the client's name\n",
    "        \n",
    "        self.fl_ctx = fl_ctx\n",
    "        self.client_name = \"local_client\" if fl_ctx is None else fl_ctx.get_identity_name()\n",
    "        \n",
    "        transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=self.dataset_path, train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.CIFAR10(root=self.dataset_path, train=False,  download=True, transform=transform)\n",
    "        self.dataset_lengths = {\"train\": len(trainset), \"test\":len(testset)}\n",
    "        \n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "        testloader = torch.utils.data.DataLoader(testset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        self.loaders = {\"train\": trainloader, \"test\": testloader}\n",
    "\n",
    "    def features(self) -> Dict[str, List[Feature]]:\n",
    "        return {\"train\": self.image_features, \n",
    "                \"test\":  self.image_features}\n",
    "\n",
    "    def count(self, dataset_name: str, feature_name: str) -> int:\n",
    "        return self.dataset_lengths[dataset_name]\n",
    "        \n",
    " \n",
    "    def histogram(self,\n",
    "                  dataset_name: str,\n",
    "                  feature_name: str, \n",
    "                  num_of_bins: int, \n",
    "                  global_min_value: float, \n",
    "                  global_max_value: float) -> Histogram:\n",
    "     \n",
    "        print(f\"calculating image intensity histogram for client {self.client_name}\")\n",
    "        channel = self.features_ids[feature_name]\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        histogram_bins: List[Bin] = []\n",
    "        bin_edges = []\n",
    "        histogram = np.zeros(num_of_bins, dtype=float)\n",
    "\n",
    "        for inputs, _ in self.loaders[dataset_name]:\n",
    "            for img in inputs:\n",
    "                counts, bin_edges = np.histogram(img[channel, : , :],\n",
    "                                                 bins=num_of_bins,\n",
    "                                                 range=(global_min_value, global_max_value))\n",
    "                histogram += counts\n",
    "\n",
    "        for i in range(num_of_bins):\n",
    "            low_value = bin_edges[i]\n",
    "            high_value = bin_edges[i + 1]\n",
    "            bin_sample_count = histogram[i]\n",
    "            histogram_bins.append(Bin(low_value=low_value, high_value=high_value, sample_count=bin_sample_count))\n",
    "\n",
    "        return Histogram(HistogramType.STANDARD, histogram_bins)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7cfbf9-35cd-40d3-b1fd-851edff06f70",
   "metadata": {},
   "source": [
    "Let's test if the code works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499fac50-35b5-43ba-bc05-cc4a01c3af55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_cal = ImageStatistics()\n",
    "\n",
    "hist_cal.initialize(fl_ctx = None)\n",
    "features = hist_cal.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458722d-08c8-4d7f-b7fb-cd063a991c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_cal.count(\"train\", \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed64ae6-6887-4e84-88e1-7647b42d5fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_cal.histogram(\"train\", \"red\", 20, 0, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38287e-c425-4c31-8760-f10e3a1edc8b",
   "metadata": {},
   "source": [
    "The code is working. Let's setup NVFLARE job in federated computing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6becb401-c4f1-4ad3-b30a-169cc9b35525",
   "metadata": {},
   "source": [
    "## Create Federated Histogram Job\n",
    "We are going to use NVFLARE job cli to create job. For detailed instructions on Job CLI, please follow the [job cli tutorial](https://github.com/NVIDIA/NVFlare/blob/main/examples/tutorials/job_cli.ipynb).\n",
    "\n",
    "Let's check the available job templates, we are going to use one of the existing job template and modify to fit our needs. \n",
    "The job template is nothing but server and client-side job configurations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1daf5-f115-4910-933a-1fa148b1b13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare job list_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed966234-9f97-4ad9-9a86-6b19371595a7",
   "metadata": {},
   "source": [
    "there is \"stats_image\" job template, which what we need. We are going to use that. Now, use ```nvflare job create``` command\n",
    "Let's try using the default values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a361a4b-745d-49d4-9fcb-1125c4d6ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvflare job create -w stats_image -j /tmp/nvflare/jobs/stats_image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff1da2e-3151-4e69-b266-d387b460a2e8",
   "metadata": {},
   "source": [
    "The default seems to be ok, execept for few values: \n",
    "* data_root=/tmp/nvflare/image_stats/data, it not the same as CIFAR10_ROOT\n",
    "* meta.conf's min_clients should be 2, as we are going to use two clients\n",
    "\n",
    "Now let's look closer at the configurations, in particular, the client side, and make sure it matches to the new class we just created"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379cbf3b-b058-4b5a-b3f8-aa6824f0cf2d",
   "metadata": {},
   "source": [
    ">Note: \n",
    "In the upcoming sections, we'll utilize the 'tree' command. To install this command on a Linux system, you can use the sudo apt install tree command. As an alternative to 'tree', you can use the ls -al command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de9b6f-35eb-4491-9bce-412fc3025e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tree /tmp/nvflare/jobs/stats_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fb3651-76cd-4955-945b-ccfbe29d0774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare job create -w stats_image -j /tmp/nvflare/jobs/stats_image -force \\\n",
    "-f meta.conf min_clients=2 \\\n",
    "-f config_fed_client.conf app_script=image_statistics.py data_root={CIFAR10_ROOT} \\\n",
    "-f config_fed_server.conf bins=20  \\\n",
    "-sd ./ \\\n",
    "-debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e1a05-200f-4e7b-bc57-1fa8a7742842",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tree /tmp/nvflare/jobs/stats_image "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaa3e33-0c41-4d5f-875a-3bef85aaae79",
   "metadata": {},
   "source": [
    "We can see we also copied some files doesn't belong there. Let's remove them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09aed14-5011-4418-8840-5f7c16c97534",
   "metadata": {
    "tags": []
   },
   "source": [
    "> **Note**: if your histogram calculator file name is not \"image_statistics.py\" but other file name such as \"image_histogram.py\" and the class name is not 'ImageStatistics' but \"HistogramStatistics\", you will need to manually edit the config_fed_client.conf to replace 'image_statistics.ImageStatistics' with 'image_histogram.HistogramStatistics'. Any other adjustments that your need to change, you can directly edit the server and client configurations\n",
    "\n",
    "\n",
    "Now we have created the job folder. we are ready to run job\n",
    "\n",
    "## Run Job in FL Simulator\n",
    "\n",
    "**Run Job using Simulator CLI**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc008c-2447-4d4a-9911-4c1fb1c28aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare simulator /tmp/nvflare/jobs/stats_image -w /tmp/nvflare/image_stats -n 2 -t 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf6e9a-3265-4e45-8b06-c8e543605f21",
   "metadata": {},
   "source": [
    "\n",
    "**Examine the result**\n",
    "\n",
    "Notice the result is written at \n",
    "\n",
    "**/tmp/nvflare/image_stats/simulate_job/statistics/image_statistics.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a7dd0-45d9-42ea-98b2-f72a3bbccf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -al /tmp/nvflare/image_stats/simulate_job/statistics/image_statistics.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd042db-6ce0-4e37-bcbe-d96051e4d164",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization\n",
    "We can visualize the results easly via the visualizaiton notebook. Before we do that, we need to copy the data to the notebook directory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c89693-37b9-450c-85dd-8a2d78fee3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cp /tmp/nvflare/image_stats/simulate_job/statistics/image_statistics.json ./."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57ca01-ba1c-42cb-8ff0-2952ead1f8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from nvflare.app_opt.statistics.visualization.statistics_visualization import Visualization\n",
    "with open('image_statistics.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "vis = Visualization()\n",
    "vis.show_stats(data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabd36b-970b-4cfb-8a67-16a959e96e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100%  depth:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878fb01-a70f-4d39-b538-34d3f93ad38c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vis.show_histograms(data = data, plot_type=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda06c0b-798d-480d-9b4c-a62fab95bcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "The global and local histograms differences are none as we are using the same dataset for all clients. \n",
    "\n",
    "## We are done !\n",
    "Congratulations! you have just completed the federated stats image histogram calulation. \n",
    "\n",
    "If you would like to see another example of federated statistics calculations and configurations, please checkout [federated_statistics](https://github.com/NVIDIA/NVFlare/tree/main/examples/advanced/federated-statistics) and [fed_stats with spleen_ct_segmentation](https://github.com/NVIDIA/NVFlare/tree/main/integration/monai/examples/spleen_ct_segmentation_sim)\n",
    "\n",
    "Let's move on to the next example and see how can we train the image classifier using pytorch with CIFAR10 data.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
