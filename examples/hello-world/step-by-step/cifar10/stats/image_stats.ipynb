{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26cb3afa",
   "metadata": {},
   "source": [
    "# Calculate CIFAR10 Image Histogram\n",
    "\n",
    "Before training the image classifier, the PyTorch example follows the following steps: \n",
    "\n",
    "* **Prepare Data**\n",
    "    * Load and normalize the CIFAR10 training and test datasets using torchvision\n",
    "    \n",
    "* **Training**\n",
    "    * Define a Convolutional Neural Network\n",
    "    * Define a loss function\n",
    "    * Train the network on the training data\n",
    "    * Test the network on the test data\n",
    "    \n",
    "We will add another step to calculate the data histogram and compare the local (site) histogram and global histograms. So the above steps become\n",
    "\n",
    "\n",
    "* **Prepare Data**\n",
    "    * Load and normalize the CIFAR10 training and test datasets using torchvision\n",
    "\n",
    "* **Data Statistics**\n",
    "    * Calculate data stastics: image intensity histograms\n",
    "    \n",
    "* **Training**\n",
    "    * Define a Convolutional Neural Network\n",
    "    * Define a loss function\n",
    "    * Train the network on the training data\n",
    "    * Test the network on the test data\n",
    "\n",
    "\n",
    "\n",
    "## Setup NVFLARE\n",
    "\n",
    "Follow [Getting Started](../../../../getting_started/readme.ipynb) to set up a virtual environment and install NVFLARE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a17f22-5667-4f99-b4f6-d49116db74b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install requirements\n",
    "assuming the current directory is 'cifar10/stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92caefd5-9438-46b1-b46f-12b929ff11a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8969bf-d010-42b5-a807-0808922402d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0065b351-baac-4f84-aa15-3d875f86cb93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare Data\n",
    "\n",
    "Generally, when you have to deal with image, text, audio or video data, you can use standard python packages that load data into a numpy array. Then you can convert this array into a torch.*Tensor. Torch provided a package called torchvision, that has data loaders for common datasets such as ImageNet, CIFAR10, MNIST, etc. and data transformers for images, viz., torchvision.datasets and torch.utils.data.DataLoader.\n",
    "\n",
    "For CIFAR10 dataset, it has the classes: ‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’. \n",
    "The images in CIFAR-10 are of size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size.\n",
    "\n",
    "![cifar10](../data/cifar10.png)\n",
    "\n",
    "\n",
    "The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1].\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e64769-17f1-4805-9399-1c141e050065",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "CIFAR10_ROOT = \"/tmp/nvflare/data/cifar10\"\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 6\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=CIFAR10_ROOT, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562f713-5892-43c7-a3d6-d277c337b5ea",
   "metadata": {},
   "source": [
    "Once you have extract the data from zip file, you can check the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc68ebf-6071-479d-8cc1-15439bedea02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls -al {CIFAR10_ROOT}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a2400-6c8c-4a19-af1b-3cc8ad526a77",
   "metadata": {},
   "source": [
    "Let's explore the data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c68fd-12a8-40f4-8794-55bcde83606d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))\n",
    "\n",
    "# dimension and shapes\n",
    " \n",
    "# Display image and label.\n",
    "print(f\"\\nFeature batch shape: {images.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()} \\n\")\n",
    "\n",
    "\n",
    "print(\"train datasize =\", len(trainset))\n",
    "print(\"test datasize =\", len(testset))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498d3c6-1e6e-4501-a013-a2430f8d2d24",
   "metadata": {},
   "source": [
    "We can see the images has shape of [batch, channel, rows, cols] = [6,3,32,32]\n",
    "\n",
    "## Download data in script\n",
    "We have prepared python script to download the data as well. \n",
    "```\n",
    "python ../data/download.py  --dataset_path <data_path>\n",
    "```\n",
    "if dataset_path is not specified, it default to CIFAR10_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19223a-25bf-42c3-8109-2aaf80b298f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python ../data/download.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda6b93-1723-4ba5-a123-f7a782bde545",
   "metadata": {},
   "source": [
    "## Create Local Image Intensity Histogram Calculator\n",
    "\n",
    "We ignored all other statistical calculations (mean, standard deviation, etc.) as they do not apply. all methods have default implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfefa1-7ad7-4602-8370-2c621dd8eb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "from nvflare.apis.fl_context import FLContext\n",
    "from nvflare.app_common.abstract.statistics_spec import Bin, DataType, Feature, Histogram, HistogramType, Statistics\n",
    " \n",
    "# the dataset path    \n",
    "CIFAR10_ROOT = \"/tmp/nvflare/data/cifar10\"\n",
    "\n",
    "\n",
    "class ImageStatistics(Statistics):\n",
    "\n",
    "    def __init__(self, \n",
    "                 data_root: str = CIFAR10_ROOT, \n",
    "                 batch_size: int = 4):\n",
    "        \"\"\"local image intensity calculator.\n",
    "\n",
    "        Args:\n",
    "            dataset_path: directory with local image data.\n",
    "         Returns:\n",
    "            Histogram of local statistics`\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dataset_path = data_root\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # there are three color channels : RGB, each corresponding to each channel index\n",
    "        # we are going treat each channel as one feature, the feature Ids are corresponding to tensor channel index. \n",
    "        # The feature name is named \"red\", \"gree\", \"blue\" (RGB). \n",
    "        \n",
    "        self.features_ids = { \"red\": 0, \"green\": 1,\"blue\": 2}\n",
    "        self.image_features  = [Feature(\"red\", DataType.FLOAT),\n",
    "                                Feature(\"green\", DataType.FLOAT),\n",
    "                                Feature(\"blue\", DataType.FLOAT)]\n",
    "        self.dataset_lengths = {}\n",
    "        self.loaders = {}\n",
    "\n",
    "        self.client_name = None\n",
    "        self.fl_ctx = None\n",
    "\n",
    "\n",
    "\n",
    "    def initialize(self, fl_ctx: FLContext):\n",
    "\n",
    "        # FLContext is context information for the client side NVFLARE engine. \n",
    "        # it includes many runtime information. \n",
    "        # Here we only interested in client site name. \n",
    "        # fl_ctx.get_identity_name() will return the client's name\n",
    "        \n",
    "        self.fl_ctx = fl_ctx\n",
    "        self.client_name = \"local_client\" if fl_ctx is None else fl_ctx.get_identity_name()\n",
    "        \n",
    "        transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "        trainset = torchvision.datasets.CIFAR10(root=self.dataset_path, train=True, download=True, transform=transform)\n",
    "        testset = torchvision.datasets.CIFAR10(root=self.dataset_path, train=False,  download=True, transform=transform)\n",
    "        self.dataset_lengths = {\"train\": len(trainset), \"test\":len(testset)}\n",
    "        \n",
    "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "        testloader = torch.utils.data.DataLoader(testset, batch_size=self.batch_size, shuffle=False, num_workers=2)\n",
    "        self.loaders = {\"train\": trainloader, \"test\": testloader}\n",
    "\n",
    "    def features(self) -> Dict[str, List[Feature]]:\n",
    "        return {\"train\": self.image_features, \n",
    "                \"test\":  self.image_features}\n",
    "\n",
    "    def count(self, dataset_name: str, feature_name: str) -> int:\n",
    "        return self.dataset_lengths[dataset_name]\n",
    "        \n",
    " \n",
    "    def histogram(self,\n",
    "                  dataset_name: str,\n",
    "                  feature_name: str, \n",
    "                  num_of_bins: int, \n",
    "                  global_min_value: float, \n",
    "                  global_max_value: float) -> Histogram:\n",
    "     \n",
    "        print(f\"calculating image intensity histogram for client {self.client_name}\")\n",
    "        channel = self.features_ids[feature_name]\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        histogram_bins: List[Bin] = []\n",
    "        bin_edges = []\n",
    "        histogram = np.zeros(num_of_bins, dtype=float)\n",
    "\n",
    "        for inputs, _ in self.loaders[dataset_name]:\n",
    "            for img in inputs:\n",
    "                counts, bin_edges = np.histogram(img[channel, : , :],\n",
    "                                                 bins=num_of_bins,\n",
    "                                                 range=(global_min_value, global_max_value))\n",
    "                histogram += counts\n",
    "\n",
    "        for i in range(num_of_bins):\n",
    "            low_value = bin_edges[i]\n",
    "            high_value = bin_edges[i + 1]\n",
    "            bin_sample_count = histogram[i]\n",
    "            histogram_bins.append(Bin(low_value=low_value, high_value=high_value, sample_count=bin_sample_count))\n",
    "\n",
    "        return Histogram(HistogramType.STANDARD, histogram_bins)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7cfbf9-35cd-40d3-b1fd-851edff06f70",
   "metadata": {},
   "source": [
    "Let's test if the code works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499fac50-35b5-43ba-bc05-cc4a01c3af55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_cal = ImageStatistics()\n",
    "\n",
    "hist_cal.initialize(fl_ctx = None)\n",
    "features = hist_cal.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c458722d-08c8-4d7f-b7fb-cd063a991c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_cal.count(\"train\", \"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed64ae6-6887-4e84-88e1-7647b42d5fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist_cal.histogram(\"train\", \"red\", 20, 0, 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38287e-c425-4c31-8760-f10e3a1edc8b",
   "metadata": {},
   "source": [
    "The code is working. Let's set up an NVFLARE job in federated computing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6becb401-c4f1-4ad3-b30a-169cc9b35525",
   "metadata": {},
   "source": [
    "## Create Federated Histogram Job\n",
    "We are going to use NVFLARE job API to construct a FedJob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1daf5-f115-4910-933a-1fa148b1b13a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat image_stats_job.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09aed14-5011-4418-8840-5f7c16c97534",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Run Job in FL Simulator\n",
    "\n",
    "\n",
    "**Run job.simulator_run()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b8638-d7a2-4207-9821-e444cf40f8ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python image_stats_job.py -w /tmp/nvflare/image_stats -n 2 -d /tmp/nvflare/data/cifar10 -o \"statistics/image_statistics.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e73b8-efa9-4a52-8342-324eb07a0422",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Run Job using Simulator CLI**\n",
    "\n",
    "```\n",
    "! python image_stats_job.py -co -j /tmp/nvflare/jobs/stats_image -n 2\n",
    "! nvflare simulator /tmp/nvflare/jobs/stats_image -w /tmp/nvflare/image_stats -n 2 -t 2\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bf6e9a-3265-4e45-8b06-c8e543605f21",
   "metadata": {},
   "source": [
    "\n",
    "**Examine the result**\n",
    "\n",
    "Note that the result is written at \n",
    "\n",
    "**/tmp/nvflare/image_stats/server/simulate_job/statistics/image_statistics.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112a7dd0-45d9-42ea-98b2-f72a3bbccf48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -al /tmp/nvflare/image_stats/server/simulate_job/statistics/image_statistics.json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd042db-6ce0-4e37-bcbe-d96051e4d164",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization\n",
    "We can visualize the results easly via the visualizaiton notebook. Before we do that, we need to copy the data to the notebook directory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c89693-37b9-450c-85dd-8a2d78fee3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cp /tmp/nvflare/image_stats/server/simulate_job/statistics/image_statistics.json ./."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e57ca01-ba1c-42cb-8ff0-2952ead1f8ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from nvflare.app_opt.statistics.visualization.statistics_visualization import Visualization\n",
    "with open('image_statistics.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "vis = Visualization()\n",
    "vis.show_stats(data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabd36b-970b-4cfb-8a67-16a959e96e64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100%  depth:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3878fb01-a70f-4d39-b538-34d3f93ad38c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vis.show_histograms(data = data, plot_type=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda06c0b-798d-480d-9b4c-a62fab95bcf0",
   "metadata": {
    "tags": []
   },
   "source": [
    "The global and local histograms show no differences because we are using the same dataset for all clients.\n",
    "\n",
    "## We are done !\n",
    "Congratulations! you have just completed the federated stats image histogram calulation. \n",
    "\n",
    "If you would like to see another example of federated statistics calculations and configurations, please checkout [federated_statistics](https://github.com/NVIDIA/NVFlare/tree/main/examples/advanced/federated-statistics) and [fed_stats with spleen_ct_segmentation](https://github.com/NVIDIA/NVFlare/tree/main/integration/monai/examples/spleen_ct_segmentation_sim)\n",
    "\n",
    "Let's move on to the next examples and see how can we train the image classifier using pytorch with CIFAR10 data.\n",
    "First we will look at the [sag](../sag/sag.ipynb) example, which illustrates how to use the ScatterAndGather workflow for FedAvg with the Client API.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
