{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ccbfaee-9efe-4fe1-bab6-60462769fede",
   "metadata": {},
   "source": [
    "# Federated Horizontal XGBoost with Bagging Collaboration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ea300-8fd1-4568-8121-9bb9d3d303b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "This tutorial illustrates a federated horizontal xgboost learning on tabular data with bagging collaboration. \n",
    "\n",
    "Before do the training, we need to setup NVFLARE\n",
    "\n",
    "## Setup NVFLARE\n",
    "\n",
    "Follow [Getting Started](https://nvflare.readthedocs.io/en/main/getting_started.html) to set up a virtual environment and install NVFLARE.\n",
    "\n",
    "You can also follow this [notebook](https://github.com/NVIDIA/NVFlare/blob/main/examples/nvflare_setup.ipynb) to get set up.\n",
    "\n",
    "> Make sure you have installed nvflare from **terminal** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c1eb8-3070-45b8-a83c-f16d060d0385",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install requirements\n",
    "assuming the current directory is '/examples/hello-world/step-by-step/higgs/xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3f5a8f-cd09-441e-a4c3-f1a121fd8244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ziyuexu/Research/Experiment/nvflare_tab_exp/examples/hello-world/step-by-step/higgs/xgboost\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7bf5e9-1f21-430a-9227-3e912eba5f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pandas in /home/ziyuexu/anaconda3/envs/nvflare_ml/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in /home/ziyuexu/anaconda3/envs/nvflare_ml/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.3.2)\n",
      "Requirement already satisfied: xgboost in /home/ziyuexu/anaconda3/envs/nvflare_ml/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ziyuexu/anaconda3/envs/nvflare_ml/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ziyuexu/anaconda3/envs/nvflare_ml/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 1)) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/ziyuexu/anaconda3/envs/nvflare_ml/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 1)) (1.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ziyuexu/anaconda3/envs/nvflare_ml/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 2)) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/ziyuexu/anaconda3/envs/nvflare_ml/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/ziyuexu/.local/lib/python3.8/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.9.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ziyuexu/anaconda3/envs/nvflare_ml/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87c3da-59dc-4551-9448-b20b64a57137",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare data\n",
    "Please reference [prepare_higgs_data](../prepare_data.ipynb) notebooks. Pay attention to the current location. You need to switch \"higgs\" directory to run the data split.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe5bc9a-d589-4d48-89b8-73fc1d8fe44b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we have our data prepared. we are ready to do the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc62f2-5571-44d0-bb8b-b6a660a500c5",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "\n",
    "We noticed from time-to-time the Higgs dataset is making small changes which causing job to fail. so we need to do some clean up or skip certain rows. \n",
    "For example: certain floating number mistakenly add an alphabetical letter at some point of time. This may have already fixed by UCI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dbec4e-b4a9-44fe-954e-62f1d0f99c40",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "This tutorial uses [XGBoost](https://github.com/dmlc/xgboost), which is an optimized distributed gradient boosting library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80eb797-f1aa-4537-ab50-4c27fb9cff60",
   "metadata": {},
   "source": [
    "### Federated XGBoost Model\n",
    "Here we use tree-based bagging collaboration\n",
    "\n",
    "Under this setting, individual trees are independently trained on each client's local data without aggregating the global sample gradient histogram information.\n",
    "Trained trees are collected and passed to the server / other clients for bagging aggregation and further boosting rounds.\n",
    "\n",
    "The XGBoost Booster api is leveraged to create in-memory Booster objects that persist across rounds to cache predictions from trees added in previous rounds and retain other data structures needed for training.\n",
    "\n",
    "Let's look at the code see how we convert the local training script to the federated training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b5d82b-3c5a-492a-bc92-86ca7367ff92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/ziyuexu/Research/Experiment/nvflare_tab_exp/examples/hello-world/step-by-step/higgs/xgboost\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fdbcc96-3ce0-4e9e-a986-f877e082c0ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import argparse\n",
      "import csv\n",
      "import json\n",
      "from typing import Dict, List, Tuple\n",
      "\n",
      "import pandas as pd\n",
      "import xgboost as xgb\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# (1) import nvflare client API\n",
      "from nvflare import client as flare\n",
      "from nvflare.app_opt.xgboost.tree_based.shareable_generator import update_model\n",
      "\n",
      "\n",
      "def to_dataset_tuple(data: dict):\n",
      "    dataset_tuples = {}\n",
      "    for dataset_name, dataset in data.items():\n",
      "        dataset_tuples[dataset_name] = _to_data_tuple(dataset)\n",
      "    return dataset_tuples\n",
      "\n",
      "\n",
      "def _to_data_tuple(data):\n",
      "    data_num = data.shape[0]\n",
      "    # split to feature and label\n",
      "    x = data.iloc[:, 1:]\n",
      "    y = data.iloc[:, 0]\n",
      "    return x.to_numpy(), y.to_numpy(), data_num\n",
      "\n",
      "\n",
      "def load_features(feature_data_path: str) -> List:\n",
      "    try:\n",
      "        features = []\n",
      "        with open(feature_data_path, \"r\") as file:\n",
      "            # Create a CSV reader object\n",
      "            csv_reader = csv.reader(file)\n",
      "            line_list = next(csv_reader)\n",
      "            features = line_list\n",
      "        return features\n",
      "    except Exception as e:\n",
      "        raise Exception(f\"Load header for path'{feature_data_path} failed! {e}\")\n",
      "\n",
      "\n",
      "def load_data(\n",
      "    data_path: str, data_features: List, random_state: int, test_size: float, skip_rows=None\n",
      ") -> Dict[str, pd.DataFrame]:\n",
      "    try:\n",
      "        df: pd.DataFrame = pd.read_csv(\n",
      "            data_path, names=data_features, sep=r\"\\s*,\\s*\", engine=\"python\", na_values=\"?\", skiprows=skip_rows\n",
      "        )\n",
      "\n",
      "        train, test = train_test_split(df, test_size=test_size, random_state=random_state)\n",
      "\n",
      "        return {\"train\": train, \"test\": test}\n",
      "\n",
      "    except Exception as e:\n",
      "        raise Exception(f\"Load data for path '{data_path}' failed! {e}\")\n",
      "\n",
      "\n",
      "def transform_data(data: Dict[str, Tuple]) -> Dict[str, Tuple]:\n",
      "    # Standardize features by removing the mean and scaling to unit variance\n",
      "    scaler = StandardScaler()\n",
      "    scaled_datasets = {}\n",
      "    for dataset_name, (x_data, y_data, data_num) in data.items():\n",
      "        x_scaled = scaler.fit_transform(x_data)\n",
      "        scaled_datasets[dataset_name] = (x_scaled, y_data, data_num)\n",
      "    return scaled_datasets\n",
      "\n",
      "\n",
      "def main():\n",
      "    parser = define_args_parser()\n",
      "    args = parser.parse_args()\n",
      "    data_root_dir = args.data_root_dir\n",
      "    random_state = args.random_state\n",
      "    test_size = args.test_size\n",
      "    skip_rows = args.skip_rows\n",
      "    num_client_bagging = args.num_client_bagging\n",
      "\n",
      "    # (2) initializes NVFlare client API\n",
      "    flare.init()\n",
      "\n",
      "    site_name = flare.get_site_name()\n",
      "    feature_data_path = f\"{data_root_dir}/{site_name}_header.csv\"\n",
      "    features = load_features(feature_data_path)\n",
      "    n_features = len(features) - 1  # remove label\n",
      "\n",
      "    data_path = f\"{data_root_dir}/{site_name}.csv\"\n",
      "    data = load_data(\n",
      "        data_path=data_path, data_features=features, random_state=random_state, test_size=test_size, skip_rows=skip_rows\n",
      "    )\n",
      "\n",
      "    data = to_dataset_tuple(data)\n",
      "    dataset = transform_data(data)\n",
      "    x_train, y_train, train_size = dataset[\"train\"]\n",
      "    x_test, y_test, test_size = dataset[\"test\"]\n",
      "\n",
      "    # convert to xgboost data matrix\n",
      "    dmat_train = xgb.DMatrix(x_train, label=y_train)\n",
      "    dmat_test = xgb.DMatrix(x_test, label=y_test)\n",
      "\n",
      "    xgb_params = {\n",
      "        \"eta\": 0.1 / num_client_bagging,\n",
      "        \"objective\": \"binary:logistic\",\n",
      "        \"max_depth\": 8,\n",
      "        \"eval_metric\": \"auc\",\n",
      "        \"nthread\": 16,\n",
      "        \"num_parallel_tree\": 1,\n",
      "        \"subsample\": 1.0,\n",
      "        \"tree_method\": \"hist\",\n",
      "    }\n",
      "\n",
      "    global_model_as_dict = None\n",
      "    while flare.is_running():\n",
      "        # (3) receives FLModel from NVFlare\n",
      "        input_model = flare.receive()\n",
      "        global_params = input_model.params\n",
      "        curr_round = input_model.current_round\n",
      "\n",
      "        print(f\"current_round={curr_round}\")\n",
      "        if curr_round == 0:\n",
      "            # (4) first round, no global model\n",
      "            model = xgb.train(\n",
      "                xgb_params,\n",
      "                dmat_train,\n",
      "                num_boost_round=1,\n",
      "                evals=[(dmat_train, \"train\"), (dmat_test, \"test\")],\n",
      "            )\n",
      "            config = model.save_config()\n",
      "        else:\n",
      "            # (5) update model based on global updates\n",
      "            model_updates = global_params[\"model_data\"]\n",
      "            for update in model_updates:\n",
      "                global_model_as_dict = update_model(global_model_as_dict, json.loads(update))\n",
      "            loadable_model = bytearray(json.dumps(global_model_as_dict), \"utf-8\")\n",
      "            # load model\n",
      "            model.load_model(loadable_model)\n",
      "            model.load_config(config)\n",
      "\n",
      "            # (6) evaluate model\n",
      "            auc = evaluate_model(x_test, model, y_test)\n",
      "            # Print the results\n",
      "            print(f\"{site_name}: global model AUC: {auc:.5f}\")\n",
      "\n",
      "            # train model in two steps\n",
      "            # first, eval on train and test\n",
      "            eval_results = model.eval_set(\n",
      "                evals=[(dmat_train, \"train\"), (dmat_test, \"test\")], iteration=model.num_boosted_rounds() - 1\n",
      "            )\n",
      "            print(eval_results)\n",
      "            # second, train for one round\n",
      "            model.update(dmat_train, model.num_boosted_rounds())\n",
      "\n",
      "        # (7) construct trained FL model\n",
      "        # Extract newly added tree using xgboost_bagging slicing api\n",
      "        bst_new = model[model.num_boosted_rounds() - 1 : model.num_boosted_rounds()]\n",
      "        local_model_update = bst_new.save_raw(\"json\")\n",
      "        params = {\"model_data\": local_model_update}\n",
      "        metrics = {\"accuracy\": auc}\n",
      "        output_model = flare.FLModel(params=params, metrics=metrics)\n",
      "\n",
      "        # (8) send model back to NVFlare\n",
      "        flare.send(output_model)\n",
      "\n",
      "\n",
      "def evaluate_model(x_test, model, y_test):\n",
      "    # Make predictions on the testing set\n",
      "    dtest = xgb.DMatrix(x_test)\n",
      "    y_pred = model.predict(dtest)\n",
      "\n",
      "    # Evaluate the model\n",
      "    auc = roc_auc_score(y_test, y_pred)\n",
      "    return auc\n",
      "\n",
      "\n",
      "def define_args_parser():\n",
      "    parser = argparse.ArgumentParser(description=\"scikit learn linear model with SGD\")\n",
      "    parser.add_argument(\"--data_root_dir\", type=str, help=\"root directory path to csv data file\")\n",
      "    parser.add_argument(\"--random_state\", type=int, default=0, help=\"random state\")\n",
      "    parser.add_argument(\"--test_size\", type=float, default=0.2, help=\"test ratio, default to 20%\")\n",
      "    parser.add_argument(\n",
      "        \"--num_client_bagging\",\n",
      "        type=int,\n",
      "        default=3,\n",
      "        help=\"number of clients with uniform data sizes participating in bagging\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--skip_rows\",\n",
      "        type=str,\n",
      "        default=None,\n",
      "        help=\"\"\"If skip_rows = N, the first N rows will be skipped, \n",
      "       if skiprows=[0, 1, 4], the rows will be skip by row indices such as row 0,1,4 will be skipped. \"\"\",\n",
      "    )\n",
      "    return parser\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "!cat code/xgboost_fl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d932e8c-7c7c-470b-8448-43288feba261",
   "metadata": {
    "tags": []
   },
   "source": [
    "The code is pretty much like the standard scikit-learn training script of `code/xgboost_local_iter.py`\n",
    "\n",
    "#### load data\n",
    "\n",
    "We first load the features from the header file: \n",
    "    \n",
    "```\n",
    "    site_name = flare.get_site_name()\n",
    "    feature_data_path = f\"{data_root_dir}/{site_name}_header.csv\"\n",
    "    features = load_features(feature_data_path)\n",
    "    n_features = len(features) -1\n",
    "\n",
    "    data_path = f\"{data_root_dir}/{site_name}.csv\"\n",
    "    data = load_data(data_path=data_path, data_features=features, test_size=test_size, skip_rows=skip_rows)\n",
    "\n",
    "```\n",
    "\n",
    "then load the data from the main csv file, then transform the data and split the training and test data based on the test_size provided.  \n",
    "\n",
    "```\n",
    "    data = to_dataset_tuple(data)\n",
    "    dataset = transform_data(data)\n",
    "    x_train, y_train, train_size = dataset[\"train\"]\n",
    "    x_test, y_test, test_size = dataset[\"test\"]\n",
    "\n",
    "```\n",
    "\n",
    "The part that's specific to Federated Learning is in the following codes\n",
    "\n",
    "```\n",
    "# (1) import nvflare client API\n",
    "from nvflare import client as flare\n",
    "\n",
    "```\n",
    "```\n",
    "# (2) initializes NVFlare client API\n",
    "    flare.init()\n",
    "\n",
    "    site_name = flare.get_site_name()\n",
    "    \n",
    "```\n",
    "    \n",
    "These few lines, import NVFLARE Client API and initialize it, then use the API to find the site_name (such as site-1, site-2 etc.). With the site-name, we can construct the site-specific \n",
    "data path such as\n",
    "\n",
    "```\n",
    "    feature_data_path = f\"{data_root_dir}/{site_name}_header.csv\"\n",
    "\n",
    "    data_path = f\"{data_root_dir}/{site_name}.csv\"\n",
    "```\n",
    "\n",
    "#### Training \n",
    "\n",
    "In the standard traditional xgboost, we would train the model such as\n",
    "```\n",
    "  model = xgb.train(...) \n",
    "```\n",
    "\n",
    "with federated learning, using FLARE Client API, we need to make a few changes\n",
    "* 1) we are not only training in local iterations, but also global rounds, we need to keep the program running until we reached to the totoal number of rounds \n",
    "  \n",
    "  ```\n",
    "      while flare.is_running():\n",
    "          ... rest of code\n",
    "  \n",
    "  ```\n",
    "  \n",
    "* 2) Unlike local learning, we have now have more than one clients/sites participating the training. To ensure every site starts with the same model parameters, we use server to broadcase the initial model parameters to every sites at the first round ( current_round = 0). \n",
    "\n",
    "* 3) We will need to use FLARE client API to receive global model updates \n",
    "\n",
    "```\n",
    "        # (3) receives FLModel from NVFlare\n",
    "        input_model = flare.receive()\n",
    "        global_params = input_model.params\n",
    "        curr_round = input_model.current_round\n",
    "```\n",
    "\n",
    "```\n",
    "        if curr_round == 0:\n",
    "            # (4) first round, no global model\n",
    "            model = xgb.train(\n",
    "                xgb_params,\n",
    "                dmat_train,\n",
    "                num_boost_round=1,\n",
    "                evals=[(dmat_train, \"train\"), (dmat_test, \"test\")],\n",
    "            )\n",
    "            config = model.save_config()\n",
    "        ....\n",
    "```\n",
    "* 4) if it is not the first round, we need to use the global model to update the local model before training the next round. \n",
    "\n",
    "```\n",
    "            # (5) update model based on global updates\n",
    "            model_updates = global_params[\"model_data\"]\n",
    "            for update in model_updates:\n",
    "                global_model_as_dict = update_model(global_model_as_dict, json.loads(update))\n",
    "            loadable_model = bytearray(json.dumps(global_model_as_dict), \"utf-8\")\n",
    "            # load model\n",
    "            model.load_model(loadable_model)\n",
    "            model.load_config(config)\n",
    "```\n",
    "\n",
    "* 5) we then evaluate the global model using the local data\n",
    "\n",
    "```\n",
    "            # (6) evaluate model\n",
    "            auc = evaluate_model(x_test, model, y_test)\n",
    "```\n",
    "* 6) finally we do the training \n",
    "\n",
    "```\n",
    "            # train model in two steps\n",
    "            # first, eval on train and test\n",
    "            eval_results = model.eval_set(\n",
    "                evals=[(dmat_train, \"train\"), (dmat_test, \"test\")], iteration=model.num_boosted_rounds() - 1\n",
    "            )\n",
    "            print(eval_results)\n",
    "            # second, train for one round\n",
    "            model.update(dmat_train, model.num_boosted_rounds())\n",
    "        \n",
    "```\n",
    "\n",
    "* 7) we need the new training result (new tree) back to server for aggregation, to do that, we have the following code\n",
    "\n",
    "```\n",
    "        # (7) construct trained FL model\n",
    "        # Extract newly added tree using xgboost_bagging slicing api\n",
    "        bst_new = model[model.num_boosted_rounds() - 1 : model.num_boosted_rounds()]\n",
    "        local_model_update = bst_new.save_raw(\"json\")\n",
    "        params = {\"model_data\": local_model_update}\n",
    "        metrics = {\"accuracy\": auc}\n",
    "        output_model = flare.FLModel(params=params, metrics=metrics)\n",
    "\n",
    "        # (8) send model back to NVFlare\n",
    "        flare.send(output_model)\n",
    "```\n",
    "\n",
    "## Prepare Job  \n",
    "\n",
    "Now, we have the code, we need to prepare job folder with configurations to run in NVFLARE. To do this, we can leveage the job template for scikit learn. First look at the the available job templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fdd962-e63c-4b58-81e7-beeedd05509b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvflare config -jt ../../../../../job_templates/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5ed150d-4692-49fe-87a6-1779ec64d9df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following job templates are available: \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  name                 Description                                                  Controller Type      Client Category     \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  cyclic_cc_pt         client-controlled cyclic workflow with PyTorch ClientAPI tra client               client_api          \n",
      "  cyclic_pt            server-controlled cyclic workflow with PyTorch ClientAPI tra server               client_api          \n",
      "  psi_csv              private-set intersection for csv data                        server               Executor            \n",
      "  sag_cross_np         scatter & gather and cross-site validation using numpy       server               client executor     \n",
      "  sag_cse_pt           scatter & gather workflow and cross-site evaluation with PyT server               client_api          \n",
      "  sag_gnn              scatter & gather workflow for gnn learning                   server               client_api          \n",
      "  sag_nemo             Scatter and Gather Workflow for NeMo                         server               client_api          \n",
      "  sag_np               scatter & gather workflow using numpy                        server               client_api          \n",
      "  sag_pt               scatter & gather workflow using pytorch                      server               client_api          \n",
      "  sag_pt_deploy_map    SAG workflow with pytorch, deploy_map, site-specific configs server               client_api          \n",
      "  sag_pt_executor      scatter & gather workflow and cross-site evaluation with PyT server               Executor            \n",
      "  sag_pt_model_learner scatter & gather workflow and cross-site evaluation with PyT server               ModelLearner        \n",
      "  sag_tf               scatter & gather workflow using TensorFlow                   server               client_api          \n",
      "  sklearn_kmeans       scikit-learn KMeans model                                    server               client_api          \n",
      "  sklearn_linear       scikit-learn linear model                                    server               client_api          \n",
      "  sklearn_svm          scikit-learn SVM model                                       server               client_api          \n",
      "  stats_df             FedStats: tabular data with pandas                           server               stats executor      \n",
      "  stats_image          FedStats: image intensity histogram                          server               stats executor      \n",
      "  swarm_cse_pt         Swarm Learning with Cross-Site Evaluation with PyTorch       client               client_api          \n",
      "  swarm_cse_pt_model_l Swarm Learning with Cross-Site Evaluation with PyTorch Model client               ModelLearner        \n",
      "  vertical_xgb         vertical federated xgboost                                   server               Executor            \n",
      "  xgboost_bagging      xgboost horizontal bagging model                             server               client_api          \n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!nvflare job list_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7602cec-5fb5-4b23-a82a-b7444f2af471",
   "metadata": {},
   "source": [
    "the `xgboost_bagging` is the one we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9899a684-444f-4c04-a388-4bdd9b7dc649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following are the variables you can change in the template\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                                                                                       \n",
      "  job folder: /tmp/nvflare/jobs/xgboost                                                                                                  \n",
      "                                                                                                                                       \n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "  file_name                      var_name                       value                               component                          \n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "  meta.conf                      app                            ['@ALL']                                                               \n",
      "\n",
      "  config_fed_client.conf         app_config                     --data_root_dir /tmp/nvflare/datase                                    \n",
      "  config_fed_client.conf         app_script                     xgboost_fl.py                                                          \n",
      "  config_fed_client.conf         evaluate_task_name             evaluate                                                               \n",
      "  config_fed_client.conf         file_check_interval            0.1                                 FilePipe                           \n",
      "  config_fed_client.conf         heartbeat_interval             5.0                                                                    \n",
      "  config_fed_client.conf         heartbeat_timeout              60                                                                     \n",
      "  config_fed_client.conf         last_result_transfer_timeout   5.0                                                                    \n",
      "  config_fed_client.conf         launch_once                    True                                                                   \n",
      "  config_fed_client.conf         mode                           PASSIVE                             FilePipe                           \n",
      "  config_fed_client.conf         params_exchange_format         raw                                                                    \n",
      "  config_fed_client.conf         params_transfer_type           FULL                                                                   \n",
      "  config_fed_client.conf         read_interval                  0.001                                                                  \n",
      "  config_fed_client.conf         result_poll_interval           0.001                                                                  \n",
      "  config_fed_client.conf         root_path                                                          FilePipe                           \n",
      "  config_fed_client.conf         script                         python3 custom/{app_script}  {app_c SubprocessLauncher                 \n",
      "  config_fed_client.conf         train_with_evaluation          True                                                                   \n",
      "  config_fed_client.conf         workers                        4                                                                      \n",
      "\n",
      "  config_fed_server.conf         allow_empty_global_weights     True                                                                   \n",
      "  config_fed_server.conf         load_as_dict                   True                                XGBModelPersistor                  \n",
      "  config_fed_server.conf         min_clients                    3                                                                      \n",
      "  config_fed_server.conf         num_rounds                     101                                                                    \n",
      "  config_fed_server.conf         save_name                      xgboost_model.json                  XGBModelPersistor                  \n",
      "  config_fed_server.conf         start_round                    0                                                                      \n",
      "  config_fed_server.conf         task_request_interval          0.05                                                                   \n",
      "  config_fed_server.conf         train_timeout                  0                                                                      \n",
      "  config_fed_server.conf         wait_time_after_min_received   0                                                                      \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!nvflare job create -j /tmp/nvflare/jobs/xgboost -force -w xgboost_bagging \\\n",
    "-sd code \\\n",
    "-f config_fed_client.conf app_script=\"xgboost_fl.py\" app_config=\"--data_root_dir /tmp/nvflare/dataset/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17dc5dcc-573e-479e-965d-0cca8b58995a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "format_version = 2\n",
      "app_script = \"xgboost_fl.py\"\n",
      "app_config = \"--data_root_dir /tmp/nvflare/dataset/output\"\n",
      "executors = [\n",
      "  {\n",
      "    tasks = [\n",
      "      \"train\"\n",
      "    ]\n",
      "    executor {\n",
      "      path = \"nvflare.app_opt.pt.client_api_launcher_executor.ClientAPILauncherExecutor\"\n",
      "      args {\n",
      "        launcher_id = \"launcher\"\n",
      "        pipe_id = \"pipe\"\n",
      "        heartbeat_timeout = 60\n",
      "        params_exchange_format = \"raw\"\n",
      "        params_transfer_type = \"FULL\"\n",
      "        train_with_evaluation = true\n",
      "        launch_once = true\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n",
      "task_data_filters = []\n",
      "task_result_filters = []\n",
      "components = [\n",
      "  {\n",
      "    id = \"launcher\"\n",
      "    path = \"nvflare.app_common.launchers.subprocess_launcher.SubprocessLauncher\"\n",
      "    args {\n",
      "      script = \"python3 custom/{app_script}  {app_config} \"\n",
      "    }\n",
      "  }\n",
      "  {\n",
      "    id = \"pipe\"\n",
      "    path = \"nvflare.fuel.utils.pipe.file_pipe.FilePipe\"\n",
      "    args {\n",
      "      mode = \"PASSIVE\"\n",
      "      root_path = \"\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/nvflare/jobs/xgboost/app/config/config_fed_client.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3086fd-ecf3-4584-a643-5706ae2069b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/tmp/nvflare/jobs/xgboost\u001b[0m\n",
      "├── \u001b[01;34mapp\u001b[0m\n",
      "│   ├── \u001b[01;34mconfig\u001b[0m\n",
      "│   │   ├── \u001b[01;32mconfig_fed_client.conf\u001b[0m\n",
      "│   │   └── \u001b[01;32mconfig_fed_server.conf\u001b[0m\n",
      "│   └── \u001b[01;34mcustom\u001b[0m\n",
      "│       ├── xgboost_fl.py\n",
      "│       ├── xgboost_local_iter.py\n",
      "│       └── xgboost_local_oneshot.py\n",
      "└── meta.conf\n",
      "\n",
      "3 directories, 6 files\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/nvflare/jobs/xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111b784-1722-4aff-a48b-fc50c6c67281",
   "metadata": {},
   "source": [
    ">Note \n",
    " we use skip_rows = 0 to skip 1st row. We could skip_rows = [0, 3] to skip first and 4th rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ff6b6-5e39-4636-8a48-44d2b503dcdb",
   "metadata": {},
   "source": [
    "\n",
    "## Run job in simulator\n",
    "\n",
    "We use the simulator to run this job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09800691-46e0-4a95-bea6-d2a4a425052b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-28 22:48:34,316 - SimulatorRunner - INFO - Create the Simulator Server.\n",
      "2023-11-28 22:48:34,317 - CoreCell - INFO - server: creating listener on tcp://0:58789\n",
      "2023-11-28 22:48:34,334 - CoreCell - INFO - server: created backbone external listener for tcp://0:58789\n",
      "2023-11-28 22:48:34,334 - ConnectorManager - INFO - 133663: Try start_listener Listener resources: {'secure': False, 'host': 'localhost'}\n",
      "2023-11-28 22:48:34,335 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00002 PASSIVE tcp://0:4680] is starting\n",
      "2023-11-28 22:48:34,836 - CoreCell - INFO - server: created backbone internal listener for tcp://localhost:4680\n",
      "2023-11-28 22:48:34,836 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 PASSIVE tcp://0:58789] is starting\n",
      "2023-11-28 22:48:34,948 - nvflare.fuel.hci.server.hci - INFO - Starting Admin Server localhost on Port 54865\n",
      "2023-11-28 22:48:34,949 - SimulatorRunner - INFO - Deploy the Apps.\n",
      "2023-11-28 22:48:34,955 - SimulatorRunner - INFO - Create the simulate clients.\n",
      "2023-11-28 22:48:34,959 - ClientManager - INFO - Client: New client site-1@192.168.1.232 joined. Sent token: df44f197-ecf8-4aef-bccc-3a4a85dcb01a.  Total clients: 1\n",
      "2023-11-28 22:48:34,959 - FederatedClient - INFO - Successfully registered client:site-1 for project simulator_server. Token:df44f197-ecf8-4aef-bccc-3a4a85dcb01a SSID:\n",
      "2023-11-28 22:48:34,960 - ClientManager - INFO - Client: New client site-2@192.168.1.232 joined. Sent token: 634d8b9e-bd36-4d9e-8332-1aaf23001192.  Total clients: 2\n",
      "2023-11-28 22:48:34,961 - FederatedClient - INFO - Successfully registered client:site-2 for project simulator_server. Token:634d8b9e-bd36-4d9e-8332-1aaf23001192 SSID:\n",
      "2023-11-28 22:48:34,962 - ClientManager - INFO - Client: New client site-3@192.168.1.232 joined. Sent token: 883e62e7-91ec-41ea-8b58-76298961ca02.  Total clients: 3\n",
      "2023-11-28 22:48:34,962 - FederatedClient - INFO - Successfully registered client:site-3 for project simulator_server. Token:883e62e7-91ec-41ea-8b58-76298961ca02 SSID:\n",
      "2023-11-28 22:48:34,963 - SimulatorRunner - INFO - Set the client status ready.\n",
      "2023-11-28 22:48:34,963 - SimulatorRunner - INFO - Deploy and start the Server App.\n",
      "2023-11-28 22:48:34,964 - Cell - INFO - Register blob CB for channel='server_command', topic='*'\n",
      "2023-11-28 22:48:34,964 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-11-28 22:48:34,964 - ServerCommandAgent - INFO - ServerCommandAgent cell register_request_cb: server.simulate_job\n",
      "2023-11-28 22:48:36,989 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Server runner starting ...\n",
      "2023-11-28 22:48:36,990 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: starting workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) ...\n",
      "2023-11-28 22:48:36,990 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Initializing ScatterAndGather workflow.\n",
      "2023-11-28 22:48:36,990 - XGBModelPersistor - INFO - Initializing server model as None\n",
      "2023-11-28 22:48:36,990 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) started\n",
      "2023-11-28 22:48:36,990 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Beginning ScatterAndGather training phase.\n",
      "2023-11-28 22:48:36,991 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 0 started.\n",
      "2023-11-28 22:48:36,991 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: scheduled task train\n",
      "2023-11-28 22:48:37,972 - SimulatorClientRunner - INFO - Start the clients run simulation.\n",
      "2023-11-28 22:48:38,974 - SimulatorClientRunner - INFO - Simulate Run client: site-1 on GPU group: None\n",
      "2023-11-28 22:48:38,975 - SimulatorClientRunner - INFO - Simulate Run client: site-2 on GPU group: None\n",
      "2023-11-28 22:48:38,976 - SimulatorClientRunner - INFO - Simulate Run client: site-3 on GPU group: None\n",
      "2023-11-28 22:48:40,010 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-11-28 22:48:40,011 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-11-28 22:48:40,022 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-11-28 22:48:40,071 - CoreCell - INFO - site-1.simulate_job: created backbone external connector to tcp://localhost:58789\n",
      "2023-11-28 22:48:40,071 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:58789] is starting\n",
      "2023-11-28 22:48:40,072 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:33356 => 127.0.0.1:58789] is created: PID: 133693\n",
      "2023-11-28 22:48:40,072 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 127.0.0.1:58789 <= 127.0.0.1:33356] is created: PID: 133663\n",
      "2023-11-28 22:48:40,076 - CoreCell - INFO - site-2.simulate_job: created backbone external connector to tcp://localhost:58789\n",
      "2023-11-28 22:48:40,076 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:58789] is starting\n",
      "2023-11-28 22:48:40,077 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:33358 => 127.0.0.1:58789] is created: PID: 133694\n",
      "2023-11-28 22:48:40,077 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00007 127.0.0.1:58789 <= 127.0.0.1:33358] is created: PID: 133663\n",
      "2023-11-28 22:48:40,086 - CoreCell - INFO - site-3.simulate_job: created backbone external connector to tcp://localhost:58789\n",
      "2023-11-28 22:48:40,086 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:58789] is starting\n",
      "2023-11-28 22:48:40,086 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:33360 => 127.0.0.1:58789] is created: PID: 133695\n",
      "2023-11-28 22:48:40,086 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00008 127.0.0.1:58789 <= 127.0.0.1:33360] is created: PID: 133663\n",
      "2023-11-28 22:48:42,616 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-11-28 22:48:42,654 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-11-28 22:48:42,679 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-11-28 22:48:43,121 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-11-28 22:48:43,130 - ClientRunner - INFO - [identity=site-3, run=simulate_job]: synced to Server Runner in 0.5093259811401367 seconds\n",
      "2023-11-28 22:48:43,130 - ClientRunner - INFO - [identity=site-3, run=simulate_job]: client runner started\n",
      "2023-11-28 22:48:43,130 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-3\n",
      "2023-11-28 22:48:43,133 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: assigned task to client site-3: name=train, id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476\n",
      "2023-11-28 22:48:43,134 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: sent task assignment to client. client_name:site-3 task_id:41a3bde1-3508-4cc9-b2b4-44e8b0ef3476\n",
      "2023-11-28 22:48:43,134 - GetTaskCommand - INFO - return task to client.  client_name: site-3  task_name: train   task_id: 41a3bde1-3508-4cc9-b2b4-44e8b0ef3476  sharable_header_task_id: 41a3bde1-3508-4cc9-b2b4-44e8b0ef3476\n",
      "2023-11-28 22:48:43,136 - Communicator - INFO - Received from simulator_server server. getTask: train size: 652B (652 Bytes) time: 0.005563 seconds\n",
      "2023-11-28 22:48:43,136 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-28 22:48:43,137 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476\n",
      "2023-11-28 22:48:43,137 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-28 22:48:43,137 - ClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: execute for task (train)\n",
      "2023-11-28 22:48:43,156 - ClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: External process for task (train) is launched.\n",
      "2023-11-28 22:48:43,159 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-11-28 22:48:43,163 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: synced to Server Runner in 0.5048258304595947 seconds\n",
      "2023-11-28 22:48:43,164 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: client runner started\n",
      "2023-11-28 22:48:43,164 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-2\n",
      "2023-11-28 22:48:43,166 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: assigned task to client site-2: name=train, id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b\n",
      "2023-11-28 22:48:43,166 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: sent task assignment to client. client_name:site-2 task_id:5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b\n",
      "2023-11-28 22:48:43,166 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: train   task_id: 5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b  sharable_header_task_id: 5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b\n",
      "2023-11-28 22:48:43,168 - Communicator - INFO - Received from simulator_server server. getTask: train size: 652B (652 Bytes) time: 0.003662 seconds\n",
      "2023-11-28 22:48:43,168 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-28 22:48:43,168 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b\n",
      "2023-11-28 22:48:43,168 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-28 22:48:43,168 - ClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: execute for task (train)\n",
      "2023-11-28 22:48:43,184 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-11-28 22:48:43,184 - ClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: External process for task (train) is launched.\n",
      "2023-11-28 22:48:43,188 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: synced to Server Runner in 0.5047235488891602 seconds\n",
      "2023-11-28 22:48:43,188 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: client runner started\n",
      "2023-11-28 22:48:43,189 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-1\n",
      "2023-11-28 22:48:43,191 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: assigned task to client site-1: name=train, id=98980ae7-f6b7-4807-8578-cd577b97add5\n",
      "2023-11-28 22:48:43,191 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: sent task assignment to client. client_name:site-1 task_id:98980ae7-f6b7-4807-8578-cd577b97add5\n",
      "2023-11-28 22:48:43,191 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: train   task_id: 98980ae7-f6b7-4807-8578-cd577b97add5  sharable_header_task_id: 98980ae7-f6b7-4807-8578-cd577b97add5\n",
      "2023-11-28 22:48:43,193 - Communicator - INFO - Received from simulator_server server. getTask: train size: 652B (652 Bytes) time: 0.003860 seconds\n",
      "2023-11-28 22:48:43,193 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-28 22:48:43,193 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=98980ae7-f6b7-4807-8578-cd577b97add5\n",
      "2023-11-28 22:48:43,193 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-28 22:48:43,193 - ClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: execute for task (train)\n",
      "2023-11-28 22:48:43,219 - ClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: External process for task (train) is launched.\n",
      "current_round=0\n",
      "[0]\ttrain-auc:0.76632\ttest-auc:0.76160\n",
      "2023-11-28 22:49:19,894 - ClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: got result 'Message(topic=train, msg_id=6c2cab70-9f63-45fd-bf5b-1dd0a8336ca0, req_id=fd5d7520-0444-44b4-a8bc-d28d8bd65420, msg_type=REP)' for task 'train'\n",
      "2023-11-28 22:49:19,894 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: finished processing task\n",
      "2023-11-28 22:49:19,895 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: try #1: sending task result to server\n",
      "2023-11-28 22:49:19,895 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: checking task ...\n",
      "2023-11-28 22:49:19,895 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-28 22:49:19,903 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: start to send task result to server\n",
      "2023-11-28 22:49:19,903 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-28 22:49:19,905 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=train, id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b\n",
      "2023-11-28 22:49:19,906 - XGBBaggingAggregator - ERROR - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: expected XGB_MODEL but got WEIGHTS\n",
      "2023-11-28 22:49:19,906 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: Contribution from site-2 REJECTED by the aggregator at round 0.\n",
      "2023-11-28 22:49:19,906 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: finished processing client result by scatter_and_gather\n",
      "2023-11-28 22:49:19,906 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b\n",
      "2023-11-28 22:49:19,908 - Communicator - INFO -  SubmitUpdate size: 33KB (32999 Bytes). time: 0.004422 seconds\n",
      "2023-11-28 22:49:19,908 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e7132c3-1d1f-4e01-b5a6-ad2cce57d68b]: task result sent to server\n",
      "2023-11-28 22:49:19,908 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 0.05 task_processed: True\n",
      "current_round=0\n",
      "current_round=0\n",
      "[0]\ttrain-auc:0.76565\ttest-auc:0.76323\n",
      "2023-11-28 22:49:21,399 - ClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: got result 'Message(topic=train, msg_id=0d1a2a9d-8084-43a2-99f7-398227685347, req_id=22ba7727-707f-4b72-8dc3-8820b4bfb4ef, msg_type=REP)' for task 'train'\n",
      "2023-11-28 22:49:21,400 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: finished processing task\n",
      "2023-11-28 22:49:21,400 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: try #1: sending task result to server\n",
      "2023-11-28 22:49:21,400 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: checking task ...\n",
      "2023-11-28 22:49:21,401 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-28 22:49:21,411 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: start to send task result to server\n",
      "2023-11-28 22:49:21,411 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-28 22:49:21,417 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job]: got result from client site-3 for task: name=train, id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476\n",
      "2023-11-28 22:49:21,417 - XGBBaggingAggregator - ERROR - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: expected XGB_MODEL but got WEIGHTS\n",
      "2023-11-28 22:49:21,418 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: Contribution from site-3 REJECTED by the aggregator at round 0.\n",
      "2023-11-28 22:49:21,418 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: finished processing client result by scatter_and_gather\n",
      "2023-11-28 22:49:21,418 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-3   task_id:41a3bde1-3508-4cc9-b2b4-44e8b0ef3476\n",
      "2023-11-28 22:49:21,419 - Communicator - INFO -  SubmitUpdate size: 33.1KB (33147 Bytes). time: 0.008133 seconds\n",
      "2023-11-28 22:49:21,419 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=41a3bde1-3508-4cc9-b2b4-44e8b0ef3476]: task result sent to server\n",
      "2023-11-28 22:49:21,419 - ClientTaskWorker - INFO - Finished one task run for client: site-3 interval: 0.05 task_processed: True\n",
      "[0]\ttrain-auc:0.76459\ttest-auc:0.76176\n",
      "2023-11-28 22:49:21,541 - ClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: got result 'Message(topic=train, msg_id=d5b548a1-a3e2-4b18-b037-3e0604e4b36d, req_id=227f1fc5-19b0-4beb-8bb5-fdd0a8b37956, msg_type=REP)' for task 'train'\n",
      "2023-11-28 22:49:21,542 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: finished processing task\n",
      "2023-11-28 22:49:21,542 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: try #1: sending task result to server\n",
      "2023-11-28 22:49:21,542 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: checking task ...\n",
      "2023-11-28 22:49:21,543 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-28 22:49:21,550 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: start to send task result to server\n",
      "2023-11-28 22:49:21,550 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-28 22:49:21,551 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=train, id=98980ae7-f6b7-4807-8578-cd577b97add5\n",
      "2023-11-28 22:49:21,552 - XGBBaggingAggregator - ERROR - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: expected XGB_MODEL but got WEIGHTS\n",
      "2023-11-28 22:49:21,552 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: Contribution from site-1 REJECTED by the aggregator at round 0.\n",
      "2023-11-28 22:49:21,552 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: finished processing client result by scatter_and_gather\n",
      "2023-11-28 22:49:21,553 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:98980ae7-f6b7-4807-8578-cd577b97add5\n",
      "2023-11-28 22:49:21,553 - Communicator - INFO -  SubmitUpdate size: 33.3KB (33276 Bytes). time: 0.003368 seconds\n",
      "2023-11-28 22:49:21,553 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=98980ae7-f6b7-4807-8578-cd577b97add5]: task result sent to server\n",
      "2023-11-28 22:49:21,554 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 0.05 task_processed: True\n",
      "2023-11-28 22:49:22,052 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: task train exit with status TaskCompletionStatus.OK\n",
      "2023-11-28 22:49:22,539 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Start aggregation.\n",
      "2023-11-28 22:49:22,539 - XGBBaggingAggregator - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: aggregating 0 update(s) at round 0\n",
      "2023-11-28 22:49:22,542 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: End aggregation.\n",
      "2023-11-28 22:49:22,543 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Start persist model on server.\n",
      "2023-11-28 22:49:22,544 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: End persist model on server.\n",
      "2023-11-28 22:49:22,544 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 0 finished.\n",
      "2023-11-28 22:49:22,545 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 1 started.\n",
      "2023-11-28 22:49:22,545 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: scheduled task train\n",
      "2023-11-28 22:49:22,594 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: assigned task to client site-2: name=train, id=9a6a02f3-103a-4ef7-8406-d6af12187099\n",
      "2023-11-28 22:49:22,595 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: sent task assignment to client. client_name:site-2 task_id:9a6a02f3-103a-4ef7-8406-d6af12187099\n",
      "2023-11-28 22:49:22,595 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: train   task_id: 9a6a02f3-103a-4ef7-8406-d6af12187099  sharable_header_task_id: 9a6a02f3-103a-4ef7-8406-d6af12187099\n",
      "2023-11-28 22:49:22,598 - Communicator - INFO - Received from simulator_server server. getTask: train size: 683B (683 Bytes) time: 0.007387 seconds\n",
      "2023-11-28 22:49:22,599 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-28 22:49:22,599 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=9a6a02f3-103a-4ef7-8406-d6af12187099\n",
      "2023-11-28 22:49:22,600 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-28 22:49:22,600 - ClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: execute for task (train)\n",
      "2023-11-28 22:49:22,617 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: assigned task to client site-1: name=train, id=94db53a8-a59d-47f6-9416-fc7b696046be\n",
      "2023-11-28 22:49:22,618 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: sent task assignment to client. client_name:site-1 task_id:94db53a8-a59d-47f6-9416-fc7b696046be\n",
      "2023-11-28 22:49:22,619 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: train   task_id: 94db53a8-a59d-47f6-9416-fc7b696046be  sharable_header_task_id: 94db53a8-a59d-47f6-9416-fc7b696046be\n",
      "2023-11-28 22:49:22,621 - Communicator - INFO - Received from simulator_server server. getTask: train size: 683B (683 Bytes) time: 0.007126 seconds\n",
      "2023-11-28 22:49:22,622 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-28 22:49:22,622 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=94db53a8-a59d-47f6-9416-fc7b696046be\n",
      "2023-11-28 22:49:22,623 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-28 22:49:22,623 - ClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: execute for task (train)\n",
      "current_round=1\n",
      "Traceback (most recent call last):\n",
      "  File \"custom/xgboost_fl.py\", line 213, in <module>\n",
      "    main()\n",
      "  File \"custom/xgboost_fl.py\", line 147, in main\n",
      "    model_updates = global_params[\"model_data\"]\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "current_round=1\n",
      "Traceback (most recent call last):\n",
      "  File \"custom/xgboost_fl.py\", line 213, in <module>\n",
      "    main()\n",
      "  File \"custom/xgboost_fl.py\", line 147, in main\n",
      "    model_updates = global_params[\"model_data\"]\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "2023-11-28 22:49:23,048 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=428b4b00-185e-4833-a213-74cf8e25555a]: assigned task to client site-3: name=train, id=428b4b00-185e-4833-a213-74cf8e25555a\n",
      "2023-11-28 22:49:23,048 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=428b4b00-185e-4833-a213-74cf8e25555a]: sent task assignment to client. client_name:site-3 task_id:428b4b00-185e-4833-a213-74cf8e25555a\n",
      "2023-11-28 22:49:23,049 - GetTaskCommand - INFO - return task to client.  client_name: site-3  task_name: train   task_id: 428b4b00-185e-4833-a213-74cf8e25555a  sharable_header_task_id: 428b4b00-185e-4833-a213-74cf8e25555a\n",
      "2023-11-28 22:49:23,051 - Communicator - INFO - Received from simulator_server server. getTask: train size: 683B (683 Bytes) time: 0.006769 seconds\n",
      "2023-11-28 22:49:23,052 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-28 22:49:23,052 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=428b4b00-185e-4833-a213-74cf8e25555a\n",
      "2023-11-28 22:49:23,053 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=428b4b00-185e-4833-a213-74cf8e25555a]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-28 22:49:23,053 - ClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=428b4b00-185e-4833-a213-74cf8e25555a]: execute for task (train)\n",
      "current_round=1\n",
      "Traceback (most recent call last):\n",
      "  File \"custom/xgboost_fl.py\", line 213, in <module>\n",
      "    main()\n",
      "  File \"custom/xgboost_fl.py\", line 147, in main\n",
      "    model_updates = global_params[\"model_data\"]\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "2023-11-28 22:50:21,426 - PipeHandler - ERROR - read timeout after 60 secs\n",
      "2023-11-28 22:50:21,427 - ClientAPILauncherExecutor - ERROR - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: received reply: 'Message(topic=_PEER_GONE_, msg_id=651b24e8-40b7-444d-aec4-a8993644e64d, req_id=None, msg_type=REQ)' while waiting for the result of train\n",
      "2023-11-28 22:50:21,428 - ClientAPILauncherExecutor - ERROR - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: missing result FLModel for train_task: train.\n",
      "2023-11-28 22:50:21,429 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: finished processing task\n",
      "2023-11-28 22:50:21,429 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: try #1: sending task result to server\n",
      "2023-11-28 22:50:21,429 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: checking task ...\n",
      "2023-11-28 22:50:21,430 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-28 22:50:21,436 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: start to send task result to server\n",
      "2023-11-28 22:50:21,436 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-28 22:50:21,439 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=train, id=9a6a02f3-103a-4ef7-8406-d6af12187099\n",
      "2023-11-28 22:50:21,440 - ServerRunner - ERROR - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: Aborting current RUN due to FATAL_SYSTEM_ERROR received: Result from site-2 is bad, error code: EXECUTION_EXCEPTION. ScatterAndGather exiting at round 1.\n",
      "2023-11-28 22:50:21,441 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: asked to abort - triggered abort_signal to stop the RUN\n",
      "2023-11-28 22:50:21,441 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: finished processing client result by scatter_and_gather\n",
      "2023-11-28 22:50:21,441 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:9a6a02f3-103a-4ef7-8406-d6af12187099\n",
      "2023-11-28 22:50:21,443 - Communicator - INFO -  SubmitUpdate size: 567B (567 Bytes). time: 0.006974 seconds\n",
      "2023-11-28 22:50:21,444 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=9a6a02f3-103a-4ef7-8406-d6af12187099]: task result sent to server\n",
      "2023-11-28 22:50:21,444 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 0.05 task_processed: True\n",
      "2023-11-28 22:50:21,498 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2023-11-28 22:50:21,499 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2023-11-28 22:50:21,502 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2023-11-28 22:50:21,502 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2023-11-28 22:50:21,502 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2023-11-28 22:50:21,503 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-2 \n",
      "2023-11-28 22:50:21,505 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00007 Not Connected] is closed PID: 133663\n",
      "2023-11-28 22:50:21,505 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 133694\n",
      "2023-11-28 22:50:21,543 - PipeHandler - ERROR - read timeout after 60 secs\n",
      "2023-11-28 22:50:21,544 - ClientAPILauncherExecutor - ERROR - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: received reply: 'Message(topic=_PEER_GONE_, msg_id=572a9788-25d1-443a-80dc-37fdddc169e7, req_id=None, msg_type=REQ)' while waiting for the result of train\n",
      "2023-11-28 22:50:21,545 - ClientAPILauncherExecutor - ERROR - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: missing result FLModel for train_task: train.\n",
      "2023-11-28 22:50:21,546 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: finished processing task\n",
      "2023-11-28 22:50:21,546 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: try #1: sending task result to server\n",
      "2023-11-28 22:50:21,546 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: checking task ...\n",
      "2023-11-28 22:50:21,546 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-28 22:50:21,552 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: start to send task result to server\n",
      "2023-11-28 22:50:21,552 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-28 22:50:21,554 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=train, id=94db53a8-a59d-47f6-9416-fc7b696046be\n",
      "2023-11-28 22:50:21,555 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: ignored result submission since server runner's status is done\n",
      "2023-11-28 22:50:21,555 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:94db53a8-a59d-47f6-9416-fc7b696046be\n",
      "2023-11-28 22:50:21,557 - Communicator - INFO -  SubmitUpdate size: 567B (567 Bytes). time: 0.004917 seconds\n",
      "2023-11-28 22:50:21,557 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=94db53a8-a59d-47f6-9416-fc7b696046be]: task result sent to server\n",
      "2023-11-28 22:50:21,558 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 0.05 task_processed: True\n",
      "2023-11-28 22:50:21,603 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Abort signal received. Exiting at round 1.\n",
      "2023-11-28 22:50:21,603 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Workflow: scatter_and_gather finalizing ...\n",
      "2023-11-28 22:50:21,611 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2023-11-28 22:50:21,611 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2023-11-28 22:50:21,612 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2023-11-28 22:50:21,613 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2023-11-28 22:50:21,613 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2023-11-28 22:50:21,613 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-1 \n",
      "2023-11-28 22:50:21,614 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 Not Connected] is closed PID: 133663\n",
      "2023-11-28 22:50:21,614 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 133693\n",
      "2023-11-28 22:50:21,629 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: ABOUT_TO_END_RUN fired\n",
      "2023-11-28 22:50:21,630 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Firing CHECK_END_RUN_READINESS ...\n",
      "2023-11-28 22:50:21,630 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: END_RUN fired\n",
      "2023-11-28 22:50:21,630 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Server runner finished.\n",
      "2023-11-28 22:50:21,978 - PipeHandler - ERROR - read timeout after 60 secs\n",
      "2023-11-28 22:50:21,979 - ClientAPILauncherExecutor - ERROR - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=428b4b00-185e-4833-a213-74cf8e25555a]: received reply: 'Message(topic=_PEER_GONE_, msg_id=037c3874-cd32-4495-a48a-c49efe923d02, req_id=None, msg_type=REQ)' while waiting for the result of train\n",
      "2023-11-28 22:50:21,980 - ClientAPILauncherExecutor - ERROR - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=428b4b00-185e-4833-a213-74cf8e25555a]: missing result FLModel for train_task: train.\n",
      "2023-11-28 22:50:21,981 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=428b4b00-185e-4833-a213-74cf8e25555a]: finished processing task\n",
      "2023-11-28 22:50:21,981 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=428b4b00-185e-4833-a213-74cf8e25555a]: try #1: sending task result to server\n",
      "2023-11-28 22:50:21,982 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=428b4b00-185e-4833-a213-74cf8e25555a]: checking task ...\n",
      "2023-11-28 22:50:21,982 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-28 22:50:21,986 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job]: no current workflow - dropped task_check.\n",
      "2023-11-28 22:50:21,989 - ClientRunner - ERROR - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=428b4b00-185e-4833-a213-74cf8e25555a]: task no longer exists on server: TASK_UNKNOWN\n",
      "2023-11-28 22:50:21,990 - ClientTaskWorker - INFO - Finished one task run for client: site-3 interval: 0.05 task_processed: True\n",
      "2023-11-28 22:50:22,043 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2023-11-28 22:50:22,044 - GetTaskCommand - INFO - return task to client.  client_name: site-3  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2023-11-28 22:50:22,046 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2023-11-28 22:50:22,046 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2023-11-28 22:50:22,046 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2023-11-28 22:50:22,047 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-3 \n",
      "2023-11-28 22:50:22,047 - FederatedClient - INFO - Shutting down client run: site-1\n",
      "2023-11-28 22:50:22,047 - FederatedClient - INFO - Shutting down client run: site-2\n",
      "2023-11-28 22:50:22,048 - FederatedClient - INFO - Shutting down client run: site-3\n",
      "2023-11-28 22:50:22,049 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: asked to abort - triggered abort_signal to stop the RUN\n",
      "2023-11-28 22:50:22,049 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00008 Not Connected] is closed PID: 133663\n",
      "2023-11-28 22:50:22,050 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 133695\n",
      "2023-11-28 22:50:22,081 - SimulatorServer - INFO - Server app stopped.\n",
      "\n",
      "\n",
      "2023-11-28 22:50:22,100 - nvflare.fuel.hci.server.hci - INFO - Admin Server localhost on Port 54865 shutdown!\n",
      "2023-11-28 22:50:22,100 - SimulatorServer - INFO - shutting down server\n",
      "2023-11-28 22:50:22,100 - SimulatorServer - INFO - canceling sync locks\n",
      "2023-11-28 22:50:22,100 - SimulatorServer - INFO - server off\n",
      "2023-11-28 22:50:25,600 - MPM - WARNING - #### MPM: still running thread Thread-9\n",
      "2023-11-28 22:50:25,600 - MPM - INFO - MPM: Good Bye!\n"
     ]
    }
   ],
   "source": [
    "!nvflare simulator /tmp/nvflare/jobs/xgboost -w /tmp/nvflare/xgboost -n 3 -t 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe301fa-8f51-4d3f-b61e-fd851b9f92d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's examine the results.\n",
    "\n",
    "We can notice from the FL training log, at the last round of local training, site-1 reports `site-1: local model AUC: 0.6351`\n",
    "Now let's run a local training to verify if this number makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba843de-96ed-4c41-a5c1-d770d604a1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ./code/sgd_local_iter.py --data_root_dir /tmp/nvflare/dataset/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009591f-83c4-4c84-be79-176573296a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ./code/sgd_local_oneshot.py --data_root_dir /tmp/nvflare/dataset/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bbacc-b059-4f82-9785-2b22bf840ef9",
   "metadata": {},
   "source": [
    "In this experiment, all three clients have relatively large amount data wiht homogeneous distribution, we would expect the three numbers align within reasonable variation range. \n",
    "\n",
    "The final result for iterative learning is `ending model AUC: 0.6352`, and one-shot learning is `local model AUC: 0.6355`, as compared with FL's `local model AUC: 0.6351`, the numbers do align.\n",
    "\n",
    "## We are done !\n",
    "Congratulations! you have just completed the federated linear model for tabular data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5329a14-1a81-488a-8edc-2b7c85cffd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
