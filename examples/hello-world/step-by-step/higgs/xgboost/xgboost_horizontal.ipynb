{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ccbfaee-9efe-4fe1-bab6-60462769fede",
   "metadata": {},
   "source": [
    "# Federated Horizontal XGBoost with Tree-based Collaboration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ea300-8fd1-4568-8121-9bb9d3d303b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "This tutorial illustrates a federated horizontal xgboost learning on tabular data with bagging collaboration. \n",
    "\n",
    "Before do the training, we need to setup NVFLARE\n",
    "\n",
    "## Setup NVFLARE\n",
    "\n",
    "Follow [Getting Started](https://nvflare.readthedocs.io/en/main/getting_started.html) to set up a virtual environment and install NVFLARE.\n",
    "\n",
    "You can also follow this [notebook](https://github.com/NVIDIA/NVFlare/blob/main/examples/nvflare_setup.ipynb) to get set up.\n",
    "\n",
    "> Make sure you have installed nvflare from **terminal** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c1eb8-3070-45b8-a83c-f16d060d0385",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install requirements\n",
    "assuming the current directory is '/examples/hello-world/step-by-step/higgs/xgboost'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3f5a8f-cd09-441e-a4c3-f1a121fd8244",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ziyuex/NVFlare/nvflare_tab_exp/examples/hello-world/step-by-step/higgs/xgboost\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a7bf5e9-1f21-430a-9227-3e912eba5f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas (from -r requirements.txt (line 1))\n",
      "  Downloading pandas-2.1.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Collecting scikit-learn (from -r requirements.txt (line 2))\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting xgboost (from -r requirements.txt (line 3))\n",
      "  Downloading xgboost-2.0.2-py3-none-macosx_12_0_arm64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/ziyuex/anaconda3/envs/nvflare_py310/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 1)) (1.26.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ziyuex/anaconda3/envs/nvflare_py310/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas->-r requirements.txt (line 1))\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.5.0 (from scikit-learn->-r requirements.txt (line 2))\n",
      "  Downloading scipy-1.11.4-cp310-cp310-macosx_12_0_arm64.whl.metadata (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn->-r requirements.txt (line 2))\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn->-r requirements.txt (line 2))\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ziyuex/anaconda3/envs/nvflare_py310/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Downloading pandas-2.1.3-cp310-cp310-macosx_11_0_arm64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp310-cp310-macosx_12_0_arm64.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xgboost-2.0.2-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.11.4-cp310-cp310-macosx_12_0_arm64.whl (29.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.8/29.8 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pytz, tzdata, threadpoolctl, scipy, joblib, xgboost, scikit-learn, pandas\n",
      "Successfully installed joblib-1.3.2 pandas-2.1.3 pytz-2023.3.post1 scikit-learn-1.3.2 scipy-1.11.4 threadpoolctl-3.2.0 tzdata-2023.3 xgboost-2.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87c3da-59dc-4551-9448-b20b64a57137",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare data\n",
    "Please reference [prepare_higgs_data](../prepare_data.ipynb) notebooks. Pay attention to the current location. You need to switch \"higgs\" directory to run the data split.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe5bc9a-d589-4d48-89b8-73fc1d8fe44b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we have our data prepared. we are ready to do the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc62f2-5571-44d0-bb8b-b6a660a500c5",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "\n",
    "We noticed from time-to-time the Higgs dataset is making small changes which causing job to fail. so we need to do some clean up or skip certain rows. \n",
    "For example: certain floating number mistakenly add an alphabetical letter at some point of time. This may have already fixed by UCI. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dbec4e-b4a9-44fe-954e-62f1d0f99c40",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "This tutorial uses [XGBoost](https://github.com/dmlc/xgboost), which is an optimized distributed gradient boosting library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80eb797-f1aa-4537-ab50-4c27fb9cff60",
   "metadata": {},
   "source": [
    "### Federated XGBoost Model\n",
    "Here we use tree-based collaboration for horizontal federated XGBoost.\n",
    "\n",
    "Under this setting, individual trees are independently trained on each client's local data without aggregating the global sample gradient histogram information.\n",
    "Trained trees are collected and passed to the server / other clients for bagging aggregation and further boosting rounds.\n",
    "\n",
    "The XGBoost Booster api is leveraged to create in-memory Booster objects that persist across rounds to cache predictions from trees added in previous rounds and retain other data structures needed for training.\n",
    "\n",
    "Let's look at the code see how we convert the local training script to the federated training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70b5d82b-3c5a-492a-bc92-86ca7367ff92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ziyuex/NVFlare/nvflare_tab_exp/examples/hello-world/step-by-step/higgs/xgboost\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fdbcc96-3ce0-4e9e-a986-f877e082c0ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) 2023, NVIDIA CORPORATION.  All rights reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import argparse\n",
      "import csv\n",
      "import json\n",
      "from typing import Dict, List, Tuple\n",
      "\n",
      "import pandas as pd\n",
      "import xgboost as xgb\n",
      "from sklearn.metrics import roc_auc_score\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# (1) import nvflare client API\n",
      "from nvflare import client as flare\n",
      "from nvflare.app_opt.xgboost.tree_based.shareable_generator import update_model\n",
      "\n",
      "\n",
      "def to_dataset_tuple(data: dict):\n",
      "    dataset_tuples = {}\n",
      "    for dataset_name, dataset in data.items():\n",
      "        dataset_tuples[dataset_name] = _to_data_tuple(dataset)\n",
      "    return dataset_tuples\n",
      "\n",
      "\n",
      "def _to_data_tuple(data):\n",
      "    data_num = data.shape[0]\n",
      "    # split to feature and label\n",
      "    x = data.iloc[:, 1:]\n",
      "    y = data.iloc[:, 0]\n",
      "    return x.to_numpy(), y.to_numpy(), data_num\n",
      "\n",
      "\n",
      "def load_features(feature_data_path: str) -> List:\n",
      "    try:\n",
      "        features = []\n",
      "        with open(feature_data_path, \"r\") as file:\n",
      "            # Create a CSV reader object\n",
      "            csv_reader = csv.reader(file)\n",
      "            line_list = next(csv_reader)\n",
      "            features = line_list\n",
      "        return features\n",
      "    except Exception as e:\n",
      "        raise Exception(f\"Load header for path'{feature_data_path} failed! {e}\")\n",
      "\n",
      "\n",
      "def load_data(\n",
      "    data_path: str, data_features: List, random_state: int, test_size: float, skip_rows=None\n",
      ") -> Dict[str, pd.DataFrame]:\n",
      "    try:\n",
      "        df: pd.DataFrame = pd.read_csv(\n",
      "            data_path, names=data_features, sep=r\"\\s*,\\s*\", engine=\"python\", na_values=\"?\", skiprows=skip_rows\n",
      "        )\n",
      "\n",
      "        train, test = train_test_split(df, test_size=test_size, random_state=random_state)\n",
      "\n",
      "        return {\"train\": train, \"test\": test}\n",
      "\n",
      "    except Exception as e:\n",
      "        raise Exception(f\"Load data for path '{data_path}' failed! {e}\")\n",
      "\n",
      "\n",
      "def transform_data(data: Dict[str, Tuple]) -> Dict[str, Tuple]:\n",
      "    # Standardize features by removing the mean and scaling to unit variance\n",
      "    scaler = StandardScaler()\n",
      "    scaled_datasets = {}\n",
      "    for dataset_name, (x_data, y_data, data_num) in data.items():\n",
      "        x_scaled = scaler.fit_transform(x_data)\n",
      "        scaled_datasets[dataset_name] = (x_scaled, y_data, data_num)\n",
      "    return scaled_datasets\n",
      "\n",
      "\n",
      "def main():\n",
      "    parser = define_args_parser()\n",
      "    args = parser.parse_args()\n",
      "    data_root_dir = args.data_root_dir\n",
      "    random_state = args.random_state\n",
      "    test_size = args.test_size\n",
      "    skip_rows = args.skip_rows\n",
      "    num_client_bagging = args.num_client_bagging\n",
      "\n",
      "    # (2) initializes NVFlare client API\n",
      "    flare.init()\n",
      "\n",
      "    site_name = flare.get_site_name()\n",
      "    feature_data_path = f\"{data_root_dir}/{site_name}_header.csv\"\n",
      "    features = load_features(feature_data_path)\n",
      "    n_features = len(features) - 1  # remove label\n",
      "\n",
      "    data_path = f\"{data_root_dir}/{site_name}.csv\"\n",
      "    data = load_data(\n",
      "        data_path=data_path, data_features=features, random_state=random_state, test_size=test_size, skip_rows=skip_rows\n",
      "    )\n",
      "\n",
      "    data = to_dataset_tuple(data)\n",
      "    dataset = transform_data(data)\n",
      "    x_train, y_train, train_size = dataset[\"train\"]\n",
      "    x_test, y_test, test_size = dataset[\"test\"]\n",
      "\n",
      "    # convert to xgboost data matrix\n",
      "    dmat_train = xgb.DMatrix(x_train, label=y_train)\n",
      "    dmat_test = xgb.DMatrix(x_test, label=y_test)\n",
      "\n",
      "    xgb_params = {\n",
      "        \"eta\": 0.1 / num_client_bagging,\n",
      "        \"objective\": \"binary:logistic\",\n",
      "        \"max_depth\": 8,\n",
      "        \"eval_metric\": \"auc\",\n",
      "        \"nthread\": 16,\n",
      "        \"num_parallel_tree\": 1,\n",
      "        \"subsample\": 1.0,\n",
      "        \"tree_method\": \"hist\",\n",
      "    }\n",
      "\n",
      "    global_model_as_dict = None\n",
      "    auc = 0.5\n",
      "    while flare.is_running():\n",
      "        # (3) receives FLModel from NVFlare\n",
      "        input_model = flare.receive()\n",
      "        global_params = input_model.params\n",
      "        curr_round = input_model.current_round\n",
      "\n",
      "        print(f\"current_round={curr_round}\")\n",
      "        if curr_round == 0:\n",
      "            # (4) first round, no global model\n",
      "            model = xgb.train(\n",
      "                xgb_params,\n",
      "                dmat_train,\n",
      "                num_boost_round=1,\n",
      "                evals=[(dmat_train, \"train\"), (dmat_test, \"test\")],\n",
      "            )\n",
      "            config = model.save_config()\n",
      "        else:\n",
      "            # (5) update model based on global updates\n",
      "            model_updates = global_params[\"model_data\"]\n",
      "            for update in model_updates:\n",
      "                global_model_as_dict = update_model(global_model_as_dict, json.loads(update))\n",
      "            loadable_model = bytearray(json.dumps(global_model_as_dict), \"utf-8\")\n",
      "            # load model\n",
      "            model.load_model(loadable_model)\n",
      "            model.load_config(config)\n",
      "\n",
      "            # (6) evaluate model\n",
      "            auc = evaluate_model(x_test, model, y_test)\n",
      "            # Print the results\n",
      "            print(f\"{site_name}: global model AUC: {auc:.5f}\")\n",
      "\n",
      "            # train model in two steps\n",
      "            # first, eval on train and test\n",
      "            eval_results = model.eval_set(\n",
      "                evals=[(dmat_train, \"train\"), (dmat_test, \"test\")], iteration=model.num_boosted_rounds() - 1\n",
      "            )\n",
      "            print(eval_results)\n",
      "            # second, train for one round\n",
      "            model.update(dmat_train, model.num_boosted_rounds())\n",
      "\n",
      "        # (7) construct trained FL model\n",
      "        # Extract newly added tree using xgboost_bagging slicing api\n",
      "        bst_new = model[model.num_boosted_rounds() - 1 : model.num_boosted_rounds()]\n",
      "        local_model_update = bst_new.save_raw(\"json\")\n",
      "        params = {\"model_data\": local_model_update}\n",
      "        metrics = {\"accuracy\": auc}\n",
      "        output_model = flare.FLModel(params=params, metrics=metrics)\n",
      "\n",
      "        # (8) send model back to NVFlare\n",
      "        flare.send(output_model)\n",
      "\n",
      "\n",
      "def evaluate_model(x_test, model, y_test):\n",
      "    # Make predictions on the testing set\n",
      "    dtest = xgb.DMatrix(x_test)\n",
      "    y_pred = model.predict(dtest)\n",
      "\n",
      "    # Evaluate the model\n",
      "    auc = roc_auc_score(y_test, y_pred)\n",
      "    return auc\n",
      "\n",
      "\n",
      "def define_args_parser():\n",
      "    parser = argparse.ArgumentParser(description=\"scikit learn linear model with SGD\")\n",
      "    parser.add_argument(\"--data_root_dir\", type=str, help=\"root directory path to csv data file\")\n",
      "    parser.add_argument(\"--random_state\", type=int, default=0, help=\"random state\")\n",
      "    parser.add_argument(\"--test_size\", type=float, default=0.2, help=\"test ratio, default to 20%\")\n",
      "    parser.add_argument(\n",
      "        \"--num_client_bagging\",\n",
      "        type=int,\n",
      "        default=3,\n",
      "        help=\"number of clients with uniform data sizes participating in bagging\",\n",
      "    )\n",
      "    parser.add_argument(\n",
      "        \"--skip_rows\",\n",
      "        type=str,\n",
      "        default=None,\n",
      "        help=\"\"\"If skip_rows = N, the first N rows will be skipped, \n",
      "       if skiprows=[0, 1, 4], the rows will be skip by row indices such as row 0,1,4 will be skipped. \"\"\",\n",
      "    )\n",
      "    return parser\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "!cat code/xgboost_fl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d932e8c-7c7c-470b-8448-43288feba261",
   "metadata": {
    "tags": []
   },
   "source": [
    "The code is pretty much like the standard scikit-learn training script of `code/xgboost_local_iter.py`\n",
    "\n",
    "#### load data\n",
    "\n",
    "We first load the features from the header file: \n",
    "    \n",
    "```\n",
    "    site_name = flare.get_site_name()\n",
    "    feature_data_path = f\"{data_root_dir}/{site_name}_header.csv\"\n",
    "    features = load_features(feature_data_path)\n",
    "    n_features = len(features) -1\n",
    "\n",
    "    data_path = f\"{data_root_dir}/{site_name}.csv\"\n",
    "    data = load_data(data_path=data_path, data_features=features, test_size=test_size, skip_rows=skip_rows)\n",
    "\n",
    "```\n",
    "\n",
    "then load the data from the main csv file, then transform the data and split the training and test data based on the test_size provided.  \n",
    "\n",
    "```\n",
    "    data = to_dataset_tuple(data)\n",
    "    dataset = transform_data(data)\n",
    "    x_train, y_train, train_size = dataset[\"train\"]\n",
    "    x_test, y_test, test_size = dataset[\"test\"]\n",
    "\n",
    "```\n",
    "\n",
    "The part that's specific to Federated Learning is in the following codes\n",
    "\n",
    "```\n",
    "# (1) import nvflare client API\n",
    "from nvflare import client as flare\n",
    "\n",
    "```\n",
    "```\n",
    "# (2) initializes NVFlare client API\n",
    "    flare.init()\n",
    "\n",
    "    site_name = flare.get_site_name()\n",
    "    \n",
    "```\n",
    "    \n",
    "These few lines, import NVFLARE Client API and initialize it, then use the API to find the site_name (such as site-1, site-2 etc.). With the site-name, we can construct the site-specific \n",
    "data path such as\n",
    "\n",
    "```\n",
    "    feature_data_path = f\"{data_root_dir}/{site_name}_header.csv\"\n",
    "\n",
    "    data_path = f\"{data_root_dir}/{site_name}.csv\"\n",
    "```\n",
    "\n",
    "#### Training \n",
    "\n",
    "In the standard traditional xgboost, we would train the model such as\n",
    "```\n",
    "  model = xgb.train(...) \n",
    "```\n",
    "\n",
    "with federated learning, using FLARE Client API, we need to make a few changes\n",
    "* 1) we are not only training in local iterations, but also global rounds, we need to keep the program running until we reached to the totoal number of rounds \n",
    "  \n",
    "  ```\n",
    "      while flare.is_running():\n",
    "          ... rest of code\n",
    "  \n",
    "  ```\n",
    "  \n",
    "* 2) Unlike local learning, we have now have more than one clients/sites participating the training. To ensure every site starts with the same model parameters, we use server to broadcase the initial model parameters to every sites at the first round ( current_round = 0). \n",
    "\n",
    "* 3) We will need to use FLARE client API to receive global model updates \n",
    "\n",
    "```\n",
    "        # (3) receives FLModel from NVFlare\n",
    "        input_model = flare.receive()\n",
    "        global_params = input_model.params\n",
    "        curr_round = input_model.current_round\n",
    "```\n",
    "\n",
    "```\n",
    "        if curr_round == 0:\n",
    "            # (4) first round, no global model\n",
    "            model = xgb.train(\n",
    "                xgb_params,\n",
    "                dmat_train,\n",
    "                num_boost_round=1,\n",
    "                evals=[(dmat_train, \"train\"), (dmat_test, \"test\")],\n",
    "            )\n",
    "            config = model.save_config()\n",
    "        ....\n",
    "```\n",
    "* 4) if it is not the first round, we need to use the global model to update the local model before training the next round. \n",
    "\n",
    "```\n",
    "            # (5) update model based on global updates\n",
    "            model_updates = global_params[\"model_data\"]\n",
    "            for update in model_updates:\n",
    "                global_model_as_dict = update_model(global_model_as_dict, json.loads(update))\n",
    "            loadable_model = bytearray(json.dumps(global_model_as_dict), \"utf-8\")\n",
    "            # load model\n",
    "            model.load_model(loadable_model)\n",
    "            model.load_config(config)\n",
    "```\n",
    "\n",
    "* 5) we then evaluate the global model using the local data\n",
    "\n",
    "```\n",
    "            # (6) evaluate model\n",
    "            auc = evaluate_model(x_test, model, y_test)\n",
    "```\n",
    "* 6) finally we do the training \n",
    "\n",
    "```\n",
    "            # train model in two steps\n",
    "            # first, eval on train and test\n",
    "            eval_results = model.eval_set(\n",
    "                evals=[(dmat_train, \"train\"), (dmat_test, \"test\")], iteration=model.num_boosted_rounds() - 1\n",
    "            )\n",
    "            print(eval_results)\n",
    "            # second, train for one round\n",
    "            model.update(dmat_train, model.num_boosted_rounds())\n",
    "        \n",
    "```\n",
    "\n",
    "* 7) we need the new training result (new tree) back to server for aggregation, to do that, we have the following code\n",
    "\n",
    "```\n",
    "        # (7) construct trained FL model\n",
    "        # Extract newly added tree using xgboost_bagging slicing api\n",
    "        bst_new = model[model.num_boosted_rounds() - 1 : model.num_boosted_rounds()]\n",
    "        local_model_update = bst_new.save_raw(\"json\")\n",
    "        params = {\"model_data\": local_model_update}\n",
    "        metrics = {\"accuracy\": auc}\n",
    "        output_model = flare.FLModel(params=params, metrics=metrics)\n",
    "\n",
    "        # (8) send model back to NVFlare\n",
    "        flare.send(output_model)\n",
    "```\n",
    "\n",
    "## Prepare Job  \n",
    "\n",
    "Now, we have the code, we need to prepare job folder with configurations to run in NVFLARE. To do this, we can leveage the job template for scikit learn. First look at the the available job templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fdd962-e63c-4b58-81e7-beeedd05509b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvflare config -jt ../../../../../job_templates/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ed150d-4692-49fe-87a6-1779ec64d9df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following job templates are available: \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  name                 Description                                                  Controller Type      Client Category     \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  cyclic_cc_pt         client-controlled cyclic workflow with PyTorch ClientAPI tra client               client_api          \n",
      "  cyclic_pt            server-controlled cyclic workflow with PyTorch ClientAPI tra server               client_api          \n",
      "  psi_csv              private-set intersection for csv data                        server               Executor            \n",
      "  sag_cross_np         scatter & gather and cross-site validation using numpy       server               client executor     \n",
      "  sag_cse_pt           scatter & gather workflow and cross-site evaluation with PyT server               client_api          \n",
      "  sag_gnn              scatter & gather workflow for gnn learning                   server               client_api          \n",
      "  sag_nemo             Scatter and Gather Workflow for NeMo                         server               client_api          \n",
      "  sag_np               scatter & gather workflow using numpy                        server               client_api          \n",
      "  sag_pt               scatter & gather workflow using pytorch                      server               client_api          \n",
      "  sag_pt_deploy_map    SAG workflow with pytorch, deploy_map, site-specific configs server               client_api          \n",
      "  sag_pt_executor      scatter & gather workflow and cross-site evaluation with PyT server               Executor            \n",
      "  sag_pt_model_learner scatter & gather workflow and cross-site evaluation with PyT server               ModelLearner        \n",
      "  sag_tf               scatter & gather workflow using TensorFlow                   server               client_api          \n",
      "  sklearn_kmeans       scikit-learn KMeans model                                    server               client_api          \n",
      "  sklearn_linear       scikit-learn linear model                                    server               client_api          \n",
      "  sklearn_svm          scikit-learn SVM model                                       server               client_api          \n",
      "  stats_df             FedStats: tabular data with pandas                           server               stats executor      \n",
      "  stats_image          FedStats: image intensity histogram                          server               stats executor      \n",
      "  swarm_cse_pt         Swarm Learning with Cross-Site Evaluation with PyTorch       client               client_api          \n",
      "  swarm_cse_pt_model_l Swarm Learning with Cross-Site Evaluation with PyTorch Model client               ModelLearner        \n",
      "  vertical_xgb         vertical federated xgboost                                   server               Executor            \n",
      "  xgboost_tree         xgboost horizontal bagging model                             server               client_api          \n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!nvflare job list_templates"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b702033-6084-4002-8306-2d9aeab901b9",
   "metadata": {},
   "source": [
    "the `xgboost_tree` is the one we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9899a684-444f-4c04-a388-4bdd9b7dc649",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following are the variables you can change in the template\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                                                                                       \n",
      "  job folder: /tmp/nvflare/jobs/xgboost                                                                                                  \n",
      "                                                                                                                                       \n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "  file_name                      var_name                       value                               component                          \n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "  meta.conf                      app                            ['@ALL']                                                               \n",
      "\n",
      "  config_fed_client.conf         app_config                     --data_root_dir /tmp/nvflare/datase                                    \n",
      "  config_fed_client.conf         app_script                     xgboost_fl.py                                                          \n",
      "  config_fed_client.conf         file_check_interval            0.1                                 FilePipe                           \n",
      "  config_fed_client.conf         heartbeat_timeout              60                                                                     \n",
      "  config_fed_client.conf         launch_once                    True                                                                   \n",
      "  config_fed_client.conf         mode                           PASSIVE                             FilePipe                           \n",
      "  config_fed_client.conf         params_exchange_format         raw                                                                    \n",
      "  config_fed_client.conf         params_transfer_type           FULL                                                                   \n",
      "  config_fed_client.conf         root_path                                                          FilePipe                           \n",
      "  config_fed_client.conf         script                         python3 custom/{app_script}  {app_c SubprocessLauncher                 \n",
      "  config_fed_client.conf         train_with_evaluation          True                                                                   \n",
      "\n",
      "  config_fed_server.conf         allow_empty_global_weights     True                                                                   \n",
      "  config_fed_server.conf         load_as_dict                   True                                XGBModelPersistor                  \n",
      "  config_fed_server.conf         min_clients                    3                                                                      \n",
      "  config_fed_server.conf         num_rounds                     101                                                                    \n",
      "  config_fed_server.conf         save_name                      xgboost_model.json                  XGBModelPersistor                  \n",
      "  config_fed_server.conf         start_round                    0                                                                      \n",
      "  config_fed_server.conf         task_request_interval          0.05                                                                   \n",
      "  config_fed_server.conf         train_timeout                  0                                                                      \n",
      "  config_fed_server.conf         wait_time_after_min_received   0                                                                      \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!nvflare job create -j /tmp/nvflare/jobs/xgboost -force -w xgboost_tree \\\n",
    "-sd code \\\n",
    "-f config_fed_client.conf app_script=\"xgboost_fl.py\" app_config=\"--data_root_dir /tmp/nvflare/dataset/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17dc5dcc-573e-479e-965d-0cca8b58995a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "format_version = 2\n",
      "app_script = \"xgboost_fl.py\"\n",
      "app_config = \"--data_root_dir /tmp/nvflare/dataset/output\"\n",
      "executors = [\n",
      "  {\n",
      "    tasks = [\n",
      "      \"train\"\n",
      "    ]\n",
      "    executor {\n",
      "      path = \"nvflare.app_opt.pt.client_api_launcher_executor.ClientAPILauncherExecutor\"\n",
      "      args {\n",
      "        launcher_id = \"launcher\"\n",
      "        pipe_id = \"pipe\"\n",
      "        heartbeat_timeout = 60\n",
      "        params_exchange_format = \"raw\"\n",
      "        params_transfer_type = \"FULL\"\n",
      "        train_with_evaluation = true\n",
      "        launch_once = true\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n",
      "task_data_filters = []\n",
      "task_result_filters = []\n",
      "components = [\n",
      "  {\n",
      "    id = \"launcher\"\n",
      "    path = \"nvflare.app_common.launchers.subprocess_launcher.SubprocessLauncher\"\n",
      "    args {\n",
      "      script = \"python3 custom/{app_script}  {app_config} \"\n",
      "    }\n",
      "  }\n",
      "  {\n",
      "    id = \"pipe\"\n",
      "    path = \"nvflare.fuel.utils.pipe.file_pipe.FilePipe\"\n",
      "    args {\n",
      "      mode = \"PASSIVE\"\n",
      "      root_path = \"\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/nvflare/jobs/xgboost/app/config/config_fed_client.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b3086fd-ecf3-4584-a643-5706ae2069b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: tree\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/nvflare/jobs/xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111b784-1722-4aff-a48b-fc50c6c67281",
   "metadata": {},
   "source": [
    ">Note \n",
    " we use skip_rows = 0 to skip 1st row. We could skip_rows = [0, 3] to skip first and 4th rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ff6b6-5e39-4636-8a48-44d2b503dcdb",
   "metadata": {},
   "source": [
    "\n",
    "## Run job in simulator\n",
    "\n",
    "We use the simulator to run this job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09800691-46e0-4a95-bea6-d2a4a425052b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-29 12:57:55,136 - SimulatorRunner - INFO - Create the Simulator Server.\n",
      "2023-11-29 12:57:55,140 - CoreCell - INFO - server: creating listener on tcp://0:65467\n",
      "2023-11-29 12:57:55,180 - CoreCell - INFO - server: created backbone external listener for tcp://0:65467\n",
      "2023-11-29 12:57:55,180 - ConnectorManager - INFO - 99598: Try start_listener Listener resources: {'secure': False, 'host': 'localhost'}\n",
      "2023-11-29 12:57:55,181 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00002 PASSIVE tcp://0:1345] is starting\n",
      "2023-11-29 12:57:55,686 - CoreCell - INFO - server: created backbone internal listener for tcp://localhost:1345\n",
      "2023-11-29 12:57:55,687 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 PASSIVE tcp://0:65467] is starting\n",
      "2023-11-29 12:57:55,760 - nvflare.fuel.hci.server.hci - INFO - Starting Admin Server localhost on Port 65468\n",
      "2023-11-29 12:57:55,760 - SimulatorRunner - INFO - Deploy the Apps.\n",
      "2023-11-29 12:57:55,835 - SimulatorRunner - INFO - Create the simulate clients.\n",
      "2023-11-29 12:57:55,839 - ClientManager - INFO - Client: New client site-1@192.168.1.180 joined. Sent token: a761b9bc-601a-4f78-8982-b4feaf66606b.  Total clients: 1\n",
      "2023-11-29 12:57:55,839 - FederatedClient - INFO - Successfully registered client:site-1 for project simulator_server. Token:a761b9bc-601a-4f78-8982-b4feaf66606b SSID:\n",
      "2023-11-29 12:57:55,840 - ClientManager - INFO - Client: New client site-2@192.168.1.180 joined. Sent token: 2e2f25f8-12e4-4d17-bce8-b232343dd4ce.  Total clients: 2\n",
      "2023-11-29 12:57:55,841 - FederatedClient - INFO - Successfully registered client:site-2 for project simulator_server. Token:2e2f25f8-12e4-4d17-bce8-b232343dd4ce SSID:\n",
      "2023-11-29 12:57:55,841 - ClientManager - INFO - Client: New client site-3@192.168.1.180 joined. Sent token: c2cc98c9-095c-465e-a9df-d8b6b134e35a.  Total clients: 3\n",
      "2023-11-29 12:57:55,841 - FederatedClient - INFO - Successfully registered client:site-3 for project simulator_server. Token:c2cc98c9-095c-465e-a9df-d8b6b134e35a SSID:\n",
      "2023-11-29 12:57:55,841 - SimulatorRunner - INFO - Set the client status ready.\n",
      "2023-11-29 12:57:55,841 - SimulatorRunner - INFO - Deploy and start the Server App.\n",
      "2023-11-29 12:57:55,842 - Cell - INFO - Register blob CB for channel='server_command', topic='*'\n",
      "2023-11-29 12:57:55,842 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-11-29 12:57:55,842 - ServerCommandAgent - INFO - ServerCommandAgent cell register_request_cb: server.simulate_job\n",
      "2023-11-29 12:57:59,466 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Server runner starting ...\n",
      "2023-11-29 12:57:59,466 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: starting workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) ...\n",
      "2023-11-29 12:57:59,466 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Initializing ScatterAndGather workflow.\n",
      "2023-11-29 12:57:59,466 - XGBModelPersistor - INFO - Initializing server model as None\n",
      "2023-11-29 12:57:59,466 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) started\n",
      "2023-11-29 12:57:59,466 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Beginning ScatterAndGather training phase.\n",
      "2023-11-29 12:57:59,466 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 0 started.\n",
      "2023-11-29 12:57:59,466 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: scheduled task train\n",
      "2023-11-29 12:57:59,542 - SimulatorClientRunner - INFO - Start the clients run simulation.\n",
      "2023-11-29 12:58:00,548 - SimulatorClientRunner - INFO - Simulate Run client: site-1 on GPU group: None\n",
      "2023-11-29 12:58:00,549 - SimulatorClientRunner - INFO - Simulate Run client: site-2 on GPU group: None\n",
      "2023-11-29 12:58:00,549 - SimulatorClientRunner - INFO - Simulate Run client: site-3 on GPU group: None\n",
      "2023-11-29 12:58:01,597 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-11-29 12:58:01,597 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-11-29 12:58:01,599 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-11-29 12:58:01,635 - CoreCell - INFO - site-3.simulate_job: created backbone external connector to tcp://localhost:65467\n",
      "2023-11-29 12:58:01,635 - CoreCell - INFO - site-2.simulate_job: created backbone external connector to tcp://localhost:65467\n",
      "2023-11-29 12:58:01,635 - CoreCell - INFO - site-1.simulate_job: created backbone external connector to tcp://localhost:65467\n",
      "2023-11-29 12:58:01,635 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:65467] is starting\n",
      "2023-11-29 12:58:01,635 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:65467] is starting\n",
      "2023-11-29 12:58:01,635 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:65467] is starting\n",
      "2023-11-29 12:58:01,636 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:65502 => 127.0.0.1:65467] is created: PID: 99618\n",
      "2023-11-29 12:58:01,636 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:65503 => 127.0.0.1:65467] is created: PID: 99616\n",
      "2023-11-29 12:58:01,636 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:65504 => 127.0.0.1:65467] is created: PID: 99617\n",
      "2023-11-29 12:58:01,636 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 127.0.0.1:65467 <= 127.0.0.1:65502] is created: PID: 99598\n",
      "2023-11-29 12:58:01,637 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00007 127.0.0.1:65467 <= 127.0.0.1:65503] is created: PID: 99598\n",
      "2023-11-29 12:58:01,637 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00008 127.0.0.1:65467 <= 127.0.0.1:65504] is created: PID: 99598\n",
      "2023-11-29 12:58:04,147 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-11-29 12:58:04,147 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-11-29 12:58:04,147 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-11-29 12:58:04,650 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-11-29 12:58:04,650 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-11-29 12:58:04,651 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-11-29 12:58:04,661 - ClientRunner - INFO - [identity=site-3, run=simulate_job]: synced to Server Runner in 0.5112478733062744 seconds\n",
      "2023-11-29 12:58:04,662 - ClientRunner - INFO - [identity=site-3, run=simulate_job]: client runner started\n",
      "2023-11-29 12:58:04,662 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-3\n",
      "2023-11-29 12:58:04,662 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: synced to Server Runner in 0.5123658180236816 seconds\n",
      "2023-11-29 12:58:04,663 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: client runner started\n",
      "2023-11-29 12:58:04,664 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-1\n",
      "2023-11-29 12:58:04,664 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: synced to Server Runner in 0.5141251087188721 seconds\n",
      "2023-11-29 12:58:04,664 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: client runner started\n",
      "2023-11-29 12:58:04,665 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-2\n",
      "2023-11-29 12:58:04,668 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: assigned task to client site-3: name=train, id=51f63d4b-e971-4664-b398-ce74e8649a8f\n",
      "2023-11-29 12:58:04,669 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: sent task assignment to client. client_name:site-3 task_id:51f63d4b-e971-4664-b398-ce74e8649a8f\n",
      "2023-11-29 12:58:04,669 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: assigned task to client site-1: name=train, id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf\n",
      "2023-11-29 12:58:04,670 - GetTaskCommand - INFO - return task to client.  client_name: site-3  task_name: train   task_id: 51f63d4b-e971-4664-b398-ce74e8649a8f  sharable_header_task_id: 51f63d4b-e971-4664-b398-ce74e8649a8f\n",
      "2023-11-29 12:58:04,671 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: assigned task to client site-2: name=train, id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec\n",
      "2023-11-29 12:58:04,671 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: sent task assignment to client. client_name:site-1 task_id:53a6fff1-5e40-4ae5-ace6-3c1926c691cf\n",
      "2023-11-29 12:58:04,673 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: sent task assignment to client. client_name:site-2 task_id:a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec\n",
      "2023-11-29 12:58:04,673 - Communicator - INFO - Received from simulator_server server. getTask: train size: 652B (652 Bytes) time: 0.010192 seconds\n",
      "2023-11-29 12:58:04,673 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: train   task_id: 53a6fff1-5e40-4ae5-ace6-3c1926c691cf  sharable_header_task_id: 53a6fff1-5e40-4ae5-ace6-3c1926c691cf\n",
      "2023-11-29 12:58:04,673 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-29 12:58:04,673 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=51f63d4b-e971-4664-b398-ce74e8649a8f\n",
      "2023-11-29 12:58:04,674 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-29 12:58:04,674 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: train   task_id: a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec  sharable_header_task_id: a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec\n",
      "2023-11-29 12:58:04,674 - ClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: execute for task (train)\n",
      "2023-11-29 12:58:04,675 - Communicator - INFO - Received from simulator_server server. getTask: train size: 652B (652 Bytes) time: 0.011293 seconds\n",
      "2023-11-29 12:58:04,675 - Communicator - INFO - Received from simulator_server server. getTask: train size: 652B (652 Bytes) time: 0.010285 seconds\n",
      "2023-11-29 12:58:04,676 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-29 12:58:04,676 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-29 12:58:04,676 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf\n",
      "2023-11-29 12:58:04,676 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec\n",
      "2023-11-29 12:58:04,676 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-29 12:58:04,676 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-29 12:58:04,677 - ClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: execute for task (train)\n",
      "2023-11-29 12:58:04,677 - ClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: execute for task (train)\n",
      "2023-11-29 12:58:04,686 - ClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: External process for task (train) is launched.\n",
      "2023-11-29 12:58:04,686 - ClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: External process for task (train) is launched.\n",
      "2023-11-29 12:58:04,686 - ClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: External process for task (train) is launched.\n",
      "current_round=0\n",
      "current_round=0\n",
      "current_round=0\n",
      "[0]\ttrain-auc:0.76632\ttest-auc:0.76160\n",
      "2023-11-29 12:59:32,182 - ClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: got result 'Message(topic=train, msg_id=081f7a83-2df4-459d-98c5-cf7151734140, req_id=b2d1811a-42c2-4388-859e-251b8c35dea2, msg_type=REP)' for task 'train'\n",
      "2023-11-29 12:59:32,183 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: finished processing task\n",
      "2023-11-29 12:59:32,183 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: try #1: sending task result to server\n",
      "2023-11-29 12:59:32,183 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: checking task ...\n",
      "2023-11-29 12:59:32,184 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "[0]\ttrain-auc:0.76565\ttest-auc:0.76323\n",
      "2023-11-29 12:59:32,189 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: start to send task result to server\n",
      "2023-11-29 12:59:32,189 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-29 12:59:32,190 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=train, id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec\n",
      "2023-11-29 12:59:32,192 - XGBBaggingAggregator - ERROR - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: expected XGB_MODEL but got WEIGHTS\n",
      "2023-11-29 12:59:32,192 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: Contribution from site-2 REJECTED by the aggregator at round 0.\n",
      "2023-11-29 12:59:32,192 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: finished processing client result by scatter_and_gather\n",
      "2023-11-29 12:59:32,192 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec\n",
      "2023-11-29 12:59:32,193 - Communicator - INFO -  SubmitUpdate size: 33KB (32999 Bytes). time: 0.003750 seconds\n",
      "2023-11-29 12:59:32,193 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=a22bd52d-6e7e-408b-a2ae-37a6c6a3e4ec]: task result sent to server\n",
      "2023-11-29 12:59:32,193 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 0.05 task_processed: True\n",
      "2023-11-29 12:59:32,199 - ClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: got result 'Message(topic=train, msg_id=f4719a08-a9d9-4a31-bc75-25755a0b2155, req_id=41e159b8-79af-4856-9727-ef93a3b382ca, msg_type=REP)' for task 'train'\n",
      "2023-11-29 12:59:32,200 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: finished processing task\n",
      "2023-11-29 12:59:32,200 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: try #1: sending task result to server\n",
      "2023-11-29 12:59:32,200 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: checking task ...\n",
      "2023-11-29 12:59:32,201 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-29 12:59:32,204 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: start to send task result to server\n",
      "2023-11-29 12:59:32,204 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-29 12:59:32,205 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job]: got result from client site-3 for task: name=train, id=51f63d4b-e971-4664-b398-ce74e8649a8f\n",
      "2023-11-29 12:59:32,205 - XGBBaggingAggregator - ERROR - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: expected XGB_MODEL but got WEIGHTS\n",
      "2023-11-29 12:59:32,205 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: Contribution from site-3 REJECTED by the aggregator at round 0.\n",
      "2023-11-29 12:59:32,205 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: finished processing client result by scatter_and_gather\n",
      "2023-11-29 12:59:32,205 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-3   task_id:51f63d4b-e971-4664-b398-ce74e8649a8f\n",
      "2023-11-29 12:59:32,206 - Communicator - INFO -  SubmitUpdate size: 33.1KB (33147 Bytes). time: 0.002062 seconds\n",
      "2023-11-29 12:59:32,206 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=51f63d4b-e971-4664-b398-ce74e8649a8f]: task result sent to server\n",
      "2023-11-29 12:59:32,206 - ClientTaskWorker - INFO - Finished one task run for client: site-3 interval: 0.05 task_processed: True\n",
      "[0]\ttrain-auc:0.76459\ttest-auc:0.76176\n",
      "2023-11-29 12:59:32,242 - ClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: got result 'Message(topic=train, msg_id=82c5e64e-3dfb-46f1-86ae-46bf94cc9f4c, req_id=815d52e4-c70e-41f6-811e-4a57ac1be57b, msg_type=REP)' for task 'train'\n",
      "2023-11-29 12:59:32,243 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: finished processing task\n",
      "2023-11-29 12:59:32,243 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: try #1: sending task result to server\n",
      "2023-11-29 12:59:32,243 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: checking task ...\n",
      "2023-11-29 12:59:32,243 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-29 12:59:32,246 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: start to send task result to server\n",
      "2023-11-29 12:59:32,246 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-29 12:59:32,247 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=train, id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf\n",
      "2023-11-29 12:59:32,248 - XGBBaggingAggregator - ERROR - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: expected XGB_MODEL but got WEIGHTS\n",
      "2023-11-29 12:59:32,248 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: Contribution from site-1 REJECTED by the aggregator at round 0.\n",
      "2023-11-29 12:59:32,248 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=OK, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: finished processing client result by scatter_and_gather\n",
      "2023-11-29 12:59:32,248 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:53a6fff1-5e40-4ae5-ace6-3c1926c691cf\n",
      "2023-11-29 12:59:32,249 - Communicator - INFO -  SubmitUpdate size: 33.3KB (33276 Bytes). time: 0.002618 seconds\n",
      "2023-11-29 12:59:32,249 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=53a6fff1-5e40-4ae5-ace6-3c1926c691cf]: task result sent to server\n",
      "2023-11-29 12:59:32,249 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 0.05 task_processed: True\n",
      "2023-11-29 12:59:32,251 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: task train exit with status TaskCompletionStatus.OK\n",
      "2023-11-29 12:59:32,549 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Start aggregation.\n",
      "2023-11-29 12:59:32,549 - XGBBaggingAggregator - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: aggregating 0 update(s) at round 0\n",
      "2023-11-29 12:59:32,549 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: End aggregation.\n",
      "2023-11-29 12:59:32,550 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Start persist model on server.\n",
      "2023-11-29 12:59:32,550 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: End persist model on server.\n",
      "2023-11-29 12:59:32,550 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 0 finished.\n",
      "2023-11-29 12:59:32,550 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 1 started.\n",
      "2023-11-29 12:59:32,550 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: scheduled task train\n",
      "2023-11-29 12:59:32,761 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: assigned task to client site-2: name=train, id=6b540ae4-d302-46f9-9061-34717755f1a4\n",
      "2023-11-29 12:59:32,762 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: sent task assignment to client. client_name:site-2 task_id:6b540ae4-d302-46f9-9061-34717755f1a4\n",
      "2023-11-29 12:59:32,763 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: train   task_id: 6b540ae4-d302-46f9-9061-34717755f1a4  sharable_header_task_id: 6b540ae4-d302-46f9-9061-34717755f1a4\n",
      "2023-11-29 12:59:32,766 - Communicator - INFO - Received from simulator_server server. getTask: train size: 683B (683 Bytes) time: 0.011526 seconds\n",
      "2023-11-29 12:59:32,767 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-29 12:59:32,768 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=6b540ae4-d302-46f9-9061-34717755f1a4\n",
      "2023-11-29 12:59:32,768 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-29 12:59:32,769 - ClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: execute for task (train)\n",
      "2023-11-29 12:59:32,773 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: assigned task to client site-3: name=train, id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f\n",
      "2023-11-29 12:59:32,774 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: sent task assignment to client. client_name:site-3 task_id:5e18e8c9-801d-4f5f-90b9-fcda99c72a3f\n",
      "2023-11-29 12:59:32,774 - GetTaskCommand - INFO - return task to client.  client_name: site-3  task_name: train   task_id: 5e18e8c9-801d-4f5f-90b9-fcda99c72a3f  sharable_header_task_id: 5e18e8c9-801d-4f5f-90b9-fcda99c72a3f\n",
      "2023-11-29 12:59:32,778 - Communicator - INFO - Received from simulator_server server. getTask: train size: 683B (683 Bytes) time: 0.010122 seconds\n",
      "2023-11-29 12:59:32,781 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-29 12:59:32,786 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f\n",
      "2023-11-29 12:59:32,786 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-29 12:59:32,787 - ClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: execute for task (train)\n",
      "2023-11-29 12:59:32,811 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: assigned task to client site-1: name=train, id=630dad4d-a653-4f3a-a395-e6a61513ae95\n",
      "2023-11-29 12:59:32,812 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: sent task assignment to client. client_name:site-1 task_id:630dad4d-a653-4f3a-a395-e6a61513ae95\n",
      "2023-11-29 12:59:32,812 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: train   task_id: 630dad4d-a653-4f3a-a395-e6a61513ae95  sharable_header_task_id: 630dad4d-a653-4f3a-a395-e6a61513ae95\n",
      "2023-11-29 12:59:32,814 - Communicator - INFO - Received from simulator_server server. getTask: train size: 683B (683 Bytes) time: 0.005109 seconds\n",
      "2023-11-29 12:59:32,814 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-11-29 12:59:32,815 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=630dad4d-a653-4f3a-a395-e6a61513ae95\n",
      "2023-11-29 12:59:32,815 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: invoking task executor ClientAPILauncherExecutor\n",
      "2023-11-29 12:59:32,815 - ClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: execute for task (train)\n",
      "current_round=1\n",
      "Traceback (most recent call last):\n",
      "  File \"/private/tmp/nvflare/xgboost/simulate_job/app_site-2/custom/xgboost_fl.py\", line 213, in <module>\n",
      "    main()\n",
      "  File \"/private/tmp/nvflare/xgboost/simulate_job/app_site-2/custom/xgboost_fl.py\", line 147, in main\n",
      "    model_updates = global_params[\"model_data\"]\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "current_round=1\n",
      "Traceback (most recent call last):\n",
      "  File \"/private/tmp/nvflare/xgboost/simulate_job/app_site-3/custom/xgboost_fl.py\", line 213, in <module>\n",
      "    main()\n",
      "  File \"/private/tmp/nvflare/xgboost/simulate_job/app_site-3/custom/xgboost_fl.py\", line 147, in main\n",
      "    model_updates = global_params[\"model_data\"]\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "current_round=1\n",
      "Traceback (most recent call last):\n",
      "  File \"/private/tmp/nvflare/xgboost/simulate_job/app_site-1/custom/xgboost_fl.py\", line 213, in <module>\n",
      "    main()\n",
      "  File \"/private/tmp/nvflare/xgboost/simulate_job/app_site-1/custom/xgboost_fl.py\", line 147, in main\n",
      "    model_updates = global_params[\"model_data\"]\n",
      "TypeError: 'NoneType' object is not subscriptable\n",
      "2023-11-29 13:00:32,182 - PipeHandler - ERROR - read timeout after 60 secs\n",
      "2023-11-29 13:00:32,187 - ClientAPILauncherExecutor - ERROR - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: received reply: 'Message(topic=_PEER_GONE_, msg_id=404b1aeb-26d2-4bf5-8b6d-b4ef2a9ec3d7, req_id=None, msg_type=REQ)' while waiting for the result of train\n",
      "2023-11-29 13:00:32,188 - ClientAPILauncherExecutor - ERROR - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: missing result FLModel for train_task: train.\n",
      "2023-11-29 13:00:32,188 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: finished processing task\n",
      "2023-11-29 13:00:32,189 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: try #1: sending task result to server\n",
      "2023-11-29 13:00:32,195 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: checking task ...\n",
      "2023-11-29 13:00:32,195 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-29 13:00:32,198 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: start to send task result to server\n",
      "2023-11-29 13:00:32,199 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-29 13:00:32,199 - PipeHandler - ERROR - read timeout after 60 secs\n",
      "2023-11-29 13:00:32,199 - ClientAPILauncherExecutor - ERROR - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: received reply: 'Message(topic=_PEER_GONE_, msg_id=42480033-505a-46b8-8e2f-983cc280853d, req_id=None, msg_type=REQ)' while waiting for the result of train\n",
      "2023-11-29 13:00:32,199 - ClientAPILauncherExecutor - ERROR - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: missing result FLModel for train_task: train.\n",
      "2023-11-29 13:00:32,200 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: finished processing task\n",
      "2023-11-29 13:00:32,200 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: try #1: sending task result to server\n",
      "2023-11-29 13:00:32,200 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: checking task ...\n",
      "2023-11-29 13:00:32,200 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-29 13:00:32,200 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job]: got result from client site-2 for task: name=train, id=6b540ae4-d302-46f9-9061-34717755f1a4\n",
      "2023-11-29 13:00:32,200 - ServerRunner - ERROR - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: Aborting current RUN due to FATAL_SYSTEM_ERROR received: Result from site-2 is bad, error code: EXECUTION_EXCEPTION. ScatterAndGather exiting at round 1.\n",
      "2023-11-29 13:00:32,200 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: asked to abort - triggered abort_signal to stop the RUN\n",
      "2023-11-29 13:00:32,200 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: finished processing client result by scatter_and_gather\n",
      "2023-11-29 13:00:32,200 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-2   task_id:6b540ae4-d302-46f9-9061-34717755f1a4\n",
      "2023-11-29 13:00:32,201 - Communicator - INFO -  SubmitUpdate size: 567B (567 Bytes). time: 0.002604 seconds\n",
      "2023-11-29 13:00:32,202 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=6b540ae4-d302-46f9-9061-34717755f1a4]: task result sent to server\n",
      "2023-11-29 13:00:32,202 - ClientTaskWorker - INFO - Finished one task run for client: site-2 interval: 0.05 task_processed: True\n",
      "2023-11-29 13:00:32,205 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: start to send task result to server\n",
      "2023-11-29 13:00:32,205 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-29 13:00:32,207 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job]: got result from client site-3 for task: name=train, id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f\n",
      "2023-11-29 13:00:32,207 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: ignored result submission since server runner's status is done\n",
      "2023-11-29 13:00:32,207 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-3   task_id:5e18e8c9-801d-4f5f-90b9-fcda99c72a3f\n",
      "2023-11-29 13:00:32,208 - Communicator - INFO -  SubmitUpdate size: 567B (567 Bytes). time: 0.001901 seconds\n",
      "2023-11-29 13:00:32,208 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=5e18e8c9-801d-4f5f-90b9-fcda99c72a3f]: task result sent to server\n",
      "2023-11-29 13:00:32,208 - ClientTaskWorker - INFO - Finished one task run for client: site-3 interval: 0.05 task_processed: True\n",
      "2023-11-29 13:00:32,240 - PipeHandler - ERROR - read timeout after 60 secs\n",
      "2023-11-29 13:00:32,241 - ClientAPILauncherExecutor - ERROR - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: received reply: 'Message(topic=_PEER_GONE_, msg_id=0b753702-7eae-45d8-9d88-86c35fb2e993, req_id=None, msg_type=REQ)' while waiting for the result of train\n",
      "2023-11-29 13:00:32,243 - ClientAPILauncherExecutor - ERROR - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: missing result FLModel for train_task: train.\n",
      "2023-11-29 13:00:32,244 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: finished processing task\n",
      "2023-11-29 13:00:32,244 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: try #1: sending task result to server\n",
      "2023-11-29 13:00:32,244 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: checking task ...\n",
      "2023-11-29 13:00:32,245 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-11-29 13:00:32,248 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: start to send task result to server\n",
      "2023-11-29 13:00:32,249 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-11-29 13:00:32,250 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job]: got result from client site-1 for task: name=train, id=630dad4d-a653-4f3a-a395-e6a61513ae95\n",
      "2023-11-29 13:00:32,250 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: ignored result submission since server runner's status is done\n",
      "2023-11-29 13:00:32,250 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-1   task_id:630dad4d-a653-4f3a-a395-e6a61513ae95\n",
      "2023-11-29 13:00:32,250 - Communicator - INFO -  SubmitUpdate size: 567B (567 Bytes). time: 0.001711 seconds\n",
      "2023-11-29 13:00:32,250 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=630dad4d-a653-4f3a-a395-e6a61513ae95]: task result sent to server\n",
      "2023-11-29 13:00:32,251 - ClientTaskWorker - INFO - Finished one task run for client: site-1 interval: 0.05 task_processed: True\n",
      "2023-11-29 13:00:32,260 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2023-11-29 13:00:32,260 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2023-11-29 13:00:32,261 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2023-11-29 13:00:32,261 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2023-11-29 13:00:32,262 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2023-11-29 13:00:32,263 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-2 \n",
      "2023-11-29 13:00:32,264 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2023-11-29 13:00:32,264 - GetTaskCommand - INFO - return task to client.  client_name: site-3  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2023-11-29 13:00:32,264 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 99618\n",
      "2023-11-29 13:00:32,265 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 Not Connected] is closed PID: 99598\n",
      "2023-11-29 13:00:32,265 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2023-11-29 13:00:32,265 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2023-11-29 13:00:32,265 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2023-11-29 13:00:32,266 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-3 \n",
      "2023-11-29 13:00:32,266 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00008 Not Connected] is closed PID: 99598\n",
      "2023-11-29 13:00:32,266 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 99617\n",
      "2023-11-29 13:00:32,305 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2023-11-29 13:00:32,306 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2023-11-29 13:00:32,307 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2023-11-29 13:00:32,307 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2023-11-29 13:00:32,307 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2023-11-29 13:00:32,307 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-1 \n",
      "2023-11-29 13:00:32,307 - FederatedClient - INFO - Shutting down client run: site-1\n",
      "2023-11-29 13:00:32,308 - FederatedClient - INFO - Shutting down client run: site-2\n",
      "2023-11-29 13:00:32,308 - FederatedClient - INFO - Shutting down client run: site-3\n",
      "2023-11-29 13:00:32,308 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: asked to abort - triggered abort_signal to stop the RUN\n",
      "2023-11-29 13:00:32,308 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00007 Not Connected] is closed PID: 99598\n",
      "2023-11-29 13:00:32,308 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 99616\n",
      "2023-11-29 13:00:32,553 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Abort signal received. Exiting at round 1.\n",
      "2023-11-29 13:00:32,554 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Workflow: scatter_and_gather finalizing ...\n",
      "2023-11-29 13:00:32,900 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: ABOUT_TO_END_RUN fired\n",
      "2023-11-29 13:00:32,900 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Firing CHECK_END_RUN_READINESS ...\n",
      "2023-11-29 13:00:32,901 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: END_RUN fired\n",
      "2023-11-29 13:00:32,901 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Server runner finished.\n",
      "2023-11-29 13:00:35,671 - SimulatorServer - INFO - Server app stopped.\n",
      "\n",
      "\n",
      "2023-11-29 13:00:35,995 - nvflare.fuel.hci.server.hci - INFO - Admin Server localhost on Port 65468 shutdown!\n",
      "2023-11-29 13:00:35,996 - SimulatorServer - INFO - shutting down server\n",
      "2023-11-29 13:00:35,997 - SimulatorServer - INFO - canceling sync locks\n",
      "2023-11-29 13:00:35,997 - SimulatorServer - INFO - server off\n",
      "2023-11-29 13:00:39,498 - MPM - INFO - MPM: Good Bye!\n"
     ]
    }
   ],
   "source": [
    "!nvflare simulator /tmp/nvflare/jobs/xgboost -w /tmp/nvflare/xgboost -n 3 -t 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe301fa-8f51-4d3f-b61e-fd851b9f92d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's examine the results.\n",
    "\n",
    "We can notice from the FL training log, at the last round of local training, site-1 reports `site-1: local model AUC: 0.6351`\n",
    "Now let's run a local training to verify if this number makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba843de-96ed-4c41-a5c1-d770d604a1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ./code/sgd_local_iter.py --data_root_dir /tmp/nvflare/dataset/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8009591f-83c4-4c84-be79-176573296a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python3 ./code/sgd_local_oneshot.py --data_root_dir /tmp/nvflare/dataset/output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7bbacc-b059-4f82-9785-2b22bf840ef9",
   "metadata": {},
   "source": [
    "In this experiment, all three clients have relatively large amount data wiht homogeneous distribution, we would expect the three numbers align within reasonable variation range. \n",
    "\n",
    "The final result for iterative learning is `ending model AUC: 0.6352`, and one-shot learning is `local model AUC: 0.6355`, as compared with FL's `local model AUC: 0.6351`, the numbers do align.\n",
    "\n",
    "## We are done !\n",
    "Congratulations! you have just completed the federated linear model for tabular data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5329a14-1a81-488a-8edc-2b7c85cffd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
