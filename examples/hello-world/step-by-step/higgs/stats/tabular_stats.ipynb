{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf1c433-ba7f-4967-b066-670eb5e016ed",
   "metadata": {},
   "source": [
    "# Tabular Data Federated Statistics \n",
    "\n",
    "Before we perform machine learning tasks on the Higgs data, let's examine the statistics of the dataset. \n",
    "\n",
    "\n",
    "## Setup NVFLARE\n",
    "\n",
    "Follow [Getting Started](https://nvflare.readthedocs.io/en/main/getting_started.html) to set up a virtual environment and install NVFLARE.\n",
    "\n",
    "You can also follow this [notebook](https://github.com/NVIDIA/NVFlare/blob/main/examples/nvflare_setup.ipynb) to get set up.\n",
    "\n",
    "> Make sure you have installed nvflare from **terminal** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf2d68-b020-457f-a1f1-b1f95509c929",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "assuming the current directory is 'higgs/stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552a5eb-dbfb-42da-8cfa-082c1739012e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f373c84-b3d9-43e2-9e6a-9ff0db33f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5cd9a-3da9-446c-aac0-6ae84bf0ead1",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare data\n",
    "\n",
    "### Download and Store Data\n",
    "\n",
    "To run the examples, we first download the dataset from the HIGGS link above, which is a single .csv file. By default, we assume the dataset is downloaded, uncompressed, and stored in \n",
    "\n",
    "```\n",
    "/tmp/nvflare/dataset/input/higgs.zip.\n",
    "\n",
    "```\n",
    "\n",
    "You can either use wget or curl to download directly if you have wget or curl installed. here is using curl command. This will takes a while to download 2.6+GB file. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa27561d-e665-4480-9707-2cf818a3f61f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p /tmp/nvflare/dataset/input\n",
    "\n",
    "! curl -o /tmp/nvflare/dataset/input/higgs.zip https://archive.ics.uci.edu/static/public/280/higgs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc3bf2-cd95-4fa9-90b3-c70a80043379",
   "metadata": {},
   "source": [
    "Alternative download with wget ```wget -P /tmp/nvflare/dataset/input/ https://archive.ics.uci.edu/static/public/280/higgs.zip```\n",
    "\n",
    "First we need to unzip the higgs.zip, we have already pre-installed \"unzip\" and \"gunzip\", so we just directly use this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00d35a-5a04-4344-85bf-708a892ff967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! unzip -d /tmp/nvflare/dataset/input/ /tmp/nvflare/dataset/input/higgs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1697bf5-dccd-4aee-b482-df8e411cec7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gunzip -c /tmp/nvflare/dataset/input/HIGGS.csv.gz > /tmp/nvflare/dataset/input/higgs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d98f2e8-afaa-475a-92a0-cd864cc631cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -al /tmp/nvflare/dataset/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3ac98-3e9d-4a06-b7fb-0e2867266232",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "HIGGS dataset contains 11 million instances (rows), each with 28 attributes.\n",
    "The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. \n",
    "The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. The last 500,000 examples are used as a test set.\n",
    "\n",
    "The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features): lepton  pT, lepton  eta, lepton  phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb. For more detailed information about each feature see the original paper.\n",
    "\n",
    "Since HIGGS dataset is already randomly recorded, data split will be specified by the continuous index ranges for each client, rather than a vector of random instance indices. We will split the dataset uniformly: all clients has the same amount of data. The output directory \n",
    "\n",
    "```\n",
    "/tmp/nvflare/dataset/output/\n",
    "\n",
    "```\n",
    "\n",
    "To make it similar to the real world use cases, we put features (CSV file headers) into a file in the input directory.  When we split the file, we make sure each site will has a \"header.csv\" file corresponding to the csv data. In horizontal split. all the header will be the same. but for vertical learning, each site may have different headers. \n",
    "\n",
    "We create a simple python code to split data: called split_csv.py. Let's run this, you will need to wait for few minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2148851-9df1-4853-8406-074c150b668d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Your list of data\n",
    "features = [\"label\", \"lepton_pt\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b_tag\", \"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b_tag\", \"jet_3_pt\", \"jet_3_eta\", \"jet_3_phi\", \"jet_3_b_tag\",\\\n",
    "            \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\", \"jet_4_b_tag\", \\\n",
    "            \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"]\n",
    "\n",
    "# Specify the file path\n",
    "file_path =  '/tmp/nvflare/dataset/input/headers.csv'\n",
    "\n",
    "with open(file_path, 'w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(features)\n",
    "\n",
    "print(f\"features written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab565816-3480-42ce-bf03-cfae3ae6f427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/dataset/input/headers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008ea4a2-39dd-4472-b0b7-4032177d7ac2",
   "metadata": {},
   "source": [
    "Now we prepare to split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0122950-f58d-4ae8-8abd-ed5a6c5c7b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python split_csv.py \\\n",
    "  --input_data_path=/tmp/nvflare/dataset/input/higgs.csv \\\n",
    "  --input_header_path=/tmp/nvflare/dataset/input/headers.csv \\\n",
    "  --output_dir=/tmp/nvflare/dataset/output/ \\\n",
    "  --site_num=3 \\\n",
    "  --sample_rate=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763fafdf-f45f-44cb-b445-c6ce978a8d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -al /tmp/nvflare/dataset/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2dd58-210f-4006-86ec-a821713d5cac",
   "metadata": {},
   "source": [
    "Now we have our data prepared, let's first take a look at these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47416d89-74f9-4ede-b8b9-99f0eeaa9814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\"label\", \"lepton_pt\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b_tag\", \"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b_tag\", \"jet_3_pt\", \"jet_3_eta\", \"jet_3_phi\", \"jet_3_b_tag\",\\\n",
    "            \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\", \"jet_4_b_tag\", \\\n",
    "            \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4ab17c-8af7-43af-923d-becf6dec919c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ad635-5b8a-4430-8bce-8e3bb4b0a0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df: pd.DataFrame = pd.read_csv(\"/tmp/nvflare/dataset/output/site-1.csv\", names=features, sep=r\"\\s*,\\s*\", engine=\"python\", na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1751d35c-99ce-4bef-9b8f-af3a57507a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2cefa5-d579-45c0-815e-70dac06809c4",
   "metadata": {},
   "source": [
    "## Create a statistics calculator for the local tabular dataset\n",
    "\n",
    "\n",
    "```\n",
    "import csv\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.series import Series\n",
    "\n",
    "from nvflare.apis.fl_context import FLContext\n",
    "from nvflare.app_common.abstract.statistics_spec import BinRange, Feature, Histogram, HistogramType, Statistics\n",
    "from nvflare.app_common.statistics.numpy_utils import dtype_to_data_type, get_std_histogram_buckets\n",
    "\n",
    "\n",
    "class DFStatistics(Statistics):\n",
    "    def __init__(self, data_root_dir: str):\n",
    "        super().__init__()\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.data: Optional[Dict[str, pd.DataFrame]] = None\n",
    "        self.data_features = None\n",
    "\n",
    "    def load_features(self, fl_ctx: FLContext) -> List:\n",
    "        client_name = self.get_client_name(fl_ctx)\n",
    "        try:\n",
    "            data_path = f\"{self.data_root_dir}/{client_name}_header.csv\"\n",
    "\n",
    "            features = []\n",
    "            with open(data_path, 'r') as file:\n",
    "                # Create a CSV reader object\n",
    "                csv_reader = csv.reader(file)\n",
    "                line_list = next(csv_reader)\n",
    "                features = line_list\n",
    "            return features\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Load header for client {client_name} failed! {e}\")\n",
    "\n",
    "    def load_data(self, fl_ctx: FLContext) -> Dict[str, pd.DataFrame]:\n",
    "        client_name = self.get_client_name(fl_ctx)\n",
    "        try:\n",
    "            data_path = f\"{self.data_root_dir}/{client_name}.csv\"\n",
    "            # example of load data from CSV\n",
    "            df: pd.DataFrame = pd.read_csv(\n",
    "                data_path, names=self.data_features, sep=r\"\\s*,\\s*\", engine=\"python\", na_values=\"?\"\n",
    "            )\n",
    "            return {\"train\": df}\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Load data for client {client_name} failed! {e}\")\n",
    "\n",
    "    def get_client_name(self, fl_ctx):\n",
    "        client_name = fl_ctx.get_identity_name() if fl_ctx is not None else \"site-1\"\n",
    "        if fl_ctx:\n",
    "            self.log_info(fl_ctx, f\"load data for client {client_name}\")\n",
    "        else:\n",
    "            print(f\"load data for client {client_name}\")\n",
    "        return client_name\n",
    "\n",
    "    def initialize(self, fl_ctx: FLContext):\n",
    "        self.data_features = self.load_features(fl_ctx)\n",
    "        self.data = self.load_data(fl_ctx)\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"data is not loaded. make sure the data is loaded\")\n",
    "\n",
    "    def features(self) -> Dict[str, List[Feature]]:\n",
    "        results: Dict[str, List[Feature]] = {}\n",
    "        for ds_name in self.data:\n",
    "            df = self.data[ds_name]\n",
    "            results[ds_name] = []\n",
    "            for feature_name in df:\n",
    "                data_type = dtype_to_data_type(df[feature_name].dtype)\n",
    "                results[ds_name].append(Feature(feature_name, data_type))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def count(self, dataset_name: str, feature_name: str) -> int:\n",
    "        df: pd.DataFrame = self.data[dataset_name]\n",
    "        return df[feature_name].count()\n",
    "\n",
    "    def sum(self, dataset_name: str, feature_name: str) -> float:\n",
    "        df: pd.DataFrame = self.data[dataset_name]\n",
    "        return df[feature_name].sum().item()\n",
    "\n",
    "    def mean(self, dataset_name: str, feature_name: str) -> float:\n",
    "\n",
    "        count: int = self.count(dataset_name, feature_name)\n",
    "        sum_value: float = self.sum(dataset_name, feature_name)\n",
    "        return sum_value / count\n",
    "\n",
    "    def stddev(self, dataset_name: str, feature_name: str) -> float:\n",
    "        df = self.data[dataset_name]\n",
    "        return df[feature_name].std().item()\n",
    "\n",
    "    def variance_with_mean(\n",
    "        self, dataset_name: str, feature_name: str, global_mean: float, global_count: float\n",
    "    ) -> float:\n",
    "        df = self.data[dataset_name]\n",
    "        tmp = (df[feature_name] - global_mean) * (df[feature_name] - global_mean)\n",
    "        variance = tmp.sum() / (global_count - 1)\n",
    "        return variance.item()\n",
    "\n",
    "    def histogram(\n",
    "        self, dataset_name: str, feature_name: str, num_of_bins: int, global_min_value: float, global_max_value: float\n",
    "    ) -> Histogram:\n",
    "\n",
    "        num_of_bins: int = num_of_bins\n",
    "\n",
    "        df = self.data[dataset_name]\n",
    "        feature: Series = df[feature_name]\n",
    "        flattened = feature.ravel()\n",
    "        flattened = flattened[flattened != np.array(None)]\n",
    "        buckets = get_std_histogram_buckets(flattened, num_of_bins, BinRange(global_min_value, global_max_value))\n",
    "        return Histogram(HistogramType.STANDARD, buckets)\n",
    "\n",
    "    def max_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "        \"\"\"this is needed for histogram calculation, not used for reporting\"\"\"\n",
    "\n",
    "        df = self.data[dataset_name]\n",
    "        return df[feature_name].max()\n",
    "\n",
    "    def min_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "        \"\"\"this is needed for histogram calculation, not used for reporting\"\"\"\n",
    "\n",
    "        df = self.data[dataset_name]\n",
    "        return df[feature_name].min()\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "Let's see if the code works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8e59bd-0b97-4ad9-a0de-abd10dd76d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686d46c3-3986-4faf-bde8-1d4af749bf57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from df_stats import DFStatistics\n",
    "\n",
    "df_stats_cal = DFStatistics(data_root_dir = \"/tmp/nvflare/dataset/output\")\n",
    "\n",
    "# We use fl_ctx = None for local calculation ( where the data set default to \"site-1.csv\", so we can explore the stats locally without federated settings. \n",
    "df_stats_cal.initialize(fl_ctx = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef9f923-54eb-4b7b-a23d-c93201554332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_features = df_stats_cal.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c8f648-c5b9-4b71-b80c-431613c33dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0464f5e-b9ae-42d9-8bbe-7c180ce48767",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stats_cal.count(\"train\", \"lepton_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31419dd6-afdd-4612-becf-ce5a53d756cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stats_cal.mean(\"train\", \"lepton_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36773f44-5153-4fcf-9814-c054f64f2723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stats_cal.mean(\"train\", \"m_wwbb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d0c62-1c7a-4a4c-bd98-3feb2c439268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stats_cal.stddev(\"train\", \"m_wwbb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4599e3-bd78-4152-b255-246f801a51dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_stats_cal.histogram(\"train\", \"lepton_pt\", 20, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3735304-860d-41f1-bd9b-f05b1f048e4b",
   "metadata": {},
   "source": [
    "Great ! The code works. Let's move to the federated statistics calculations. Befor we do that, we need to move back to the parent directory of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d8014-ed54-4d0e-bc96-adad905a29dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd ../."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19adbde-b204-4483-810b-56c5d1517112",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Federated Statistics Job\n",
    "\n",
    "We are going to use NVFLARE job cli to create job. For detailed instructions on Job CLI, please follow the [job cli tutorial](https://github.com/NVIDIA/NVFlare/blob/main/examples/tutorials/job_cli.ipynb)\n",
    "\n",
    "Let's check the available job templates, we are going to use one of the existing job templates and modify it to fit our needs. The job template is nothing but server and client-side job configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe46054-ada9-41da-8879-373c78ed2431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare job list_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c86f2b-5a2a-4138-88df-757fd763ef8f",
   "metadata": {},
   "source": [
    "there is \"stats_df\" job template, which what we need. We are going to use that. Now, use ```nvflare job create``` command\n",
    "We would like to use our new df_statistics.py file we just tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a307eea6-0ca8-46e5-a24b-13d7ff3a315f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare job create -w stats_df -j /tmp/nvflare/jobs/stats_df -sd code -force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7a6e07-c8c0-476d-84db-e3609f6dd0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tree /tmp/nvflare/jobs/stats_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ffdb3-7dbb-4b24-8252-39bfd51aeffa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's modify the server configuration to set the bin = 20, global min_max range in [0,10] instead of [0,120] and stats_writer output path  \"statistics/adults_stats.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bb6cb2-2e25-42ad-b881-8dea2aeb1391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare job create -w stats_df -force -j /tmp/nvflare/jobs/stats_df -sd code -f config_fed_server.conf bins=20 range=\"[0,10]\" output_path=\"statistics/stats.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699865f-8e9b-494d-ba4d-b6b952fb7cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/jobs/stats_df/app/config/config_fed_server.conf         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696eb79-dc4f-4bf7-a8a5-545f79de8891",
   "metadata": {},
   "source": [
    "Now, look at the client configuration, we notice that the job template component configuration \n",
    "```\n",
    "components = [\n",
    "  {\n",
    "    id = \"df_stats_generator\"\n",
    "    path = \"df_statistics.DFStatistics\"\n",
    "    args {\n",
    "      data_path = \"data.csv\"\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "is different from our new DFStatistics class, where the arguments are\n",
    "features, data_root_dir not \"data_path\". So we will need to modify that. \n",
    "\n",
    "```\n",
    "\n",
    "class DFStatistics(Statistics):\n",
    "    def __init__(self, data_root_dir: str):\n",
    "        super().__init__()\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.data: Optional[Dict[str, pd.DataFrame]] = None\n",
    "        self.data_features = None\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec04d1d-b3fd-4a38-b2eb-b43b9f04c7de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/jobs/stats_df/app/config/config_fed_client.conf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96a85a-056d-4594-a348-30e69142ea33",
   "metadata": {},
   "source": [
    "what we need to do are the followings\n",
    "1. remove data_path argument\n",
    "2. add data_root_dir arguments\n",
    "3. change the path of the DFStatistics class from 'df_statistics.DFStatistics' to df_stats.DFStatistics'\n",
    "\n",
    "We use the following syntax to do this (you can always open it with your editing tool to direct edit the file). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4d563-ec0e-4d03-a6ae-008d9ba62171",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvflare job create -w stats_df -force -j /tmp/nvflare/jobs/stats_df \\-sd code \\\n",
    "-f config_fed_server.conf \\\n",
    "   bins=20 \\\n",
    "   range=\"[0,10]\" \\\n",
    "   output_path=\"statistics/stats.json\" \\\n",
    "-f config_fed_client.conf \\\n",
    "   components[0].path=\"df_stats.DFStatistics\" \\\n",
    "   components[0].args.data_path- \\\n",
    "   components[0].args.data_root_dir=\"/tmp/nvflare/dataset/output\" -debug\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7118a07-9c90-4f9c-97a5-2dcfa58cb091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tree  /tmp/nvflare/jobs/stats_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b1fa7-1eef-42a4-b297-81289f4440c4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Run job in FL Simulator\n",
    "\n",
    "Now we can run the job with simulator. \n",
    "\n",
    "```\n",
    "nvflare simulator  /tmp/nvflare/jobs/stats_df  -w /tmp/nvflare/tabular/stats_df  -n 3 -t 3\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4ffde-627c-4f72-b114-fb9e92c1dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvflare simulator  /tmp/nvflare/jobs/stats_df  -w /tmp/nvflare/tabular/stats_df  -n 3 -t 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e981ae44-00b7-41b8-a2f0-4c49e05d6014",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The results are stored in \n",
    "```\n",
    "/tmp/nvflare/tabular/stats_df/simulate_job/statistics/stats.json\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6117ca2-c9bf-43f1-98ed-4eb3dc8f369b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -al  /tmp/nvflare/tabular/stats_df/simulate_job/statistics/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653f83f8-f96f-4943-af27-c5e6551d3449",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b76c3f3-af15-4ec9-bade-0ae2de86e24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cp /tmp/nvflare/tabular/stats_df/simulate_job/statistics//stats.json ./."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef772576-d87a-4a6e-b530-6a440e230839",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from nvflare.app_opt.statistics.visualization.statistics_visualization import Visualization\n",
    "with open('stats.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "vis = Visualization()\n",
    "vis.show_stats(data = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cdfe52-28a4-499f-9884-d833fec6d3cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100%  depth:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f354b9-6238-40be-91d9-67229c7b5891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vis.show_histograms(data = data, plot_type=\"main\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a3d942-9b8d-45de-85aa-b21d27ca60ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "The global and local histograms differences are none as we are using the same dataset for all clients. \n",
    "\n",
    "## We are done !\n",
    "Congratulations! you have just completed the federated stats calulation for tabular data. \n",
    "\n",
    "If you would like to see a detailed discussion regarding privacy filtering, please checkout the example in [federated statistics](https://github.com/NVIDIA/NVFlare/tree/main/examples/advanced/federated-statistics) examples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5e6273-5b79-4eb9-beb3-4ae96c21218c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
