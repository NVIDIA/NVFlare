{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf1c433-ba7f-4967-b066-670eb5e016ed",
   "metadata": {},
   "source": [
    "# Tabular Data Federated Statistics \n",
    "\n",
    "Before we perform machine learning tasks on the Higgs data, let's examine the statistics of the dataset. \n",
    "\n",
    "\n",
    "## Setup NVFLARE\n",
    "\n",
    "Follow [Getting Started](https://nvflare.readthedocs.io/en/main/getting_started.html) to set up a virtual environment and install NVFLARE.\n",
    "\n",
    "You can also follow this [notebook](../../nvflare_setup.ipynb) to get set up.\n",
    "\n",
    "> Make sure you have installed nvflare from **terminal** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf2d68-b020-457f-a1f1-b1f95509c929",
   "metadata": {},
   "source": [
    "## Install requirements\n",
    "assuming the current directory is 'higgs/stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a552a5eb-dbfb-42da-8cfa-082c1739012e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chester/projects/NVFlare/examples/hello-world/step-by-step/higgs/stats\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f373c84-b3d9-43e2-9e6a-9ff0db33f3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d5cd9a-3da9-446c-aac0-6ae84bf0ead1",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Prepare data\n",
    "\n",
    "### Download and Store Data\n",
    "\n",
    "To run the examples, we first download the dataset from the HIGGS link above, which is a single .csv file. By default, we assume the dataset is downloaded, uncompressed, and stored in \n",
    "\n",
    "```\n",
    "/tmp/nvflare/dataset/input/higgs.zip.\n",
    "\n",
    "```\n",
    "\n",
    "You can either use wget or curl to download directly if you have wget or curl installed. here is using curl command. This will takes a while to download 2.6+GB file. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa27561d-e665-4480-9707-2cf818a3f61f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2686M    0 2686M    0     0  1571k      0 --:--:--  0:29:10 --:--:-- 1355k 56.7M    0     0  1023k      0 --:--:--  0:00:56 --:--:-- 1623k:--:--  0:03:53 --:--:--  736k 0  1508k      0 --:--:--  0:09:24 --:--:-- 2963k    0 --:--:--  0:14:37 --:--:-- 1300kk      0 --:--:--  0:16:39 --:--:--  519k  0  1600k      0 --:--:--  0:20:23 --:--:-- 1593k 0:21:06 --:--:-- 1321k 0  1571k      0 --:--:--  0:24:46 --:--:-- 1188k\n"
     ]
    }
   ],
   "source": [
    "! mkdir -p /tmp/nvflare/dataset/input\n",
    "\n",
    "! curl -o /tmp/nvflare/dataset/input/higgs.zip https://archive.ics.uci.edu/static/public/280/higgs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bc3bf2-cd95-4fa9-90b3-c70a80043379",
   "metadata": {},
   "source": [
    "Alternative download with wget ```wget -P /tmp/nvflare/dataset/input/ https://archive.ics.uci.edu/static/public/280/higgs.zip```\n",
    "\n",
    "First we need to unzip the higgs.zip, we have already pre-installed \"unzip\" and \"gunzip\", so we just directly use this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e00d35a-5a04-4344-85bf-708a892ff967",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /tmp/nvflare/dataset/input/higgs.zip\n",
      "  inflating: /tmp/nvflare/dataset/input/HIGGS.csv.gz  \n"
     ]
    }
   ],
   "source": [
    "! unzip -d /tmp/nvflare/dataset/input/ /tmp/nvflare/dataset/input/higgs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1697bf5-dccd-4aee-b482-df8e411cec7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gunzip -c /tmp/nvflare/dataset/input/HIGGS.csv.gz > /tmp/nvflare/dataset/input/higgs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d98f2e8-afaa-475a-92a0-cd864cc631cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 13348436\n",
      "drwxrwxr-x 2 chester chester       4096 Nov 14 15:57 .\n",
      "drwxrwxr-x 3 chester chester       4096 Nov 14 15:22 ..\n",
      "-rw-rw-r-- 1 chester chester 8035497980 Nov 14 15:57 higgs.csv\n",
      "-rwx------ 1 chester chester 2816407858 May 22 15:16 HIGGS.csv.gz\n",
      "-rw-rw-r-- 1 chester chester 2816865137 Nov 14 15:51 higgs.zip\n"
     ]
    }
   ],
   "source": [
    "!ls -al /tmp/nvflare/dataset/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3ac98-3e9d-4a06-b7fb-0e2867266232",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "HIGGS dataset contains 11 million instances (rows), each with 28 attributes.\n",
    "The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. \n",
    "The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. The last 500,000 examples are used as a test set.\n",
    "\n",
    "The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features): lepton  pT, lepton  eta, lepton  phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb. For more detailed information about each feature see the original paper.\n",
    "\n",
    "Since HIGGS dataset is already randomly recorded, data split will be specified by the continuous index ranges for each client, rather than a vector of random instance indices. We will split the dataset uniformly: all clients has the same amount of data. The output directory \n",
    "\n",
    "```\n",
    "/tmp/nvflare/dataset/output/\n",
    "\n",
    "```\n",
    "\n",
    "We create a simple python code to split data: called split_csv.py. Let run this, need to wait for few minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0122950-f58d-4ae8-8abd-ed5a6c5c7b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python split_csv.py --input_data_path=/tmp/nvflare/dataset/input/higgs.csv --output_dir=/tmp/nvflare/dataset/output/ --site_num=3 --sample_rate=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "763fafdf-f45f-44cb-b445-c6ce978a8d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1087424\n",
      "drwxrwxr-x 2 chester chester      4096 Nov 14 18:24 .\n",
      "drwxrwxr-x 4 chester chester      4096 Nov 14 18:22 ..\n",
      "-rw-rw-r-- 1 chester chester 371161915 Nov 14 19:47 site-1.csv\n",
      "-rw-rw-r-- 1 chester chester 371163164 Nov 14 19:47 site-2.csv\n",
      "-rw-rw-r-- 1 chester chester 371171296 Nov 14 19:47 site-3.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -al /tmp/nvflare/dataset/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce2dd58-210f-4006-86ec-a821713d5cac",
   "metadata": {},
   "source": [
    "Now we have our data prepared, let first take a look at these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47416d89-74f9-4ede-b8b9-99f0eeaa9814",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\"label\", \"lepton_pt\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b_tag\", \"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b_tag\", \"jet_3_pt\", \"jet_3_eta\", \"jet_3_phi\", \"jet_3_b_tag\",\\\n",
    "            \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\", \"jet_4_b_tag\", \\\n",
    "            \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe4ab17c-8af7-43af-923d-becf6dec919c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['label',\n",
       " 'lepton_pt',\n",
       " 'lepton_eta',\n",
       " 'lepton_phi',\n",
       " 'missing_energy_magnitude',\n",
       " 'missing_energy_phi',\n",
       " 'jet_1_pt',\n",
       " 'jet_1_eta',\n",
       " 'jet_1_phi',\n",
       " 'jet_1_b_tag',\n",
       " 'jet_2_pt',\n",
       " 'jet_2_eta',\n",
       " 'jet_2_phi',\n",
       " 'jet_2_b_tag',\n",
       " 'jet_3_pt',\n",
       " 'jet_3_eta',\n",
       " 'jet_3_phi',\n",
       " 'jet_3_b_tag',\n",
       " 'jet_4_pt',\n",
       " 'jet_4_eta',\n",
       " 'jet_4_phi',\n",
       " 'jet_4_b_tag',\n",
       " 'm_jj',\n",
       " 'm_jjj',\n",
       " 'm_lv',\n",
       " 'm_jlv',\n",
       " 'm_bb',\n",
       " 'm_wbb',\n",
       " 'm_wwbb']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d7ad635-5b8a-4430-8bce-8e3bb4b0a0d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df: pd.DataFrame = pd.read_csv(\"/tmp/nvflare/dataset/output/site-1.csv\", names=features, sep=r\"\\s*,\\s*\", engine=\"python\", na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1751d35c-99ce-4bef-9b8f-af3a57507a46",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>lepton_pt</th>\n",
       "      <th>lepton_eta</th>\n",
       "      <th>lepton_phi</th>\n",
       "      <th>missing_energy_magnitude</th>\n",
       "      <th>missing_energy_phi</th>\n",
       "      <th>jet_1_pt</th>\n",
       "      <th>jet_1_eta</th>\n",
       "      <th>jet_1_phi</th>\n",
       "      <th>jet_1_b_tag</th>\n",
       "      <th>...</th>\n",
       "      <th>jet_4_eta</th>\n",
       "      <th>jet_4_phi</th>\n",
       "      <th>jet_4_b_tag</th>\n",
       "      <th>m_jj</th>\n",
       "      <th>m_jjj</th>\n",
       "      <th>m_lv</th>\n",
       "      <th>m_jlv</th>\n",
       "      <th>m_bb</th>\n",
       "      <th>m_wbb</th>\n",
       "      <th>m_wwbb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.869293</td>\n",
       "      <td>-0.635082</td>\n",
       "      <td>0.225690</td>\n",
       "      <td>0.327470</td>\n",
       "      <td>-0.689993</td>\n",
       "      <td>0.754202</td>\n",
       "      <td>-0.248573</td>\n",
       "      <td>-1.092064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010455</td>\n",
       "      <td>-0.045767</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>1.353760</td>\n",
       "      <td>0.979563</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>0.920005</td>\n",
       "      <td>0.721657</td>\n",
       "      <td>0.988751</td>\n",
       "      <td>0.876678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.907542</td>\n",
       "      <td>0.329147</td>\n",
       "      <td>0.359412</td>\n",
       "      <td>1.497970</td>\n",
       "      <td>-0.313010</td>\n",
       "      <td>1.095531</td>\n",
       "      <td>-0.557525</td>\n",
       "      <td>-1.588230</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.138930</td>\n",
       "      <td>-0.000819</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.302220</td>\n",
       "      <td>0.833048</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.978098</td>\n",
       "      <td>0.779732</td>\n",
       "      <td>0.992356</td>\n",
       "      <td>0.798343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.798835</td>\n",
       "      <td>1.470639</td>\n",
       "      <td>-1.635975</td>\n",
       "      <td>0.453773</td>\n",
       "      <td>0.425629</td>\n",
       "      <td>1.104875</td>\n",
       "      <td>1.282322</td>\n",
       "      <td>1.381664</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.128848</td>\n",
       "      <td>0.900461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.909753</td>\n",
       "      <td>1.108330</td>\n",
       "      <td>0.985692</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.803252</td>\n",
       "      <td>0.865924</td>\n",
       "      <td>0.780118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.344385</td>\n",
       "      <td>-0.876626</td>\n",
       "      <td>0.935913</td>\n",
       "      <td>1.992050</td>\n",
       "      <td>0.882454</td>\n",
       "      <td>1.786066</td>\n",
       "      <td>-1.646778</td>\n",
       "      <td>-0.942383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678379</td>\n",
       "      <td>-1.360356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946652</td>\n",
       "      <td>1.028704</td>\n",
       "      <td>0.998656</td>\n",
       "      <td>0.728281</td>\n",
       "      <td>0.869200</td>\n",
       "      <td>1.026736</td>\n",
       "      <td>0.957904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.105009</td>\n",
       "      <td>0.321356</td>\n",
       "      <td>1.522401</td>\n",
       "      <td>0.882808</td>\n",
       "      <td>-1.205349</td>\n",
       "      <td>0.681466</td>\n",
       "      <td>-1.070464</td>\n",
       "      <td>-0.921871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.373566</td>\n",
       "      <td>0.113041</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755856</td>\n",
       "      <td>1.361057</td>\n",
       "      <td>0.986610</td>\n",
       "      <td>0.838085</td>\n",
       "      <td>1.133295</td>\n",
       "      <td>0.872245</td>\n",
       "      <td>0.808487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733329</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.540426</td>\n",
       "      <td>1.305064</td>\n",
       "      <td>-0.708802</td>\n",
       "      <td>0.875872</td>\n",
       "      <td>1.675868</td>\n",
       "      <td>0.816495</td>\n",
       "      <td>-0.252534</td>\n",
       "      <td>-1.130316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.365237</td>\n",
       "      <td>-0.257189</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894322</td>\n",
       "      <td>0.952666</td>\n",
       "      <td>0.989141</td>\n",
       "      <td>1.504557</td>\n",
       "      <td>0.834270</td>\n",
       "      <td>1.229386</td>\n",
       "      <td>1.162086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733330</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.421653</td>\n",
       "      <td>-0.492882</td>\n",
       "      <td>1.643916</td>\n",
       "      <td>1.594480</td>\n",
       "      <td>0.825128</td>\n",
       "      <td>0.532879</td>\n",
       "      <td>0.796119</td>\n",
       "      <td>-0.674065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.105617</td>\n",
       "      <td>0.638542</td>\n",
       "      <td>3.101961</td>\n",
       "      <td>2.138724</td>\n",
       "      <td>1.226602</td>\n",
       "      <td>0.988765</td>\n",
       "      <td>0.644204</td>\n",
       "      <td>0.376055</td>\n",
       "      <td>0.756236</td>\n",
       "      <td>0.835567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733331</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.968118</td>\n",
       "      <td>0.351549</td>\n",
       "      <td>1.102926</td>\n",
       "      <td>0.639590</td>\n",
       "      <td>-1.318748</td>\n",
       "      <td>1.234316</td>\n",
       "      <td>-0.497121</td>\n",
       "      <td>0.751894</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>0.953955</td>\n",
       "      <td>-0.417558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.903102</td>\n",
       "      <td>0.884304</td>\n",
       "      <td>0.989137</td>\n",
       "      <td>0.934825</td>\n",
       "      <td>0.873791</td>\n",
       "      <td>0.967624</td>\n",
       "      <td>0.832607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733332</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.242998</td>\n",
       "      <td>-0.459767</td>\n",
       "      <td>-1.390171</td>\n",
       "      <td>0.780121</td>\n",
       "      <td>1.277013</td>\n",
       "      <td>1.059071</td>\n",
       "      <td>0.553513</td>\n",
       "      <td>-0.366384</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.093958</td>\n",
       "      <td>1.394887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628694</td>\n",
       "      <td>0.940251</td>\n",
       "      <td>0.987272</td>\n",
       "      <td>0.554953</td>\n",
       "      <td>0.830810</td>\n",
       "      <td>0.883622</td>\n",
       "      <td>0.719952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733333</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.559276</td>\n",
       "      <td>1.053780</td>\n",
       "      <td>0.327230</td>\n",
       "      <td>1.701451</td>\n",
       "      <td>1.296095</td>\n",
       "      <td>1.246499</td>\n",
       "      <td>1.347677</td>\n",
       "      <td>-0.593680</td>\n",
       "      <td>2.173076</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.243033</td>\n",
       "      <td>0.232347</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.510532</td>\n",
       "      <td>1.540546</td>\n",
       "      <td>1.035986</td>\n",
       "      <td>1.359093</td>\n",
       "      <td>1.511534</td>\n",
       "      <td>1.286836</td>\n",
       "      <td>1.132230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>733334 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label  lepton_pt  lepton_eta  lepton_phi  missing_energy_magnitude  \\\n",
       "0         1.0   0.869293   -0.635082    0.225690                  0.327470   \n",
       "1         1.0   0.907542    0.329147    0.359412                  1.497970   \n",
       "2         1.0   0.798835    1.470639   -1.635975                  0.453773   \n",
       "3         0.0   1.344385   -0.876626    0.935913                  1.992050   \n",
       "4         1.0   1.105009    0.321356    1.522401                  0.882808   \n",
       "...       ...        ...         ...         ...                       ...   \n",
       "733329    0.0   0.540426    1.305064   -0.708802                  0.875872   \n",
       "733330    0.0   0.421653   -0.492882    1.643916                  1.594480   \n",
       "733331    1.0   0.968118    0.351549    1.102926                  0.639590   \n",
       "733332    1.0   1.242998   -0.459767   -1.390171                  0.780121   \n",
       "733333    0.0   0.559276    1.053780    0.327230                  1.701451   \n",
       "\n",
       "        missing_energy_phi  jet_1_pt  jet_1_eta  jet_1_phi  jet_1_b_tag  ...  \\\n",
       "0                -0.689993  0.754202  -0.248573  -1.092064     0.000000  ...   \n",
       "1                -0.313010  1.095531  -0.557525  -1.588230     2.173076  ...   \n",
       "2                 0.425629  1.104875   1.282322   1.381664     0.000000  ...   \n",
       "3                 0.882454  1.786066  -1.646778  -0.942383     0.000000  ...   \n",
       "4                -1.205349  0.681466  -1.070464  -0.921871     0.000000  ...   \n",
       "...                    ...       ...        ...        ...          ...  ...   \n",
       "733329            1.675868  0.816495  -0.252534  -1.130316     0.000000  ...   \n",
       "733330            0.825128  0.532879   0.796119  -0.674065     0.000000  ...   \n",
       "733331           -1.318748  1.234316  -0.497121   0.751894     2.173076  ...   \n",
       "733332            1.277013  1.059071   0.553513  -0.366384     2.173076  ...   \n",
       "733333            1.296095  1.246499   1.347677  -0.593680     2.173076  ...   \n",
       "\n",
       "        jet_4_eta  jet_4_phi  jet_4_b_tag      m_jj     m_jjj      m_lv  \\\n",
       "0       -0.010455  -0.045767     3.101961  1.353760  0.979563  0.978076   \n",
       "1       -1.138930  -0.000819     0.000000  0.302220  0.833048  0.985700   \n",
       "2        1.128848   0.900461     0.000000  0.909753  1.108330  0.985692   \n",
       "3       -0.678379  -1.360356     0.000000  0.946652  1.028704  0.998656   \n",
       "4       -0.373566   0.113041     0.000000  0.755856  1.361057  0.986610   \n",
       "...           ...        ...          ...       ...       ...       ...   \n",
       "733329  -0.365237  -0.257189     0.000000  0.894322  0.952666  0.989141   \n",
       "733330  -1.105617   0.638542     3.101961  2.138724  1.226602  0.988765   \n",
       "733331   0.953955  -0.417558     0.000000  0.903102  0.884304  0.989137   \n",
       "733332  -1.093958   1.394887     0.000000  0.628694  0.940251  0.987272   \n",
       "733333  -1.243033   0.232347     0.000000  2.510532  1.540546  1.035986   \n",
       "\n",
       "           m_jlv      m_bb     m_wbb    m_wwbb  \n",
       "0       0.920005  0.721657  0.988751  0.876678  \n",
       "1       0.978098  0.779732  0.992356  0.798343  \n",
       "2       0.951331  0.803252  0.865924  0.780118  \n",
       "3       0.728281  0.869200  1.026736  0.957904  \n",
       "4       0.838085  1.133295  0.872245  0.808487  \n",
       "...          ...       ...       ...       ...  \n",
       "733329  1.504557  0.834270  1.229386  1.162086  \n",
       "733330  0.644204  0.376055  0.756236  0.835567  \n",
       "733331  0.934825  0.873791  0.967624  0.832607  \n",
       "733332  0.554953  0.830810  0.883622  0.719952  \n",
       "733333  1.359093  1.511534  1.286836  1.132230  \n",
       "\n",
       "[733334 rows x 29 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2cefa5-d579-45c0-815e-70dac06809c4",
   "metadata": {},
   "source": [
    "## Create a statistics calculator for the local tabular dataset\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.series import Series\n",
    "\n",
    "from nvflare.apis.fl_context import FLContext\n",
    "from nvflare.app_common.abstract.statistics_spec import BinRange, Feature, Histogram, HistogramType, Statistics\n",
    "from nvflare.app_common.statistics.numpy_utils import dtype_to_data_type, get_std_histogram_buckets\n",
    "\n",
    "\n",
    "class DFStatistics(Statistics):\n",
    "    def __init__(self, features: List, data_root_dir: str):\n",
    "        super().__init__()\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.data: Optional[Dict[str, pd.DataFrame]] = None\n",
    "        self.data_features = features\n",
    "\n",
    "    def load_data(self, fl_ctx: FLContext) -> Dict[str, pd.DataFrame]:\n",
    "        client_name = fl_ctx.get_identity_name() if fl_ctx is not None else \"site-1\"\n",
    "        if fl_ctx:\n",
    "            self.log_info(fl_ctx, f\"load data for client {client_name}\")\n",
    "        else:\n",
    "            print(f\"load data for client {client_name}\")\n",
    "        try:\n",
    "            data_path = f\"{self.data_root_dir}/{client_name}.csv\"\n",
    "            # example of load data from CSV\n",
    "            df: pd.DataFrame = pd.read_csv(\n",
    "                data_path, names=self.data_features, sep=r\"\\s*,\\s*\", engine=\"python\", na_values=\"?\"\n",
    "            )\n",
    "            return {\"train\": df}\n",
    "\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Load data for client {client_name} failed! {e}\")\n",
    "\n",
    "    def initialize(self, fl_ctx: FLContext):\n",
    "        self.data = self.load_data(fl_ctx)\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"data is not loaded. make sure the data is loaded\")\n",
    "\n",
    "    def features(self) -> Dict[str, List[Feature]]:\n",
    "        results: Dict[str, List[Feature]] = {}\n",
    "        for ds_name in self.data:\n",
    "            df = self.data[ds_name]\n",
    "            results[ds_name] = []\n",
    "            for feature_name in df:\n",
    "                data_type = dtype_to_data_type(df[feature_name].dtype)\n",
    "                results[ds_name].append(Feature(feature_name, data_type))\n",
    "\n",
    "        return results\n",
    "\n",
    "    def count(self, dataset_name: str, feature_name: str) -> int:\n",
    "        df: pd.DataFrame = self.data[dataset_name]\n",
    "        return df[feature_name].count()\n",
    "\n",
    "    def sum(self, dataset_name: str, feature_name: str) -> float:\n",
    "        df: pd.DataFrame = self.data[dataset_name]\n",
    "        return df[feature_name].sum().item()\n",
    "\n",
    "    def mean(self, dataset_name: str, feature_name: str) -> float:\n",
    "\n",
    "        count: int = self.count(dataset_name, feature_name)\n",
    "        sum_value: float = self.sum(dataset_name, feature_name)\n",
    "        return sum_value / count\n",
    "\n",
    "    def stddev(self, dataset_name: str, feature_name: str) -> float:\n",
    "        df = self.data[dataset_name]\n",
    "        return df[feature_name].std().item()\n",
    "\n",
    "    def variance_with_mean(\n",
    "        self, dataset_name: str, feature_name: str, global_mean: float, global_count: float\n",
    "    ) -> float:\n",
    "        df = self.data[dataset_name]\n",
    "        tmp = (df[feature_name] - global_mean) * (df[feature_name] - global_mean)\n",
    "        variance = tmp.sum() / (global_count - 1)\n",
    "        return variance.item()\n",
    "\n",
    "    def histogram(\n",
    "        self, dataset_name: str, feature_name: str, num_of_bins: int, global_min_value: float, global_max_value: float\n",
    "    ) -> Histogram:\n",
    "\n",
    "        num_of_bins: int = num_of_bins\n",
    "\n",
    "        df = self.data[dataset_name]\n",
    "        feature: Series = df[feature_name]\n",
    "        flattened = feature.ravel()\n",
    "        flattened = flattened[flattened != np.array(None)]\n",
    "        buckets = get_std_histogram_buckets(flattened, num_of_bins, BinRange(global_min_value, global_max_value))\n",
    "        return Histogram(HistogramType.STANDARD, buckets)\n",
    "\n",
    "    def max_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "        \"\"\"this is needed for histogram calculation, not used for reporting\"\"\"\n",
    "\n",
    "        df = self.data[dataset_name]\n",
    "        return df[feature_name].max()\n",
    "\n",
    "    def min_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "        \"\"\"this is needed for histogram calculation, not used for reporting\"\"\"\n",
    "\n",
    "        df = self.data[dataset_name]\n",
    "        return df[feature_name].min()\n",
    "\n",
    "```\n",
    "\n",
    "Let see if the code works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6c8e59bd-0b97-4ad9-a0de-abd10dd76d57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chester/projects/NVFlare/examples/hello-world/step-by-step/higgs/stats/code\n"
     ]
    }
   ],
   "source": [
    "cd code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "686d46c3-3986-4faf-bde8-1d4af749bf57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data for client site-1\n"
     ]
    }
   ],
   "source": [
    "from df_statistics import DFStatistics\n",
    "\n",
    "df_stats_cal = DFStatistics(features, data_root_dir = \"/tmp/nvflare/dataset/output\")\n",
    "\n",
    "# We use fl_ctx = None for local calculation ( where the data set default to \"site-1.csv\", so we can explore the stats locally without federated settings. \n",
    "df_stats_cal.initialize(fl_ctx = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cef9f923-54eb-4b7b-a23d-c93201554332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_features = df_stats_cal.features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "48c8f648-c5b9-4b71-b80c-431613c33dbc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': [Feature(feature_name='label', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='lepton_pt', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='lepton_eta', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='lepton_phi', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='missing_energy_magnitude', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='missing_energy_phi', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_1_pt', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_1_eta', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_1_phi', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_1_b_tag', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_2_pt', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_2_eta', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_2_phi', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_2_b_tag', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_3_pt', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_3_eta', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_3_phi', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_3_b_tag', data_type=<DataType.STRING: 2>),\n",
       "  Feature(feature_name='jet_4_pt', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_4_eta', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_4_phi', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='jet_4_b_tag', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='m_jj', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='m_jjj', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='m_lv', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='m_jlv', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='m_bb', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='m_wbb', data_type=<DataType.FLOAT: 1>),\n",
       "  Feature(feature_name='m_wwbb', data_type=<DataType.FLOAT: 1>)]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d0464f5e-b9ae-42d9-8bbe-7c180ce48767",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "733334"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats_cal.count(\"train\", \"lepton_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "31419dd6-afdd-4612-becf-ce5a53d756cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9919866823792409"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats_cal.mean(\"train\", \"lepton_pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "36773f44-5153-4fcf-9814-c054f64f2723",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.959972547557596"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats_cal.mean(\"train\", \"m_wwbb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a5d0c62-1c7a-4a4c-bd98-3feb2c439268",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3135064233941357"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats_cal.stddev(\"train\", \"m_wwbb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3c4599e3-bd78-4152-b255-246f801a51dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Histogram(hist_type=<HistogramType.STANDARD: 0>, bins=[Bin(low_value=0.0, high_value=0.5, sample_count=118338), Bin(low_value=0.5, high_value=1.0, sample_count=332406), Bin(low_value=1.0, high_value=1.5, sample_count=168176), Bin(low_value=1.5, high_value=2.0, sample_count=70749), Bin(low_value=2.0, high_value=2.5, sample_count=28184), Bin(low_value=2.5, high_value=3.0, sample_count=9404), Bin(low_value=3.0, high_value=3.5, sample_count=3527), Bin(low_value=3.5, high_value=4.0, sample_count=1433), Bin(low_value=4.0, high_value=4.5, sample_count=577), Bin(low_value=4.5, high_value=5.0, sample_count=294), Bin(low_value=5.0, high_value=5.5, sample_count=129), Bin(low_value=5.5, high_value=6.0, sample_count=58), Bin(low_value=6.0, high_value=6.5, sample_count=31), Bin(low_value=6.5, high_value=7.0, sample_count=15), Bin(low_value=7.0, high_value=7.5, sample_count=7), Bin(low_value=7.5, high_value=8.0, sample_count=5), Bin(low_value=8.0, high_value=8.5, sample_count=0), Bin(low_value=8.5, high_value=9.0, sample_count=1), Bin(low_value=9.0, high_value=9.5, sample_count=0), Bin(low_value=9.5, high_value=10.0, sample_count=0)], hist_name=None)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_stats_cal.histogram(\"train\", \"lepton_pt\", 20, 0, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3735304-860d-41f1-bd9b-f05b1f048e4b",
   "metadata": {},
   "source": [
    "Great ! The code works. Lets move on the federated statistics calculations. Let move back to the parent directory of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5b0d8014-ed54-4d0e-bc96-adad905a29dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chester/projects/NVFlare/examples/hello-world/step-by-step/higgs/stats\n"
     ]
    }
   ],
   "source": [
    "cd ../."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19adbde-b204-4483-810b-56c5d1517112",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Federated Statistics Job\n",
    "\n",
    "We are going to use NVFLARE job cli to create job. For detailed instructions on Job CLI, please follow the [job cli tutorial](https://github.com/NVIDIA/NVFlare/blob/main/examples/tutorials/job_cli.ipynb)\n",
    "\n",
    "Let's check the available job templates, we are going to use one of the existing job template and modify to fit our needs. The job template is nothing but server and client-side job configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cfe46054-ada9-41da-8879-373c78ed2431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following job templates are available: \n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  name                 Description                                                  Controller Type      Client Category     \n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "  cyclic_cc_pt         client-controlled cyclic workflow with PyTorch ClientAPI tra client               client_api          \n",
      "  cyclic_pt            server-controlled cyclic workflow with PyTorch ClientAPI tra server               client_api          \n",
      "  psi_csv              private-set intersection for csv data                        server               Executor            \n",
      "  sag_cross_np         scatter & gather and cross-site validation using numpy       server               client executor     \n",
      "  sag_cse_pt           scatter & gather workflow and cross-site evaluation with PyT server               client_api          \n",
      "  sag_gnn              scatter & gather workflow for gnn learning                   server               client_api          \n",
      "  sag_nemo             Scatter and Gather Workflow for NeMo                         server               client_api          \n",
      "  sag_np               scatter & gather workflow using numpy                        server               client_api          \n",
      "  sag_pt               scatter & gather workflow using pytorch                      server               client_api          \n",
      "  sag_pt_deploy_map    SAG workflow with pytorch, deploy_map, site-specific configs server               client_api          \n",
      "  sag_pt_executor      scatter & gather workflow and cross-site evaluation with PyT server               Executor            \n",
      "  sag_pt_model_learner scatter & gather workflow and cross-site evaluation with PyT server               ModelLearner        \n",
      "  sag_tf               scatter & gather workflow using TensorFlow                   server               client_api          \n",
      "  stats_df             FedStats: tabular data with pandas                           server               stats executor      \n",
      "  stats_image          FedStats: image intensity histogram                          server               stats executor      \n",
      "  swarm_cse_pt         Swarm Learning with Cross-Site Evaluation with PyTorch       client               client_api          \n",
      "  swarm_cse_pt_model_l Swarm Learning with Cross-Site Evaluation with PyTorch Model client               ModelLearner        \n",
      "  vertical_xgb         vertical federated xgboost                                   server               Executor            \n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "! nvflare job list_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c86f2b-5a2a-4138-88df-757fd763ef8f",
   "metadata": {},
   "source": [
    "there is \"stats_df\" job template, which what we need. We are going to use that. Now, use ```nvflare job create``` command\n",
    "We would like to use our new df_statistics.py file we just tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a307eea6-0ca8-46e5-a24b-13d7ff3a315f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following are the variables you can change in the template\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                                                                                       \n",
      "  job folder: /tmp/nvflare/jobs/stats_df                                                                                                 \n",
      "                                                                                                                                       \n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "  file_name                      var_name                       value                               component                          \n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "  meta.conf                      app                            ['@ALL']                                                               \n",
      "  meta.conf                      mandatory_clients              []                                                                     \n",
      "  meta.conf                      min_clients                    1                                                                      \n",
      "\n",
      "  config_fed_client.conf         data_path                      data.csv                            DFStatistics                       \n",
      "  config_fed_client.conf         max_bins_percent               10                                  HistogramBinsCleanser              \n",
      "  config_fed_client.conf         max_noise_level                0.3                                 AddNoiseToMinMax                   \n",
      "  config_fed_client.conf         min_count                      10                                  MinCountCleanser                   \n",
      "  config_fed_client.conf         min_noise_level                0.1                                 AddNoiseToMinMax                   \n",
      "  config_fed_client.conf         precision                      4                                                                      \n",
      "  config_fed_client.conf         result_cleanser_ids            ['min_count_cleanser', 'min_max_noi                                    \n",
      "\n",
      "  config_fed_server.conf         bins                           10                                                                     \n",
      "  config_fed_server.conf         enable_pre_run_task            False                               StatisticsController               \n",
      "  config_fed_server.conf         output_path                    statistics/adults_stats.json        JsonStatsFileWriter                \n",
      "  config_fed_server.conf         precision                      4                                   StatisticsController               \n",
      "  config_fed_server.conf         range                          [0, 120]                                                               \n",
      "  config_fed_server.conf         result_wait_timeout            10                                  StatisticsController               \n",
      "  config_fed_server.conf         wait_time_after_min_received   1                                   StatisticsController               \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "! nvflare job create -w stats_df -j /tmp/nvflare/jobs/stats_df -sd code -force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f7a6e07-c8c0-476d-84db-e3609f6dd0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/tmp/nvflare/jobs/stats_df\u001b[0m\n",
      "├── \u001b[01;34mapp\u001b[0m\n",
      "│   ├── \u001b[01;34mconfig\u001b[0m\n",
      "│   │   ├── config_fed_client.conf\n",
      "│   │   └── config_fed_server.conf\n",
      "│   └── \u001b[01;34mcustom\u001b[0m\n",
      "│       └── df_statistics.py\n",
      "└── meta.conf\n",
      "\n",
      "3 directories, 4 files\n"
     ]
    }
   ],
   "source": [
    "! tree /tmp/nvflare/jobs/stats_df  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738ffdb3-7dbb-4b24-8252-39bfd51aeffa",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's modify the server configuration to set the bin = 20, global min_max range in [0,10] instead of [0,120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e4bb6cb2-2e25-42ad-b881-8dea2aeb1391",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The following are the variables you can change in the template\n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "                                                                                                                                       \n",
      "  job folder: /tmp/nvflare/jobs/stats_df                                                                                                 \n",
      "                                                                                                                                       \n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "  file_name                      var_name                       value                               component                          \n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n",
      "  meta.conf                      app                            ['@ALL']                                                               \n",
      "  meta.conf                      mandatory_clients              []                                                                     \n",
      "  meta.conf                      min_clients                    1                                                                      \n",
      "\n",
      "  config_fed_client.conf         data_path                      data.csv                            DFStatistics                       \n",
      "  config_fed_client.conf         max_bins_percent               10                                  HistogramBinsCleanser              \n",
      "  config_fed_client.conf         max_noise_level                0.3                                 AddNoiseToMinMax                   \n",
      "  config_fed_client.conf         min_count                      10                                  MinCountCleanser                   \n",
      "  config_fed_client.conf         min_noise_level                0.1                                 AddNoiseToMinMax                   \n",
      "  config_fed_client.conf         precision                      4                                                                      \n",
      "  config_fed_client.conf         result_cleanser_ids            ['min_count_cleanser', 'min_max_noi                                    \n",
      "\n",
      "  config_fed_server.conf         bins                           20                                                                     \n",
      "  config_fed_server.conf         enable_pre_run_task            False                               StatisticsController               \n",
      "  config_fed_server.conf         output_path                    statistics/adults_stats.json        JsonStatsFileWriter                \n",
      "  config_fed_server.conf         precision                      4                                   StatisticsController               \n",
      "  config_fed_server.conf         range                          [0, 10]                                                                \n",
      "  config_fed_server.conf         result_wait_timeout            10                                  StatisticsController               \n",
      "  config_fed_server.conf         wait_time_after_min_received   1                                   StatisticsController               \n",
      "\n",
      "---------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "! nvflare job create -w stats_df -force -j /tmp/nvflare/jobs/stats_df -sd code -f config_fed_server.conf bins=20 range=\"[0,10]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8699865f-8e9b-494d-ba4d-b6b952fb7cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "format_version = 2\n",
      "workflows = [\n",
      "  {\n",
      "    id = \"fed_stats_controller\"\n",
      "    path = \"nvflare.app_common.workflows.statistics_controller.StatisticsController\"\n",
      "    args {\n",
      "      statistic_configs {\n",
      "        count {}\n",
      "        mean {}\n",
      "        sum {}\n",
      "        stddev {}\n",
      "        histogram {\n",
      "          \"*\" {\n",
      "            bins = 20\n",
      "          }\n",
      "          Age {\n",
      "            bins = 20\n",
      "            range = [\n",
      "              0\n",
      "              10\n",
      "            ]\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      writer_id = \"stats_writer\"\n",
      "      enable_pre_run_task = false\n",
      "    }\n",
      "  }\n",
      "]\n",
      "components = [\n",
      "  {\n",
      "    id = \"stats_writer\"\n",
      "    path = \"nvflare.app_common.statistics.json_stats_file_persistor.JsonStatsFileWriter\"\n",
      "    args {\n",
      "      output_path = \"statistics/adults_stats.json\"\n",
      "      json_encoder_path = \"nvflare.app_common.utils.json_utils.ObjectEncoder\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/nvflare/jobs/stats_df/app/config/config_fed_server.conf         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696eb79-dc4f-4bf7-a8a5-545f79de8891",
   "metadata": {},
   "source": [
    "Now, look at the client configuration, we notice that the job template component configuration \n",
    "```\n",
    "components = [\n",
    "  {\n",
    "    id = \"df_stats_generator\"\n",
    "    path = \"df_statistics.DFStatistics\"\n",
    "    args {\n",
    "      data_path = \"data.csv\"\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "is different from our new DFStatistics class, where the arguments are\n",
    "features, data_root_dir not \"data_path\". So we will need to modify that. \n",
    "\n",
    "```\n",
    "\n",
    "class DFStatistics(Statistics):\n",
    "    def __init__(self, features: List, data_root_dir: str):\n",
    "        super().__init__()\n",
    "        self.data_root_dir = data_root_dir\n",
    "        self.data: Optional[Dict[str, pd.DataFrame]] = None\n",
    "        self.data_features = features\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eec04d1d-b3fd-4a38-b2eb-b43b9f04c7de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "format_version = 2\n",
      "executors = [\n",
      "  {\n",
      "    tasks = [\n",
      "      \"fed_stats_pre_run\"\n",
      "      \"fed_stats\"\n",
      "    ]\n",
      "    executor {\n",
      "      id = \"Executor\"\n",
      "      path = \"nvflare.app_common.executors.statistics.statistics_executor.StatisticsExecutor\"\n",
      "      args {\n",
      "        generator_id = \"df_stats_generator\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "]\n",
      "task_result_filters = [\n",
      "  {\n",
      "    tasks = [\n",
      "      \"fed_stats\"\n",
      "    ]\n",
      "    filters = [\n",
      "      {\n",
      "        name = \"StatisticsPrivacyFilter\"\n",
      "        args {\n",
      "          result_cleanser_ids = [\n",
      "            \"min_count_cleanser\"\n",
      "            \"min_max_noise_cleanser\"\n",
      "            \"hist_bins_cleanser\"\n",
      "          ]\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n",
      "task_data_filters = []\n",
      "components = [\n",
      "  {\n",
      "    id = \"df_stats_generator\"\n",
      "    path = \"df_statistics.DFStatistics\"\n",
      "    args {\n",
      "      data_path = \"data.csv\"\n",
      "    }\n",
      "  }\n",
      "  {\n",
      "    id = \"min_max_cleanser\"\n",
      "    path = \"nvflare.app_common.statistics.min_max_cleanser.AddNoiseToMinMax\"\n",
      "    args {\n",
      "      min_noise_level = 0.1\n",
      "      max_noise_level = 0.3\n",
      "    }\n",
      "  }\n",
      "  {\n",
      "    id = \"hist_bins_cleanser\"\n",
      "    path = \"nvflare.app_common.statistics.histogram_bins_cleanser.HistogramBinsCleanser\"\n",
      "    args {\n",
      "      max_bins_percent = 10\n",
      "    }\n",
      "  }\n",
      "  {\n",
      "    id = \"min_count_cleanser\"\n",
      "    path = \"nvflare.app_common.statistics.min_count_cleanser.MinCountCleanser\"\n",
      "    args {\n",
      "      min_count = 10\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/nvflare/jobs/stats_df/app/config/config_fed_client.conf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf96a85a-056d-4594-a348-30e69142ea33",
   "metadata": {},
   "source": [
    "what we need to do are the followings\n",
    "1. remove data_path argument\n",
    "2. add features and data_root_dir arguments\n",
    "\n",
    "## TODO NEXT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4d563-ec0e-4d03-a6ae-008d9ba62171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e7b1fa7-1eef-42a4-b297-81289f4440c4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 2. Run job in FL Simulator\n",
    "\n",
    "With FL simulator, we can just run the example with CLI command \n",
    "\n",
    "\n",
    "```\n",
    "cd NVFlare/examples/advanced/federated-statistics\n",
    "nvflare simulator df_stats/jobs/df_stats -w /tmp/nvflare/df_stats -n 2 -t 2\n",
    "```\n",
    "\n",
    "The results are stored in workspace \"/tmp/nvflare\"\n",
    "```\n",
    "/tmp/nvflare/df_stats/simulate_job/statistics/adults_stats.json\n",
    "```\n",
    "\n",
    "## 3. Visualization\n",
    "   with json format, the data can be easily visualized via pandas dataframe and plots. \n",
    "   A visualization utility tools are showed in show_stats.py in visualization directory\n",
    "   You can run jupyter notebook visualization.ipynb\n",
    "\n",
    "   assuming NVFLARE_HOME env variable point to the GitHub project location (NVFlare) which contains current example. \n",
    "\n",
    "```bash\n",
    "    cp /tmp/nvflare/df_stats/simulate_job/advanced/statistics/adults_stats.json $NVFLARE_HOME/examples/advanced/federated-statistics/df_stats/demo/.\n",
    "    \n",
    "    cd $NVFLARE_HOME/examples/advanced/federated-statistics/df_stats/demo\n",
    "    \n",
    "    jupyter notebook  visualization.ipynb\n",
    "```\n",
    "you should be able to get the visualization similar to the followings\n",
    "\n",
    "![stats](demo/stats_df.png) and ![histogram plot](demo/hist_plot.png)\n",
    "\n",
    "\n",
    "## 4. Run Example using POC command\n",
    "\n",
    "Alternative way to run job is using POC mode\n",
    "\n",
    "### 4.1 Prepare POC Workspace\n",
    "\n",
    "```\n",
    "   nvflare poc prepare \n",
    "```\n",
    "This will create a poc at /tmp/nvflare/poc with n = 2 clients.\n",
    "\n",
    "If your poc_workspace is in a different location, use the following command\n",
    "\n",
    "```\n",
    "export NVFLARE_POC_WORKSPACE=<new poc workspace location>\n",
    "```\n",
    "then repeat above\n",
    "\n",
    "### 4.2 Start nvflare in POC mode\n",
    "\n",
    "```\n",
    "nvflare poc start\n",
    "```\n",
    "once you have done with above command, you are already login to the NVFLARE console (aka Admin Console)\n",
    "if you prefer to have NVFLARE Console in separate terminal, you can do\n",
    "\n",
    "```\n",
    "nvflare poc start ex admin\n",
    "```\n",
    "Then open a separate terminal to start the NVFLARE console\n",
    "```\n",
    "nvflare poc start -p admin\n",
    "```\n",
    "\n",
    "### 4.3 Submit job\n",
    "\n",
    "Inside the console, submit the job:\n",
    "```\n",
    "submit_job advanced/federated-statistics/df_stats/jobs/df_stats\n",
    "```\n",
    "\n",
    "### 4.4 List the submitted job\n",
    "\n",
    "You should see the server and clients in your first terminal executing the job now.\n",
    "You can list the running job by using `list_jobs` in the admin console.\n",
    "Your output should be similar to the following.\n",
    "\n",
    "```\n",
    "> list_jobs \n",
    "-------------------------------------------------------------------------------------------------==--------------------------------\n",
    "| JOB ID                               | NAME     | STATUS                       | SUBMIT TIME                                    |\n",
    "-----------------------------------------------------------------------------------------------------------------------------------\n",
    "| 10a92352-5459-47d2-8886-b85abf70ddd1 | df_stats | FINISHED:COMPLETED           | 2022-08-05T22:50:40.968771-07:00 | 0:00:29.4493|\n",
    "-----------------------------------------------------------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "### 4.5 Get the result\n",
    "\n",
    "If successful, the computed statis can be downloaded using this admin command:\n",
    "```\n",
    "download_job [JOB_ID]\n",
    "```\n",
    "After download, it will be available in the stated download directory under `[JOB_ID]/workspace/statistics` as  `adult_stats.json`\n",
    "then go to section [6. Visualization]\n",
    "\n",
    "## 5. Configuration and Code\n",
    "\n",
    "Since Flare has already developed the operators (server side controller and client side executor) for the federated\n",
    "statistics computing, we will only need to provide the followings\n",
    "* config_fed_server.json (server side controller configuration)\n",
    "* config_client_server.json (client side executor configuration)\n",
    "* local statistics calculator\n",
    "\n",
    "### 5.1 server side configuration\n",
    "\n",
    "```\n",
    "\"workflows\": [\n",
    "    {\n",
    "      \"id\": \"fed_stats_controller\",\n",
    "      \"path\": \"nvflare.app_common.workflows.statistics_controller.StatisticsController\",\n",
    "      \"args\": {\n",
    "        \"statistics_configs\": {\n",
    "          \"count\": {},\n",
    "          \"mean\": {},\n",
    "          \"sum\": {},\n",
    "          \"stddev\": {},\n",
    "          \"histogram\": { \"*\": {\"bins\": 10 },\n",
    "                         \"Age\": {\"bins\": 5, \"range\":[0,120]}\n",
    "                       }\n",
    "        },\n",
    "        \"writer_id\": \"stats_writer\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "```\n",
    "In above configuration, `StatisticsController` is controller. We ask the controller to calculate the following statistic\n",
    "statistics: \"count\", \"mean\", \"sum\", \"stddev\", \"histogram\" and \"Age\". Each statistic may have its own configuration.\n",
    "For example, Histogram statistic, we specify feature \"Age\" needs 5 bins and histogram range is within [0, 120), while for\n",
    "all other features (\"*\" indicate default feature), the bin is 10, range is not specified, i.e. the ranges will be dynamically estimated.\n",
    "\n",
    "The StatisticController also takes writer_id = \"stats_writer\", the writer_id identify the output writer component, defined as\n",
    "\n",
    "```\n",
    " \"components\": [\n",
    "    {\n",
    "      \"id\": \"stats_writer\",\n",
    "      \"path\": \"nvflare.app_common.statistics.json_stats_file_persistor.JsonStatsFileWriter\",\n",
    "      \"args\": {\n",
    "        \"output_path\": \"statistics/adults_stats.json\",\n",
    "        \"json_encoder_path\": \"nvflare.app_common.utils.json_utils.ObjectEncoder\"\n",
    "      }\n",
    "    }\n",
    "```\n",
    "This configuration shows a JSON file output writer, the result will be saved to the <job workspace>/\"statistics/adults_stats.json\",\n",
    "in FLARE job store.\n",
    "\n",
    "### 5.2 client side configuration\n",
    " \n",
    "First, we specify the built-in client side executor: `StatisticsExecutor`, which takes a local stats generator Id\n",
    "\n",
    "```\n",
    " \"executor\": {\n",
    "        \"id\": \"Executor\",\n",
    "        \"path\": \"nvflare.app_common.executors.statistics_executor.StatisticsExecutor\",\n",
    "        \"args\": {\n",
    "          \"generator_id\": \"df_stats_generator\",\n",
    "  },\n",
    "\n",
    "```\n",
    "\n",
    "The local statistics generator is defined as FLComponent: `DFStatistics` which implement the `Statistics` spec.\n",
    "\n",
    "```\n",
    "  \"components\": [\n",
    "    {\n",
    "      \"id\": \"df_stats_generator\",\n",
    "      \"path\": \"df_statistics.DFStatistics\",\n",
    "      \"args\": {\n",
    "        \"data_path\": \"data.csv\"\n",
    "      }\n",
    "    },\n",
    "   ...\n",
    "  ]\n",
    "```\n",
    "\n",
    "Next, we specify the `task_result_filters`. The task_result_filters are the post-process filter that takes the results\n",
    "of executor and then apply the filter before sending to server.\n",
    "\n",
    "In this example, task_result_filters is defined as task privacy filter : `StatisticsPrivacyFilter`\n",
    "```\n",
    "  \"task_result_filters\": [\n",
    "    {\n",
    "      \"tasks\": [\"fed_stats\"],\n",
    "      \"filters\":[\n",
    "        {\n",
    "          \"name\": \"StatisticsPrivacyFilter\",\n",
    "          \"args\": {\n",
    "            \"result_cleanser_ids\": [\n",
    "              \"min_count_cleanser\",\n",
    "              \"min_max_noise_cleanser\",\n",
    "              \"hist_bins_cleanser\"\n",
    "            ]\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "``` \n",
    "`StatisticsPrivacyFilter` is using three separate the `StatisticsPrivacyCleanser`, you can find more details in\n",
    "[local privacy policy](../local/privacy.json) and in later discussion on privacy.\n",
    "\n",
    "The privacy cleansers specify policy can be find in\n",
    "```\n",
    "  \"components\": [\n",
    "    {\n",
    "      \"id\": \"df_stats_generator\",\n",
    "      \"path\": \"df_statistics.DFStatistics\",\n",
    "      \"args\": {\n",
    "        \"data_path\": \"data.csv\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"min_max_cleanser\",\n",
    "      \"path\": \"nvflare.app_common.statistics.min_max_cleanser.AddNoiseToMinMax\",\n",
    "      \"args\": {\n",
    "        \"min_noise_level\": 0.1,\n",
    "        \"max_noise_level\": 0.3\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"hist_bins_cleanser\",\n",
    "      \"path\": \"nvflare.app_common.statistics.histogram_bins_cleanser.HistogramBinsCleanser\",\n",
    "      \"args\": {\n",
    "        \"max_bins_percent\": 10\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"min_count_cleanser\",\n",
    "      \"path\": \"nvflare.app_common.statistics.min_count_cleanser.MinCountCleanser\",\n",
    "      \"args\": {\n",
    "        \"min_count\": 10\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "\n",
    "```\n",
    "Or in [local private policy](../local/privacy.json)\n",
    "\n",
    "### 5.3 Local statistics generator\n",
    "\n",
    "The statistics generator `DFStatistics` implements `Statistics` spec.\n",
    "In current example, the input data in the format of Pandas DataFrame. Although we used csv file, but this can be any\n",
    "tabular data format that be expressed in pandas dataframe.\n",
    "\n",
    "```\n",
    "class DFStatistics(Statistics):\n",
    "    # rest of code \n",
    "```\n",
    "to calculate the local statistics, we will need to implements few methods\n",
    "```\n",
    "    def features(self) -> Dict[str, List[Feature]] -> Dict[str, List[Feature]]:\n",
    "\n",
    "    def count(self, dataset_name: str, feature_name: str) -> int:\n",
    " \n",
    "    def sum(self, dataset_name: str, feature_name: str) -> float:\n",
    " \n",
    "    def mean(self, dataset_name: str, feature_name: str) -> float:\n",
    " \n",
    "    def stddev(self, dataset_name: str, feature_name: str) -> float:\n",
    " \n",
    "    def variance_with_mean(self, dataset_name: str, feature_name: str, global_mean: float, global_count: float) -> float:\n",
    " \n",
    "    def histogram(self, dataset_name: str, feature_name: str, num_of_bins: int, global_min_value: float, global_max_value: float) -> Histogram:\n",
    "\n",
    "```\n",
    "since some of features do not provide histogram bin range, we will need to calculate based on local min/max to estimate\n",
    "the global min/max, and then use the global bin/max as the range for all clients' histogram bin range.\n",
    "\n",
    "so we need to provide local min/max calculation methods\n",
    "```\n",
    "   def max_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "   def min_value(self, dataset_name: str, feature_name: str) -> float:\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## to run pytest in examples\n",
    "\n",
    "under df_stats/jobs directory\n",
    "\n",
    "```\n",
    "pytest df_stats/custom/\n",
    "```\n",
    "\n",
    "### Data Split\n",
    "Since HIGGS dataset is already randomly recorded, data split will be specified by the continuous index ranges for each client, rather than a vector of random instance indices. We will split the dataset uniformly: all clients has the same amount of data. The output directory \n",
    "\n",
    "```\n",
    "\n",
    "/tmp/nvflare/dataset/output/\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb4ffde-627c-4f72-b114-fb9e92c1dc6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5278a0-128c-4c53-82c3-550bb19ebeee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
