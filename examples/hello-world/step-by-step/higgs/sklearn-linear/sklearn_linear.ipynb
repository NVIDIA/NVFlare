{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ccbfaee-9efe-4fe1-bab6-60462769fede",
   "metadata": {},
   "source": [
    "# Federated Linear Model with Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5ea300-8fd1-4568-8121-9bb9d3d303b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before do the training, we need to setup NVFLARE\n",
    "\n",
    "## Setup NVFLARE\n",
    "\n",
    "Follow [Getting Started](https://nvflare.readthedocs.io/en/main/getting_started.html) to set up a virtual environment and install NVFLARE.\n",
    "\n",
    "You can also follow this [notebook](https://github.com/NVIDIA/NVFlare/blob/main/examples/nvflare_setup.ipynb) to get set up.\n",
    "\n",
    "> Make sure you have installed nvflare from **terminal** \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c1eb8-3070-45b8-a83c-f16d060d0385",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install requirements\n",
    "assuming the current directory is 'higgs/stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3f5a8f-cd09-441e-a4c3-f1a121fd8244",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7bf5e9-1f21-430a-9227-3e912eba5f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87c3da-59dc-4551-9448-b20b64a57137",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare data\n",
    "Please reference [prepare_higgs_data](../prepare_data.ipynb) notebooks. Pay attention to the current location. You need to switch \"higgs\" directory to run the data split.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe5bc9a-d589-4d48-89b8-73fc1d8fe44b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we have our data prepared. we are ready to do the training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc62f2-5571-44d0-bb8b-b6a660a500c5",
   "metadata": {},
   "source": [
    "### Data Cleaning \n",
    "\n",
    "We noticed from time-to-time the Higgs dataset is making small changes which causing job to fail. so we need to do some clean up or skip certain rows. \n",
    "For example: certain floating number mistakenly add an alphabetical letter at some point of time. This may have already fixed by UCI. \n",
    "\n",
    "As of today: 2023-11-21, the dataset we downloaded seems to have some value that can't convert to floating point. value such as \"0.000000000000000000e+00.1\"\n",
    "\n",
    "Here we notice this seems to be at 1st row of the files. so we are going to use skip_rows during data loading to avoid this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dbec4e-b4a9-44fe-954e-62f1d0f99c40",
   "metadata": {},
   "source": [
    "## Scikit-learn\n",
    "This example shows how to use [NVIDIA FLARE](https://nvflare.readthedocs.io/en/main/index.html) on tabular data.\n",
    "It uses [Scikit-learn](https://scikit-learn.org/), a widely used \n",
    "open-source machine learning library that supports supervised and unsupervised learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80eb797-f1aa-4537-ab50-4c27fb9cff60",
   "metadata": {},
   "source": [
    "### Federated Linear Model\n",
    "Here we use of [linear classifiers with SGD training](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) in a federated scenario.\n",
    "Under this setting, federated learning can be formulated as a [FedAvg](https://arxiv.org/abs/1602.05629) process with local training that each client optimizes the local model starting from global parameters with SGD. \n",
    "\n",
    "This can be achieved by setting the `warm_start` flag of SGDClassifier to `True` in order to allow repeated fitting of the classifiers to the local data. \n",
    "\n",
    "Let's look at the code see how we convert the traditional Linear model to the federated model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5d82b-3c5a-492a-bc92-86ca7367ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdbcc96-3ce0-4e9e-a986-f877e082c0ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat code/sgd_fl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d932e8c-7c7c-470b-8448-43288feba261",
   "metadata": {
    "tags": []
   },
   "source": [
    "The code is pretty much like standard scikit learn training program\n",
    "\n",
    "#### load data\n",
    "\n",
    "We first load the features from the header file: \n",
    "    \n",
    "```\n",
    "    site_name = flare.get_site_name()\n",
    "    feature_data_path = f\"{data_root_dir}/{site_name}_header.csv\"\n",
    "    features = load_features(feature_data_path)\n",
    "    n_features = len(features) -1\n",
    "\n",
    "    data_path = f\"{data_root_dir}/{site_name}.csv\"\n",
    "    data = load_data(data_path=data_path, data_features=features, test_size=test_size, skip_rows=skip_rows)\n",
    "\n",
    "```\n",
    "\n",
    "then load the data from the main csv file, then transform the data and split the training and test data based on the test_size provided.  \n",
    "\n",
    "```\n",
    "    data = to_dataset_tuple(data)\n",
    "    dataset = transform_data(data)\n",
    "    x_train, y_train, train_size = dataset[\"train\"]\n",
    "    x_test, y_test, test_size = dataset[\"test\"]\n",
    "\n",
    "```\n",
    "\n",
    "The part that's specific to Federated Learning is in the following codes\n",
    "\n",
    "```\n",
    "# (1) import nvflare client API\n",
    "from nvflare import client as flare\n",
    "\n",
    "```\n",
    "```\n",
    "# (2) initializes NVFlare client API\n",
    "    flare.init()\n",
    "\n",
    "    site_name = flare.get_site_name()\n",
    "    \n",
    "```\n",
    "    \n",
    "These few lines, import NVFLARE Client API and initialize it, then use the API to find the site_name (such as site-1, site-2 etc.). With the site-name, we can construct the site-specific \n",
    "data path such as\n",
    "\n",
    "```\n",
    "    feature_data_path = f\"{data_root_dir}/{site_name}_header.csv\"\n",
    "\n",
    "    data_path = f\"{data_root_dir}/{site_name}.csv\"\n",
    "```\n",
    "\n",
    "#### Training \n",
    "\n",
    "In the standard traditional scikit learn, we would construct the model such as\n",
    "```\n",
    "  model = SGDClassifier(...) \n",
    "```\n",
    "then call model.fit(...)\n",
    "```\n",
    "  model.fit(x_train, y_train)\n",
    "\n",
    "  accuracy, report = evaluate_model(x_test, model, y_test)\n",
    "\n",
    "```\n",
    "\n",
    "with federated learning, using FLARE Client API, we have to make a few changes\n",
    "* 1) we are not only training in local iterations, but also global rounds, we need to keep the program running until we reached to the totoal number of rounds \n",
    "  \n",
    "  ```\n",
    "      while flare.is_running():\n",
    "          ... rest of code\n",
    "  \n",
    "  ```\n",
    "  \n",
    "* 2) Unlike traditional machine learning, we have now have more than one clients/sites participating the training. To ensure every site starts with the same model parameters, we use server to broadcase the initial model parameters to every sites at the first round ( current_round = 0). \n",
    "\n",
    "* 3) We will need to use FLARE client API to receive global model and find out the global parameters\n",
    "\n",
    "```\n",
    "     # (3) receives FLModel from NVFlare\n",
    "        input_model = flare.receive()\n",
    "        global_params = input_model.params\n",
    "        curr_round = input_model.current_round\n",
    "```\n",
    "\n",
    "```\n",
    "      if curr_round == 0:\n",
    "            # (4) initialize model with global_param\n",
    "            # and set to all zero\n",
    "            fit_intercept = bool(global_params[\"fit_intercept\"])\n",
    "            model = SGDClassifier(\n",
    "                loss=global_params[\"loss\"],\n",
    "                penalty=global_params[\"penalty\"],\n",
    "                fit_intercept=fit_intercept,\n",
    "                learning_rate=global_params[\"learning_rate\"],\n",
    "                eta0=global_params[\"eta0\"],\n",
    "                max_iter=1,\n",
    "                warm_start=True,\n",
    "                random_state=random_state,\n",
    "            )\n",
    "        ....\n",
    "```\n",
    "* 4) if it is not the first round, we need to use the global model to update the local model before training the next round. For Scikit-learn SGDClassifier, we simply update coeffient and intercept. \n",
    "\n",
    "```\n",
    "         # (5) update model based on global parameters\n",
    "            # the model has warm_start, so these parameters will be used in initialize the training\n",
    "            if \"coef\" in global_params:\n",
    "                model.coef_ = global_params[\"coef\"]\n",
    "            if model.fit_intercept and \"intercept\" in global_params:\n",
    "                model.intercept_ = global_params[\"intercept\"]\n",
    "```\n",
    "\n",
    "* 5) to make sure we have the best global model, we need to evaluate the global model using the local data\n",
    "\n",
    "```\n",
    "   # (6) evaluate global model first.\n",
    "        global_accuracy, global_report = evaluate_model(x_test, model, y_test)\n",
    "```\n",
    "* 6) finally we do the training as before.\n",
    "\n",
    "```\n",
    "        # Train the model on the training set\n",
    "        model.fit(x_train, y_train)\n",
    "        \n",
    "        accuracy, report = evaluate_model(x_test, model, y_test)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"local model Accuracy: {accuracy:.2f}\")\n",
    "        print(\"local model Classification Report:\\n\", report)\n",
    "        \n",
    "```\n",
    "\n",
    "* 7) we need the new training result (coeffient and intercept) back to the global model for aggregation, to do that, we have the following code\n",
    "\n",
    "```\n",
    "        # (7) construct trained FL model\n",
    "        params = {\"coef\": model.coef_, \"intercept\": model.intercept_}\n",
    "        metrics = {\"accuracy\": global_accuracy, \"report\": global_report}\n",
    "        output_model = flare.FLModel(params=params, metrics=metrics)\n",
    "\n",
    "        # (8) send model back to NVFlare\n",
    "        flare.send(output_model)\n",
    "```\n",
    "\n",
    "## Prepare Job  \n",
    "\n",
    "Now, we have the code, we need to prepare job folder with configurations to run in NVFLARE. To do this, we can leveage the job template for scikit learn. First look at the the available job templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fdd962-e63c-4b58-81e7-beeedd05509b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare job list_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7602cec-5fb5-4b23-a82a-b7444f2af471",
   "metadata": {},
   "source": [
    "the sklearn_linear is the one we need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899a684-444f-4c04-a388-4bdd9b7dc649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare job create -j /tmp/nvflare/jobs/sklearn_sgd -force -w sklearn_linear \\\n",
    "-sd code \\\n",
    "-f config_fed_client.conf  app_script=\"sgd_fl.py\" app_config=\"--data_root_dir /tmp/nvflare/dataset/output --test_size 0.2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc5dcc-573e-479e-965d-0cca8b58995a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! cat /tmp/nvflare/jobs/sklearn_sgd/app/config/config_fed_client.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3086fd-ecf3-4584-a643-5706ae2069b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! tree  /tmp/nvflare/jobs/sklearn_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111b784-1722-4aff-a48b-fc50c6c67281",
   "metadata": {},
   "source": [
    ">Note \n",
    " we use skip_rows = 0 to skip 1st row. We could skip_rows = [0, 3] to skip first and 4th rows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488ff6b6-5e39-4636-8a48-44d2b503dcdb",
   "metadata": {},
   "source": [
    "\n",
    "## Run job in simulator\n",
    "\n",
    "We use the simulator to run this job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09800691-46e0-4a95-bea6-d2a4a425052b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! nvflare simulator  /tmp/nvflare/jobs/sklearn_sgd   -w /tmp/nvflare/sklearn_sgd  -n 3 -t 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e9cda-39b9-4f9e-b814-220d0c8e5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree  /tmp/nvflare/sklearn_sgd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b17316-a2e2-4004-909d-e6fd7a583685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/sklearn_sgd/simulate_job/log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950bb2a5-80e1-4de8-9729-f05ced13808d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
