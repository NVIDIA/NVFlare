{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3220cbc8-47aa-4825-bc80-c504d9fa1bb9",
   "metadata": {},
   "source": [
    "# Prepare Data for Higgs Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6fa44-7dcb-4072-8b27-f722147102f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install requirements\n",
    "We will need pandas for the data preparation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747456a-978a-4ebd-8fad-d3b6f71886aa",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare data\n",
    "\n",
    "### Download and Store Data\n",
    "\n",
    "To run the examples, we first download the dataset from the HIGGS website. We will download, uncompress, and store the dataset under \n",
    "\n",
    "```\n",
    "/tmp/nvflare/dataset/input/\n",
    "\n",
    "```\n",
    "\n",
    "You can either use wget or curl to download directly if you have wget or curl installed. Here we use curl command. It will take a while to download the  2.6+ GB zip file. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4914b-79ed-444b-bdd5-0cae13a789a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p /tmp/nvflare/dataset/input\n",
    "\n",
    "!curl -o /tmp/nvflare/dataset/input/higgs.zip https://archive.ics.uci.edu/static/public/280/higgs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e10ba2-1c60-4253-9a6a-581f8faeaf57",
   "metadata": {},
   "source": [
    "Alternatively, download with wget ```wget -P /tmp/nvflare/dataset/input/ https://archive.ics.uci.edu/static/public/280/higgs.zip```\n",
    "\n",
    "With the downloaded zip file, we will unzip it with the pre-installed \"unzip\" and \"gunzip\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03df95-8d94-4027-9538-91cd00422468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip -d /tmp/nvflare/dataset/input/ /tmp/nvflare/dataset/input/higgs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb921b-731c-4733-a573-d4bb4291e556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gunzip -c /tmp/nvflare/dataset/input/HIGGS.csv.gz > /tmp/nvflare/dataset/input/higgs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c709e39-f494-4418-a9a5-745cb48294c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's check our current files under the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c27c2d0-9b20-4a6f-9dd6-ca113fb3e49b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -al /tmp/nvflare/dataset/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf4f685-31b3-499b-881e-fc5f73b3f662",
   "metadata": {},
   "source": [
    "### Data Split \n",
    "\n",
    "HIGGS dataset contains 11 million instances (rows), each with 28 attributes.\n",
    "The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. \n",
    "The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. The last 500,000 examples are used as a test set.\n",
    "\n",
    "The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features): lepton  pT, lepton  eta, lepton  phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb. For more detailed information about each feature, please see the original paper.\n",
    "\n",
    "We will split the dataset uniformly: all clients has the same amount of data under the output directory \n",
    "\n",
    "```\n",
    "/tmp/nvflare/dataset/output/\n",
    "\n",
    "```\n",
    "\n",
    "First to make it similar to the real world use cases, we generate a header file to store feature names (CSV file headers) in the data directory. \n",
    "\n",
    "#### Generate the csv header file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5c919-9d99-494f-b216-8431ff2f23c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Your list of data\n",
    "features = [\"label\", \"lepton_pt\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b_tag\", \"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b_tag\", \"jet_3_pt\", \"jet_3_eta\", \"jet_3_phi\", \"jet_3_b_tag\",\\\n",
    "            \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\", \"jet_4_b_tag\", \\\n",
    "            \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"]\n",
    "\n",
    "# Specify the file path\n",
    "file_path =  '/tmp/nvflare/dataset/input/headers.csv'\n",
    "\n",
    "with open(file_path, 'w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(features)\n",
    "\n",
    "print(f\"features written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb485b-f28b-42aa-935e-3bdcd7087047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/dataset/input/headers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010fcb6c-756a-40f1-afd7-8039d561298d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now assume you are on the \"/examples/hello-world/step-by-step/higgs\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a243cb-5857-4cc1-8c40-49f455d5745d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838fcab1-c46e-4f5d-a2d8-564ee8c40d9a",
   "metadata": {},
   "source": [
    "#### Split higgs.csv into multiple csv files for clients\n",
    "\n",
    "Then we split the data into multiple files, one for each site. We make sure each site will has a \"header.csv\" file corresponding to the csv data. In horizontal split, all the header will be the same; while for vertical learning, each site can have different headers. \n",
    "\n",
    "First, we install the requirements, assuming the current directory is '/examples/hello-world/step-by-step/higgs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6db4f96-aa72-471a-a695-89327686ee2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f48bd-d9ea-4119-8b6f-38c65a48f9a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70974257-5435-4848-842e-791904ea3b40",
   "metadata": {},
   "source": [
    "In this tutorial, we set to 3 clients with uniform split. To do so, simply run `split_csv.py`. It is going to take a few minutes. \n",
    "\n",
    ">note \n",
    "    we used a sample rate of 0.3 to make demo faster to run. You can change the number to even smaller such 0.003 to reduce the file size especially under development or debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b11273-230f-47e5-a4bb-c3672d5d862b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python split_csv.py \\\n",
    "  --input_data_path=/tmp/nvflare/dataset/input/higgs.csv \\\n",
    "  --input_header_path=/tmp/nvflare/dataset/input/headers.csv \\\n",
    "  --output_dir=/tmp/nvflare/dataset/output/ \\\n",
    "  --site_num=3 \\\n",
    "  --sample_rate=0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a74c9a-a5ac-435a-a9dd-0e697f588798",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now let's check the files and their instance counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23662631-1a51-475b-9eaf-74f86516196d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -al /tmp/nvflare/dataset/output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4553632-f692-4529-91f4-25acf6b2bc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wc -l /tmp/nvflare/dataset/output/site-1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149a765-b55d-473f-ade2-02e1c8181126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wc -l /tmp/nvflare/dataset/output/site-2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03638ede-81e0-434b-8477-1d7440fab52d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wc -l /tmp/nvflare/dataset/output/site-3.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8826f-b786-4715-b2fe-3b62a3b0a3ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we have our data prepared. we are ready to do other computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b862d96-58a4-4454-9050-0278e4a2da44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
