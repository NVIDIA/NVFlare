{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3220cbc8-47aa-4825-bc80-c504d9fa1bb9",
   "metadata": {},
   "source": [
    "# Prepare Data for Higgs Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c6fa44-7dcb-4072-8b27-f722147102f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install requirements\n",
    "We will need pandas for the data preparation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747456a-978a-4ebd-8fad-d3b6f71886aa",
   "metadata": {},
   "source": [
    "\n",
    "## Prepare data\n",
    "\n",
    "### Download and Store Data\n",
    "\n",
    "To run the examples, we first download the dataset from the HIGGS link above, which is a single .csv file. By default, we assume the dataset is downloaded, uncompressed, and stored in \n",
    "\n",
    "```\n",
    "/tmp/nvflare/dataset/input/higgs.zip.\n",
    "\n",
    "```\n",
    "\n",
    "You can either use wget or curl to download directly if you have wget or curl installed. here is using curl command. This will takes a while to download 2.6+GB file. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b4914b-79ed-444b-bdd5-0cae13a789a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! mkdir -p /tmp/nvflare/dataset/input\n",
    "\n",
    "! curl -o /tmp/nvflare/dataset/input/higgs.zip https://archive.ics.uci.edu/static/public/280/higgs.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e10ba2-1c60-4253-9a6a-581f8faeaf57",
   "metadata": {},
   "source": [
    "Alternative download with wget ```wget -P /tmp/nvflare/dataset/input/ https://archive.ics.uci.edu/static/public/280/higgs.zip```\n",
    "\n",
    "First we need to unzip the higgs.zip, we have already pre-installed \"unzip\" and \"gunzip\", so we just directly use this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a03df95-8d94-4027-9538-91cd00422468",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! unzip -d /tmp/nvflare/dataset/input/ /tmp/nvflare/dataset/input/higgs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcb921b-731c-4733-a573-d4bb4291e556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gunzip -c /tmp/nvflare/dataset/input/HIGGS.csv.gz > /tmp/nvflare/dataset/input/higgs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08774782-790c-4a7a-a48c-bbcc11f633cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -al /tmp/nvflare/dataset/input/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf4f685-31b3-499b-881e-fc5f73b3f662",
   "metadata": {},
   "source": [
    "### Data Split \n",
    "\n",
    "HIGGS dataset contains 11 million instances (rows), each with 28 attributes.\n",
    "The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. \n",
    "The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. The last 500,000 examples are used as a test set.\n",
    "\n",
    "The first column is the class label (1 for signal, 0 for background), followed by the 28 features (21 low-level features then 7 high-level features): lepton  pT, lepton  eta, lepton  phi, missing energy magnitude, missing energy phi, jet 1 pt, jet 1 eta, jet 1 phi, jet 1 b-tag, jet 2 pt, jet 2 eta, jet 2 phi, jet 2 b-tag, jet 3 pt, jet 3 eta, jet 3 phi, jet 3 b-tag, jet 4 pt, jet 4 eta, jet 4 phi, jet 4 b-tag, m_jj, m_jjj, m_lv, m_jlv, m_bb, m_wbb, m_wwbb. For more detailed information about each feature see the original paper.\n",
    "\n",
    "Since HIGGS dataset is already randomly recorded, data split will be specified by the continuous index ranges for each client, rather than a vector of random instance indices. We will split the dataset uniformly: all clients has the same amount of data. The output directory \n",
    "\n",
    "```\n",
    "/tmp/nvflare/dataset/output/\n",
    "\n",
    "```\n",
    "\n",
    "#### Generate the csv header file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5c919-9d99-494f-b216-8431ff2f23c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Your list of data\n",
    "features = [\"label\", \"lepton_pt\", \"lepton_eta\", \"lepton_phi\", \"missing_energy_magnitude\", \"missing_energy_phi\", \"jet_1_pt\", \"jet_1_eta\", \"jet_1_phi\", \"jet_1_b_tag\", \"jet_2_pt\", \"jet_2_eta\", \"jet_2_phi\", \"jet_2_b_tag\", \"jet_3_pt\", \"jet_3_eta\", \"jet_3_phi\", \"jet_3_b_tag\",\\\n",
    "            \"jet_4_pt\", \"jet_4_eta\", \"jet_4_phi\", \"jet_4_b_tag\", \\\n",
    "            \"m_jj\", \"m_jjj\", \"m_lv\", \"m_jlv\", \"m_bb\", \"m_wbb\", \"m_wwbb\"]\n",
    "\n",
    "# Specify the file path\n",
    "file_path =  '/tmp/nvflare/dataset/input/headers.csv'\n",
    "\n",
    "with open(file_path, 'w', newline='') as file:\n",
    "    csv_writer = csv.writer(file)\n",
    "    csv_writer.writerow(features)\n",
    "\n",
    "print(f\"features written to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eb485b-f28b-42aa-935e-3bdcd7087047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat /tmp/nvflare/dataset/input/headers.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010fcb6c-756a-40f1-afd7-8039d561298d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now assume you are on the \"higgs\" directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a243cb-5857-4cc1-8c40-49f455d5745d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838fcab1-c46e-4f5d-a2d8-564ee8c40d9a",
   "metadata": {},
   "source": [
    "#### Split higgs.csv into multiple csv files\n",
    "\n",
    "To make it similar to the real world use cases, we put features (CSV file headers) into a file in the input directory.  When we split the file, we make sure each site will has a \"header.csv\" file corresponding to the csv data. In horizontal split. all the header will be the same. but for vertical learning, each site may have different headers. \n",
    "\n",
    "We then split the single higgs.csv into multiple csv files, one for each site. All sites have the same size of file. To do so, we create a simple python code: called split_csv.py. Let's run this, you will need to wait for few minutes. \n",
    "\n",
    ">note \n",
    "    we used certain sample rate to make demo faster to run. You can change the number to even smaller such 0.003 to reduce the file size especially when you development or debugging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b11273-230f-47e5-a4bb-c3672d5d862b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python split_csv.py \\\n",
    "  --input_data_path=/tmp/nvflare/dataset/input/higgs.csv \\\n",
    "  --input_header_path=/tmp/nvflare/dataset/input/headers.csv \\\n",
    "  --output_dir=/tmp/nvflare/dataset/output/ \\\n",
    "  --site_num=3 \\\n",
    "  --sample_rate=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29502248-aa2a-4435-b1c3-b13269d916d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -al /tmp/nvflare/dataset/output/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d8826f-b786-4715-b2fe-3b62a3b0a3ef",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we have our data prepared. we are ready to do other computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4553632-f692-4529-91f4-25acf6b2bc39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! wc -l /tmp/nvflare/dataset/output/site-1.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
