{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d686a43c-7f91-465c-b09e-6d41ae92b215",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NVIDIA FLARE Hello World Examples\n",
    "\n",
    "Welcome to NVIDIA FLARE Hello World examples! This notebook demonstrates how to use **Job Recipes** to quickly build and run federated learning applications.\n",
    "\n",
    "It is recommended to create a virtual environment and run everything within a virtualenv.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Prerequisites](#Prerequisites)\n",
    "2. [Example 1: Hello PyTorch](#Example-1:-Hello-PyTorch) - Image classification with CIFAR-10\n",
    "3. [Example 2: Hello NumPy](#Example-2:-Hello-NumPy) - Understanding FL basics\n",
    "4. [Example 3: Hello PyTorch Lightning](#Example-3:-Hello-PyTorch-Lightning) - Lightning integration\n",
    "5. [Example 4: Hello TensorFlow](#Example-4:-Hello-TensorFlow) - MNIST digit classification\n",
    "6. [Example 5: Hello Flower](#Example-5:-Hello-Flower) - Flower on FLARE infrastructure\n",
    "7. [Summary](#Summary)\n",
    "\n",
    "## What's New in FLARE 2.7\n",
    "\n",
    "NVIDIA FLARE now features **Job Recipes** - a simplified API that makes federated learning as easy as running a Python script. Instead of manually configuring workflows and components, you simply:\n",
    "\n",
    "1. Define your model and training script\n",
    "2. Create a Job Recipe (5-10 lines of code)\n",
    "3. Run it: `python job.py`\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "Each example can be run directly from its directory:\n",
    "\n",
    "```bash\n",
    "cd hello-pt\n",
    "pip install -r requirements.txt\n",
    "python job.py\n",
    "```\n",
    "\n",
    "The job will run in the simulator with multiple clients, and results will be saved to `/tmp/nvflare/simulation/[job-name]/`.\n",
    "\n",
    "## What is a Job Recipe?\n",
    "\n",
    "A Job Recipe is a high-level API introduced in FLARE v2.7 that encapsulates the entire FL workflow:\n",
    "\n",
    "```python\n",
    "from nvflare.app_opt.pt.recipes.fedavg import FedAvgRecipe\n",
    "from nvflare.recipe import SimEnv\n",
    "\n",
    "recipe = FedAvgRecipe(\n",
    "    name=\"hello-pt\",\n",
    "    min_clients=2,\n",
    "    num_rounds=2,\n",
    "    initial_model=SimpleNetwork(),\n",
    "    train_script=\"client.py\",\n",
    ")\n",
    "\n",
    "env = SimEnv(num_clients=2)\n",
    "run = recipe.execute(env)\n",
    "```\n",
    "\n",
    "That's it! The same recipe works in simulation, POC, and production environments.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "After trying these examples:\n",
    "- üìö Explore [step-by-step examples](./step-by-step/) for specific FL techniques\n",
    "- üîÑ Learn [ML-to-FL conversion](./ml-to-fl/) to adapt your existing code\n",
    "- üìñ Read the [Job Recipe documentation](https://nvflare.readthedocs.io/en/main/user_guide/data_scientist_guide/job_recipe.html) for advanced usage\n",
    "- üöÄ Check out [advanced examples](../advanced/) for production scenarios\n",
    "\n",
    "### More Hello World Examples\n",
    "\n",
    "Beyond the 5 examples above, explore additional hello-world examples:\n",
    "- üîÑ [Hello Cyclic Weight Transfer](./hello-cyclic/) - Cyclic learning workflow\n",
    "- ‚úÖ [Hello Cross-Validation](./hello-numpy-cross-val/) - Cross-site validation\n",
    "- üéØ [Hello Client Controlled Workflow](./hello-ccwf/) - Client-driven FL\n",
    "- üìä [Hello Tabular Statistics](./hello-tabular-stats/) - Federated statistics\n",
    "- üîê [Hello Learning Rate](./hello-lr/) - Adaptive learning rates\n",
    "- üñºÔ∏è [Hello ResNet](./hello-pt-resnet/) - ResNet with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b26543",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To run the examples:\n",
    "\n",
    "1. **Install NVIDIA FLARE:**\n",
    "   ```bash\n",
    "   pip install nvflare\n",
    "   ```\n",
    "   \n",
    "   **Optional - Install with framework support:**\n",
    "   ```bash\n",
    "   pip install nvflare[PT]  # For PyTorch examples\n",
    "   pip install nvflare[TF]  # For TensorFlow examples\n",
    "   ```\n",
    "\n",
    "2. **Get the example code:**\n",
    "   ```bash\n",
    "   git clone https://github.com/NVIDIA/NVFlare.git\n",
    "   git switch <release branch>\n",
    "   cd examples/hello-world\n",
    "   ```\n",
    "  If running in google colab, running the following to get the source code\n",
    "  \n",
    "  %pip install --ignore-installed blinker\n",
    "  ! npx degit -f NVIDIA/NVFlare/examples/hello-world . -y\n",
    "\n",
    "3. **Install example dependencies:**\n",
    "   \n",
    "   Each example has its own `requirements.txt`. Navigate to the example directory and install:\n",
    "   ```bash\n",
    "   cd hello-pt  # or hello-numpy, hello-lightning, etc.\n",
    "   pip install -r requirements.txt\n",
    "   ```\n",
    "\n",
    "4. **Run the example:**\n",
    "   ```bash\n",
    "   python job.py\n",
    "   ```\n",
    "\n",
    "That's it! The simulator will run locally with multiple clients, and results will be saved to `/tmp/nvflare/simulation/[job-name]/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d2ee0e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Examples\n",
    "\n",
    "The following sections demonstrate federated learning with different frameworks using NVIDIA FLARE Job Recipes. Each example can be run directly from the terminal with `python job.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faab91ee",
   "metadata": {},
   "source": [
    "## Example 1: Hello PyTorch\n",
    "\n",
    "üìÅ **[View full example code and README](./hello-pt/)**\n",
    "\n",
    "This example trains an image classifier on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) using PyTorch and federated averaging ([FedAvg](https://arxiv.org/abs/1602.05629)).\n",
    "\n",
    "### What You'll Learn\n",
    "- Using the `FedAvgRecipe` for PyTorch\n",
    "- Running with the Simulator\n",
    "- Client API integration\n",
    "- TensorBoard metric tracking\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This example uses [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), a dataset of 60,000 32x32 color images in 10 classes. The dataset is automatically downloaded via torchvision's datasets module.\n",
    "\n",
    "In a real FL experiment, each client would have their own dataset. Here for simplicity, we use the same dataset on each client.\n",
    "\n",
    "### Code Structure\n",
    "\n",
    "```\n",
    "hello-pt/\n",
    "‚îú‚îÄ‚îÄ client.py         # Client training code\n",
    "‚îú‚îÄ‚îÄ model.py          # Model definition\n",
    "‚îú‚îÄ‚îÄ job.py            # Job recipe\n",
    "‚îî‚îÄ‚îÄ requirements.txt  # Dependencies\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79095d19",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The example uses a simple CNN for CIFAR-10 classification. In PyTorch, neural networks are implemented by defining a class that extends `nn.Module`. The network's architecture is set up in the `__init__` method, while the `forward` method determines how input data flows through the layers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1464b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "print(\"‚úÖ Model defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b35c20",
   "metadata": {},
   "source": [
    "### Client Code\n",
    "\n",
    "The client code (`client.py`) uses NVIDIA FLARE's Client API to integrate with federated learning. The training code is almost identical to standard PyTorch training, with just a few lines added to communicate with the server.\n",
    "\n",
    "**Client API Pattern - Three Essential Methods:**\n",
    "- `flare.init()` - Initialize the FLARE Client API environment\n",
    "- `flare.receive()` - Receive the global model from the FL server\n",
    "- `flare.send()` - Send the updated model back to the FL server\n",
    "\n",
    "With these simple methods, you can convert centralized training code to federated learning with minimal changes.\n",
    "\n",
    "### Job Recipe\n",
    "\n",
    "The job recipe defines the complete FL workflow in just a few lines:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fcd299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows the recipe pattern - actual execution would be in hello-pt/job.py\n",
    "from nvflare.app_opt.pt.recipes.fedavg import FedAvgRecipe\n",
    "from nvflare.recipe import SimEnv, add_experiment_tracking\n",
    "\n",
    "# Create the recipe\n",
    "recipe = FedAvgRecipe(\n",
    "    name=\"hello-pt\",\n",
    "    min_clients=2,\n",
    "    num_rounds=2,\n",
    "    initial_model=SimpleNetwork(),\n",
    "    train_script=\"client.py\",\n",
    ")\n",
    "\n",
    "# Add TensorBoard tracking\n",
    "add_experiment_tracking(recipe, tracking_type=\"tensorboard\")\n",
    "\n",
    "print(\"‚úÖ Recipe created\")\n",
    "print(f\"   Job name: {recipe.name}\")\n",
    "print(f\"   Clients: 2\")\n",
    "print(f\"   Rounds: 2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b813f6d",
   "metadata": {},
   "source": [
    "### Running the Example\n",
    "\n",
    "To run this example from the terminal:\n",
    "\n",
    "```bash\n",
    "cd hello-pt\n",
    "pip install -r requirements.txt\n",
    "python job.py\n",
    "```\n",
    "\n",
    "The job will:\n",
    "1. Start a simulation environment with 2 clients\n",
    "2. Distribute the initial model to clients\n",
    "3. Run 2 rounds of federated training\n",
    "4. Save results to `/tmp/nvflare/simulation/hello-pt/`\n",
    "\n",
    "**View TensorBoard metrics:**\n",
    "```bash\n",
    "tensorboard --logdir /tmp/nvflare/simulation/hello-pt/server/simulate_job/tb_events\n",
    "```\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "After running, you should see:\n",
    "- **Round 0 and Round 1**: Training logs with loss reported for each epoch\n",
    "- **Model aggregation**: Server aggregates models from both clients\n",
    "- **Final model**: Saved to `/tmp/nvflare/simulation/hello-pt/`\n",
    "- **TensorBoard logs**: Available at `/tmp/nvflare/simulation/hello-pt/server/simulate_job/tb_events`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21da8c",
   "metadata": {},
   "source": [
    "## Example 2: Hello NumPy\n",
    "\n",
    "üìÅ **[View full example code and README](./hello-numpy/)**\n",
    "\n",
    "This example demonstrates federated learning with a simple NumPy model, perfect for understanding FL concepts without deep learning complexity.\n",
    "\n",
    "### What You'll Learn\n",
    "- Using the `NumpyFedAvgRecipe`\n",
    "- Understanding FedAvg aggregation\n",
    "- Simple FL workflow visualization\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This example uses a simplified synthetic dataset. Each client starts with the same 3x3 weight matrix:\n",
    "\n",
    "```\n",
    "[[1., 2., 3.],\n",
    " [4., 5., 6.],\n",
    " [7., 8., 9.]]\n",
    "```\n",
    "\n",
    "During training, each client adds 1.0 to each weight. After aggregation, you'll see the weights increase by approximately 1.0 per round, clearly demonstrating the federated averaging process.\n",
    "\n",
    "### How It Works\n",
    "\n",
    "The `SimpleNumpyModel` class implements a basic model using NumPy arrays. Each client performs basic operations on the weight matrix, and the server averages the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleNumpyModel:\n",
    "    def __init__(self):\n",
    "        self.weights = np.array([[1., 2., 3.],\n",
    "                                 [4., 5., 6.],\n",
    "                                 [7., 8., 9.]], dtype=np.float32)\n",
    "    \n",
    "    def train_step(self, learning_rate=1.0):\n",
    "        # Simulate training by adding learning_rate\n",
    "        self.weights += learning_rate\n",
    "        return self.weights\n",
    "\n",
    "model = SimpleNumpyModel()\n",
    "print(\"Initial weights:\")\n",
    "print(model.weights)\n",
    "print(\"\\nAfter training step:\")\n",
    "print(model.train_step(learning_rate=1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba87d3a",
   "metadata": {},
   "source": [
    "### Running the Example\n",
    "\n",
    "```bash\n",
    "cd hello-numpy\n",
    "pip install -r requirements.txt\n",
    "python job.py\n",
    "```\n",
    "\n",
    "After 3 rounds, weights increase by approximately 3 (depending on aggregation). Results saved to `/tmp/nvflare/simulation/hello-numpy/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73c0578",
   "metadata": {},
   "source": [
    "## Example 3: Hello PyTorch Lightning\n",
    "\n",
    "üìÅ **[View full example code and README](./hello-lightning/)**\n",
    "\n",
    "This example shows PyTorch Lightning integration with minimal code changes, training on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "### What You'll Learn\n",
    "- Using `LightningModule` with FLARE\n",
    "- The `flare.patch()` method\n",
    "- Automatic training/validation/testing\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "**LightningDataModule**: Encapsulates data loading, transforming, and splitting. It separates data-related logic from model code, promoting better code organization and reusability.\n",
    "\n",
    "**LightningModule**: A high-level abstraction built on PyTorch that streamlines model training. It encapsulates the model architecture, training logic, validation, testing, and optimizer configuration.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "Before running, pre-download the CIFAR-10 dataset to prevent race conditions when multiple clients try to download simultaneously:\n",
    "\n",
    "```bash\n",
    "./prepare_data.sh\n",
    "```\n",
    "\n",
    "### Key Integration\n",
    "\n",
    "PyTorch Lightning integration requires just one line - `flare.patch(trainer)`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978737e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lightning client integration pattern\n",
    "lightning_pattern = '''\n",
    "import nvflare.client.lightning as flare\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(max_epochs=2)\n",
    "\n",
    "# Patch trainer for FL - this is the key line!\n",
    "flare.patch(trainer)\n",
    "\n",
    "# Standard Lightning training loop\n",
    "while flare.is_running():\n",
    "    input_model = flare.receive()\n",
    "    trainer.validate(model, datamodule)\n",
    "    trainer.fit(model, datamodule)\n",
    "    trainer.test(model, datamodule)\n",
    "'''\n",
    "\n",
    "print(\"PyTorch Lightning Integration:\")\n",
    "print(lightning_pattern)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e115738",
   "metadata": {},
   "source": [
    "### Running the Example\n",
    "\n",
    "**Note:** Pre-download CIFAR-10 data first:\n",
    "\n",
    "```bash\n",
    "cd hello-lightning\n",
    "./prepare_data.sh\n",
    "pip install -r requirements.txt\n",
    "python job.py --num_rounds 2 --batch_size 16\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6aaab2a",
   "metadata": {},
   "source": [
    "## Example 4: Hello TensorFlow\n",
    "\n",
    "üìÅ **[View full example code and README](./hello-tf/)**\n",
    "\n",
    "This example demonstrates TensorFlow integration using the [MNIST](https://www.tensorflow.org/datasets/catalog/mnist) handwritten digits dataset.\n",
    "\n",
    "### What You'll Learn\n",
    "- Using the `TFFedAvgRecipe`\n",
    "- GPU memory management for multi-client scenarios\n",
    "- TensorFlow-specific considerations\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "The model uses TensorFlow's Keras API with a simple sequential architecture:\n",
    "- **Flatten Layer**: Prepares 28x28 images for dense layers\n",
    "- **Dense Layer**: 128 units with ReLU activation for non-linearity\n",
    "- **Dropout Layer**: 20% dropout rate to prevent overfitting\n",
    "- **Output Layer**: 10 units for digit classification (0-9)\n",
    "\n",
    "### GPU Memory Management\n",
    "\n",
    "TensorFlow attempts to allocate all available GPU memory at startup. For multi-client simulation, set these flags to allow dynamic memory growth:\n",
    "\n",
    "```bash\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=true TF_GPU_ALLOCATOR=cuda_malloc_async\n",
    "```\n",
    "\n",
    "**Optional:** If you have more GPUs than clients, run one client per GPU using:\n",
    "```bash\n",
    "nvflare simulator -n 2 --gpu 0,1 [job]\n",
    "```\n",
    "\n",
    "### Running the Example\n",
    "\n",
    "```bash\n",
    "cd hello-tf\n",
    "pip install -r requirements.txt\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=true TF_GPU_ALLOCATOR=cuda_malloc_async python job.py\n",
    "```\n",
    "\n",
    "Results saved to `/tmp/nvflare/simulation/hello-tf/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358b5eb",
   "metadata": {},
   "source": [
    "## Example 5: Hello Flower\n",
    "\n",
    "üìÅ **[View full example code and README](./hello-flower/)**\n",
    "\n",
    "This example shows how to run [Flower](https://flower.ai/) applications on NVIDIA FLARE infrastructure, combining Flower's simplicity with FLARE's enterprise features.\n",
    "\n",
    "### What You'll Learn\n",
    "- Integrating Flower with NVIDIA FLARE\n",
    "- Using Flower's `ClientApp` and `ServerApp`\n",
    "- Optional TensorBoard metric streaming\n",
    "\n",
    "### Dataset\n",
    "\n",
    "This example uses [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) with a simple CNN based on PyTorch's \"60 Minute Blitz\" tutorial.\n",
    "\n",
    "### Code Structure\n",
    "\n",
    "```\n",
    "hello-flower/\n",
    "‚îú‚îÄ‚îÄ flwr-pt/              # Basic Flower PyTorch app\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ flwr_pt/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ client.py     # Contains ClientApp\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ server.py     # Contains ServerApp\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ task.py       # Model and data definitions\n",
    "‚îú‚îÄ‚îÄ flwr-pt-tb/           # With TensorBoard streaming\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ flwr_pt_tb/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ client.py     # ClientApp with metrics\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ server.py     # ServerApp\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ task.py       # Model and data definitions\n",
    "‚îî‚îÄ‚îÄ job.py                # FLARE Job Recipe\n",
    "```\n",
    "\n",
    "The client code in `client.py` contains the **Flower Client App**, while `server.py` uses Flower's built-in FedAvg **Strategy**.\n",
    "\n",
    "### Running the Example\n",
    "\n",
    "**Basic Flower app:**\n",
    "```bash\n",
    "cd hello-flower\n",
    "pip install -r requirements.txt\n",
    "python job.py --job_name \"flwr-pt\" --content_dir \"./flwr-pt\"\n",
    "```\n",
    "\n",
    "**With TensorBoard streaming:**\n",
    "```bash\n",
    "python job.py --job_name \"flwr-pt-tb\" --content_dir \"./flwr-pt-tb\" --stream_metrics\n",
    "tensorboard --logdir /tmp/nvflare/hello-flower-tb\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8403bf6d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "You've learned how to use NVIDIA FLARE Job Recipes to build federated learning applications across different frameworks:\n",
    "\n",
    "‚úÖ **PyTorch** - Image classification with FedAvg  \n",
    "‚úÖ **NumPy** - Understanding FL basics  \n",
    "‚úÖ **PyTorch Lightning** - Minimal integration with `flare.patch()`  \n",
    "‚úÖ **TensorFlow** - MNIST with GPU memory management  \n",
    "‚úÖ **Flower** - Running Flower apps on FLARE infrastructure  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Job Recipes are simple** - Define FL jobs in 5-10 lines of code\n",
    "2. **Same code, multiple environments** - Recipes work in simulation, POC, and production\n",
    "3. **Framework flexibility** - Use PyTorch, TensorFlow, NumPy, Lightning, or Flower\n",
    "4. **Easy to run** - Just `python job.py` in any example directory\n",
    "\n",
    "### Learn More\n",
    "\n",
    "- **Documentation**: [https://nvflare.readthedocs.io/](https://nvflare.readthedocs.io/)\n",
    "- **GitHub**: [https://github.com/NVIDIA/NVFlare](https://github.com/NVIDIA/NVFlare)\n",
    "- **Example Catalog**: [https://nvidia.github.io/NVFlare/catalog/](https://nvidia.github.io/NVFlare/catalog/)\n",
    "\n",
    "### Get Help\n",
    "\n",
    "- **GitHub Issues**: [https://github.com/NVIDIA/NVFlare/issues](https://github.com/NVIDIA/NVFlare/issues)\n",
    "- **Discussions**: [https://github.com/NVIDIA/NVFlare/discussions](https://github.com/NVIDIA/NVFlare/discussions)\n",
    "\n",
    "**Happy Federated Learning! üöÄ**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
