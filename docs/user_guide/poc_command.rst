.. _poc_command:

*****************************************
Commmand for Proof Of Concept (POC) Mode
*****************************************

Introduction to the POC Command
===============================

The POC command allows users to try out the features of FLARE in a proof of concept deployment on a single machine.

In release 2.2, the poc command has been updated to make it easier to use. 

previously, 

   poc -n  5

we create a workspace called "poc" in "current" working directory with 5 clients. 

1) The user will need to go to each package startup directories( server,  admin, site-1,..,5) and run start.sh. 

2) to run some example, need to copy from NVFlare examples directory ( assume its already cloned from github) to the admin/transfer directory

3) submit_job for the desired example

4) shutdown each packages ( server, site-1, ... 5) 

5) finally shutdown NVFLARE console (aka Admin Client) 


Although the process works, its a bit tedious. 

We made some changes to address above, here are the major changes:

    - make poc command a sub-command for nvflare
        - we will add other sub-command for provision, simulator etc. later
    - allow poc command to start all packages (include Flare console) 
    - allow poc command to shutdown all server and client packages ( not include Flare console)
    - allow poc command to specify workspace directory ( not current working directory)
    - add clean poc command to delete workspace


Syntax and Usage
=================

.. code-block::

  nvflare poc -h
  
  usage: nvflare poc [-h] [-n [NUMBER_OF_CLIENTS]] [-p [PACKAGE]] [-gpu [GPU [GPU ...]]]
                     [--prepare] [--start] [--stop] [--clean]

  optional arguments:
    -h, --help            show this help message and exit
    -n [NUMBER_OF_CLIENTS], --number_of_clients [NUMBER_OF_CLIENTS]
                          number of sites or clients, default to 2
    -p [PACKAGE], --package [PACKAGE]
                          package directory, default to all = all packages, only used for
                          start/stop-poc commands when specified
    -gpu [GPU [GPU ...]], --gpu [GPU [GPU ...]]
                          gpu device ids will be used as CUDA_VISIBLE_DEVICES. used for poc
                          start command
    --prepare             prepare poc workspace
    --start               start poc
    --stop                stop poc
    --clean               cleanup poc workspace

Before You Start
================
Unlike the previous poc command,  we add one more requirement for POC before you can use the new command:

    - After you clone the github repo of NVFLARE, you need to define a NVFLARE_HOME environment variable point to the local NVFlare directory

 for example::

   The github repo NVFlare is cloned under ~/projects directory

   Then

        NVFLARE_HOME=~/projects/NVFlare


If one doesn't set the NVFLARE_HOME env. variable, the nvflare poc command will fail.


Setup poc workspace
===================

.. code-block::

  nvflare poc --prepare
  prepare_poc at /tmp/nvflare/poc for 2 clients
  This will delete poc folder in current directory and create a new one. Is it OK to proceed? (y/N) y
  Successfully creating poc folder at /tmp/nvflare/poc.  Please read poc/Readme.rst for user guide.
   
  
  WARNING:
  ******* Files generated by this poc command are NOT intended for production environments.
  link examples from /home/chester/projects/NVFlare/examples to /tmp/nvflare/poc/admin/transfer


This is similar to old command 

.. code-block::

  poc -n 2

except that workspace is default to /tmp/nvflare/poc, and default n = 2 clients is not specified. 

notice that: 

we link examples from /home/chester/projects/NVFlare/examples to /tmp/nvflare/poc/admin/transfer

We automatically setup the transfer directory as symlink to examples, we can do this as we leverage the NVFLRAE_HOME environment variable. 


Replace the default POC workspace
---------------------------------
One can change the default  poc workspace to any location.  All you need to do is to set env. variable NVFLARE_POC_WORKSPACE

NVFLARE_POC_WORKSPACE="/tmp/nvflare/poc2"

In this example,  we  change the default workspace to different location /tmp/nvfalre/poc2

now:

nvflare poc  --prepare

will generate poc in workspace /tmp/nvfalre/poc2


Start Package(s)
================
Once we get the poc package generated with prepare command, we are ready to start. if use prepared the poc using default workspace, then you need to start with the same default workspace, otherwise, you need to specify the workspace.

Start ALL packages
------------------

.. code-block::

  nvflare poc --start

will start ALL clients (site-1, site-2) and server as well as FLARE Console (aka Admin Client) located in the default workspace=/tmp/nvflare/poc


.. code-block::

    nvflare poc --prepare
    prepare_poc at /tmp/nvflare/poc for 2 clients
    This will delete poc folder in current directory and create a new one. Is it OK to proceed? (y/N) y
    Successfully creating poc folder at /tmp/nvflare/poc.  Please read poc/Readme.rst for user guide.
    
    
    WARNING:
    ******* Files generated by this poc command are NOT intended for production environments.
    link examples from /home/chester/projects/NVFlare/examples to /tmp/nvflare/poc/admin/transfer
    (nvflare-env) chester@RTX:~/projects/NVFlare$ nvflare poc -w /tmp/nvflare/poc2 --prepare
    prepare_poc at /tmp/nvflare/poc2 for 2 clients
    This will delete poc folder in current directory and create a new one. Is it OK to proceed? (y/N) y
    Successfully creating poc folder at /tmp/nvflare/poc2.  Please read poc/Readme.rst for user guide.
    
    
    WARNING:
    ******* Files generated by this poc command are NOT intended for production environments.
    link examples from /home/chester/projects/NVFlare/examples to /tmp/nvflare/poc2/admin/transfer
    (nvflare-env) chester@RTX:~/projects/NVFlare$ nvflare poc -n 10 -w /tmp/nvflare/poc2 --prepare
    prepare_poc at /tmp/nvflare/poc2 for 10 clients
    This will delete poc folder in current directory and create a new one. Is it OK to proceed? (y/N) y
    Successfully creating poc folder at /tmp/nvflare/poc2.  Please read poc/Readme.rst for user guide.
    
    
    WARNING:
    ******* Files generated by this poc command are NOT intended for production environments.
    link examples from /home/chester/projects/NVFlare/examples to /tmp/nvflare/poc2/admin/transfer
    (nvflare-env) chester@RTX:~/projects/NVFlare$ nvflare poc --start
    start_poc at /tmp/nvflare/poc, white_list=[]
    start: package: server, executing /tmp/nvflare/poc/server/startup/start.sh
    WORKSPACE set to /tmp/nvflare/poc/server/startup/..
    WORKSPACE set to /tmp/nvflare/poc/server/startup/..
    PYTHONPATH is /local/custom:/home/chester/projects/NVFlare:
    start: package: site-1, executing /tmp/nvflare/poc/site-1/startup/start.sh
    WORKSPACE set to /tmp/nvflare/poc/site-1/startup/..
    No sp_end_point is provided.  Default sp_end_point (localhost:8002:8003) is used
    Usage: start.sh <SP_END_POINT <CLIENT_NAME>>
    WORKSPACE set to /tmp/nvflare/poc/site-1/startup/..
    PYTHONPATH is /local/custom:/home/chester/projects/NVFlare:
    2022-08-03 15:05:33,603 - FederatedServer - INFO - starting insecure server at localhost:8002
    2022-08-03 15:05:33,605 - FederatedServer - INFO - Got the primary sp: localhost fl_port: 8002 SSID: ebc6125d-0a56-4688-9b08-355fe9e4d61a. Turning to hot.
    deployed FL server trainer.
    2022-08-03 15:05:33,655 - FedAdminServer - INFO - Starting Admin Server localhost on Port 8003
    2022-08-03 15:05:33,655 - root - INFO - Server started
    start: package: site-2, executing /tmp/nvflare/poc/site-2/startup/start.sh
    WORKSPACE set to /tmp/nvflare/poc/site-2/startup/..
    No sp_end_point is provided.  Default sp_end_point (localhost:8002:8003) is used
    Usage: start.sh <SP_END_POINT <CLIENT_NAME>>
    WORKSPACE set to /tmp/nvflare/poc/site-2/startup/..
    PYTHONPATH is /local/custom:/home/chester/projects/NVFlare:
    start: package: admin, executing /tmp/nvflare/poc/admin/startup/fl_admin.sh
    /tmp/nvflare/poc/admin/startup
    Waiting for SP....
    2022-08-03 15:05:37,562 - FederatedClient - INFO - Got the new primary SP: localhost:8002
    Waiting for token from successful login...
    Got primary SP localhost:8002:8003 from overseer. Host: localhost Admin_port: 8003 SSID: ebc6125d-0a56-4688-9b08-355fe9e4d61a
    login_result: OK token: 67d08590-1378-11ed-9237-d5e224b4ae75
    Type ? to list commands; type "? cmdName" to show usage of a command.
    > 2022-08-03 15:05:39,640 - ClientManager - INFO - Client: New client site-1@127.0.0.1 joined. Sent token: cd0fa569-8ef0-4d1a-b25f-e3d00dd7e151.  Total clients: 1
    2022-08-03 15:05:39,641 - FederatedClient - INFO - Successfully registered client:site-1 for project example_project. Token:cd0fa569-8ef0-4d1a-b25f-e3d00dd7e151 SSID:ebc6125d-0a56-4688-9b08-355fe9e4d61a
    created /tmp/nvflare/poc/site-1/startup/../startup/comm/training/x
    created /tmp/nvflare/poc/site-1/startup/../startup/comm/training/y
    created /tmp/nvflare/poc/site-1/startup/../startup/comm/training/t
    Waiting for SP....
    2022-08-03 15:05:40,523 - FederatedClient - INFO - Got the new primary SP: localhost:8002
    2022-08-03 15:05:42,294 - ClientManager - INFO - Client: New client site-2@127.0.0.1 joined. Sent token: 4fe91fa2-d4cb-4403-b958-fa52ab60248d.  Total clients: 2
    2022-08-03 15:05:42,295 - FederatedClient - INFO - Successfully registered client:site-2 for project example_project. Token:4fe91fa2-d4cb-4403-b958-fa52ab60248d SSID:ebc6125d-0a56-4688-9b08-355fe9e4d61a
    created /tmp/nvflare/poc/site-2/startup/../startup/comm/training/x
    created /tmp/nvflare/poc/site-2/startup/../startup/comm/training/y
    created /tmp/nvflare/poc/site-2/startup/../startup/comm/training/t
    
    > check_status server
    Engine status: stopped
    ---------------------
    | JOB_ID | APP NAME |
    ---------------------
    ---------------------
    Registered clients: 2
    ----------------------------------------------------------------------------
    | CLIENT | TOKEN                                | LAST CONNECT TIME        |
    ----------------------------------------------------------------------------
    | site-1 | cd0fa569-8ef0-4d1a-b25f-e3d00dd7e151 | Wed Aug  3 15:07:13 2022 |
    | site-2 | 4fe91fa2-d4cb-4403-b958-fa52ab60248d | Wed Aug  3 15:06:45 2022 |
    ----------------------------------------------------------------------------
    Done [890 usecs] 2022-08-03 15:07:15.876945
    > 

.. note::

    If you run ``nvflare poc --start`` before prepare, you will get the following error:

        .. code-block::

    nvflare poc --start
    start_poc at /tmp/nvflare/poc, white_list=[]
    workspace /tmp/nvflare/poc is not ready, please use poc --prepare to prepare poc workspace

.. note::

    If you prefer to have the FLARE Console on a different terminal, you can use ``nvflare poc --start -ex admin``.

Start only the selected package
--------------------------------

You can just start just selected package,

Example, FLARE Console, where the session could timeout if there is no activity. We need just start the FLARE Console, not all other packages

assuming we are always using default workspace, the command will be 


Start Server
------------

.. code-block::

    nvflare poc --start -p server
    start_poc at /tmp/nvflare/poc, white_list=['server']
    start: package: server, executing /tmp/nvflare/poc/server/startup/start.sh
    WORKSPACE set to /tmp/nvflare/poc/server/startup/..
    WORKSPACE set to /tmp/nvflare/poc/server/startup/..
    PYTHONPATH is /local/custom:/home/chester/projects/NVFlare:
    2022-08-03 15:33:43,236 - FederatedServer - INFO - starting insecure server at localhost:8002
    2022-08-03 15:33:43,238 - FederatedServer - INFO - Got the primary sp: localhost fl_port: 8002 SSID: ebc6125d-0a56-4688-9b08-355fe9e4d61a. Turning to hot.
    deployed FL server trainer.
    2022-08-03 15:33:43,286 - FedAdminServer - INFO - Starting Admin Server localhost on Port 8003
    2022-08-03 15:33:43,286 - root - INFO - Server started

Start FLARE Console (previously called the Admin Client)
---------------------------------------------------------

.. code-block::

    nvflare poc --start -p admin
    start_poc at /tmp/nvflare/poc, white_list=['admin']
    start: package: admin, executing /tmp/nvflare/poc/admin/startup/fl_admin.sh
    /tmp/nvflare/poc/admin/startup
    Waiting for token from successful login...
    Got primary SP localhost:8002:8003 from overseer. Host: localhost Admin_port: 8003 SSID: ebc6125d-0a56-4688-9b08-355fe9e4d61a
    login_result: OK token: b9bc85c6-137c-11ed-9237-d5e224b4ae75
    Type ? to list commands; type "? cmdName" to show usage of a command.
    > check_status server
    Engine status: stopped
    ---------------------
    | JOB_ID | APP NAME |
    ---------------------
    ---------------------
    Registered clients: 0
    Done [707 usecs] 2022-08-03 15:36:45.599285



Start Clients with GPU assignment
----------------------------------

If no GPU id is specified, the host GPU id will be used if available.

.. code-block::

    nvidia-smi --list-gpus
    GPU 0: NVIDIA GeForce RTX 3090 (UUID: GPU-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxx)

If there is no GPU, then there will be no assignments. If there are GPUs, they will be assigned in the rule explained later.


If the user provides the GPU device Ids and in certain order, such as:

.. code-block::

    nvflare poc -gpu 1 0 0 2 --start

The system will try to match the clients with given GPU devices order, in such case, site-1 with GPU_id = 0, site-2 with GPU_id = 0, site-3 with GPU_id = 0 and Site-4 with GPU_id = 2

If the GPU id doesn't exist in the client machine, you will get something like:

.. code-block::

    gpu_id provided is not available in the host machine, available GPUs are [0]

Client GPU assignments
-----------------------

::

    if clients > number of GPUs
      gpu_index = client % n_gpus
      gpu_assignments[client] = [gpu_ids[gpu_index]]

GPU is shared among several clients

::

    if n_gpus > n_clients > 0:
        for gpu_index, gpu_id in enumerate(gpu_ids):
            client = gpu_index % n_clients
            gpu_assignments[client].append(gpu_ids[gpu_index])

Each Client has several GPUs.

Stop Package(s)
===============

 we can stop almost all packages , assuming we are using default poc workspace

nvflare poc --stop 

will stop ALL clients and server packages. 

This will not stop FLARE Console. you will need to exit yourself. 

.. code-block::

    nvflare poc  --stop
    stop_poc at /tmp/nvflare/poc
    stop: package: server, executing touch /tmp/nvflare/poc/server/shutdown.fl
    stop: package: site-1, executing touch /tmp/nvflare/poc/site-1/shutdown.fl
    stop: package: site-2, executing touch /tmp/nvflare/poc/site-2/shutdown.fl
    stop: package: admin, executing touch /tmp/nvflare/poc/admin/shutdown.fl

Similarly, we can selected stop a package, such as:

.. code-block::

    nvflare poc --stop -p server
    stop_poc at /tmp/nvflare/poc
    stop: package: server, executing touch /tmp/nvflare/poc/server/shutdown.fl

Clean up
========

There is a command to clean up the POC workspace added in version 2.2 that will delete the POC workspaces:

.. code-block::

    nvflare poc --clean
