.. _quickstart:

######################
Hello world Series
######################

Welcome to the NVIDIA FLARE Quick Start Series! This guide provides a set of hello-world examples to help you quickly learn how to build federated learning programs using NVIDIA FLARE.

Make sure you have completed the :ref:`installation` steps before proceeding.

Prerequisites
=============
- Python 3.9+
- pip
- Git
- NVFlare installed (see :ref:`installation`)

Quick Start Examples
====================

The following hello-world examples demonstrate different federated learning algorithms and workflows. Each example includes instructions and code to help you get started.

1. **hello pytorch** `<hello-world/hello-pt/doc.html>`_
   - Federated averaging with PyTorch models and training loops.
2. **hello lightning** `<hello-world/hello-lightning/doc.html>`_
   - Example using PyTorch Lightning for streamlined model training.
3. **hello tensorflow** `<hello-world/hello-tf/doc.html>`_
   - Federated averaging using TensorFlow models.
4. **hello LR** `<hello-world/hello-lr/doc.html>`_
   - Federated logistic regression example using scikit-learn.
5. **hello KMeans** `<hello-world/hello-KMeans/doc.html>`_
   - Federated KMeans clustering example.
6. **hello KM** `<hello-world/hello-km/doc.html>`_
   - Federated Kaplan-Meier survival analysis.
7. **hello stat** `<hello-world/hello-stats/doc.html>`_
   - Federated statistics computation example.
8. **hello cyclic** `<hello-world/hello-cyclic/doc.html>`_
   - Cyclic federated learning workflow example.
9. **hello flower** `<hello-world/hello-flower/doc.html>`_
   - running flower apps in FLARE
10. **hello xgboost** `<hello-world/hello-xgboost/doc.html>`_
    - Federated XGBoost example demonstrating gradient boosting for tabular data in a federated setting.

Let's start with hello fedavg with pytorch: `<hello-world/hello-pt/doc.html>`_
