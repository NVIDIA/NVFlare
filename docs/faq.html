<!--
# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->


<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>FAQ &mdash; NVIDIA FLARE 2.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/additions.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="nvflare" href="apidocs/modules.html" />
    <link rel="prev" title="NVIDIA FLARE Programming Best Practices" href="programming_guide/best_practices.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
           

          
            <a href="index.html" class="icon icon-home"> NVIDIA FLARE
          

          
          </a>

          
            
            
              <div class="version">
                2.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 80%;
    }

    .floatleftcol {
      float: left;
      max-width: 60%;
      padding-right: 20px;
    }
  </style>
  
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html">User Guide - Provision, Start, Operate</a></li>
<li class="toctree-l1"><a class="reference internal" href="programming_guide.html">Programming Guide - Developing Apps with NVIDIA FLARE</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">FAQ</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#general">General</a></li>
<li class="toctree-l2"><a class="reference internal" href="#operational">Operational</a></li>
<li class="toctree-l2"><a class="reference internal" href="#security">Security</a></li>
<li class="toctree-l2"><a class="reference internal" href="#client-related-questions">Client related questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#server-related-questions">Server related questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#overall-training-flow-related-questions">Overall training flow related questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#known-issues">Known issues</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="apidocs/modules.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA FLARE</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>FAQ</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/faq.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="faq">
<span id="id1"></span><h1>FAQ<a class="headerlink" href="#faq" title="Permalink to this headline">¶</a></h1>
<div class="section" id="general">
<h2>General<a class="headerlink" href="#general" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>What is NVIDIA FLARE?</p>
<blockquote>
<div><p>NVIDIA FLARE 2.0 is a general-purpose framework designed for collaborative computing.  In this collaborative
computing framework, workflows are not limited to aggregation-based federated learning (usually called a Fed-Average workflow),
and applications are not limited to deep learning.  NVIDIA FLARE is fundamentally a messaging system running in a multithreaded
environment.</p>
</div></blockquote>
</li>
<li><p>What does NVIDIA FLARE stand for?</p>
<blockquote>
<div><p>NVIDIA Federated Learning Application Runtime Environment.</p>
</div></blockquote>
</li>
<li><p>Does NVIDIA FLARE depend on Tensorflow or PyTorch?</p>
<blockquote>
<div><p>No.  NVIDIA FLARE is a Python library that implements a general collaborative computing framework.  The <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controllers</span></a>,
<span class="xref std std-ref">Executors</span>, and <a class="reference internal" href="programming_guide/controllers.html#tasks"><span class="std std-ref">Tasks</span></a> that one defines to execute the collaborative computing workflow
are entirely independent.</p>
</div></blockquote>
</li>
<li><p>Is NVIDIA FLARE designed for deep learning model training only?</p>
<blockquote>
<div><p>No.  NVIDIA FLARE implements a communication framework that can support any collaborative computing workflow.  This
could be deep learning, machine learning, or even simple statistical workflows.</p>
</div></blockquote>
</li>
<li><p>Does NVIDIA FLARE require a GPU?</p>
<blockquote>
<div><p>No.  Hardware requirements are dependent only on what is implemented in the <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controller</span></a> workflow and client <a class="reference internal" href="programming_guide/controllers.html#tasks"><span class="std std-ref">Tasks</span></a>.
Client training tasks will typically benefit from GPU acceleration.  Server <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controller</span></a> workflows may or may not require a GPU.</p>
</div></blockquote>
</li>
<li><p>How does NVIDIA FLARE implement its collaborative computing framework?</p>
<blockquote>
<div><p>NVIDIA FLARE collaborative computing is achieved through <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controller/Worker</span></a> interaction.</p>
</div></blockquote>
</li>
<li><p>What is a Controller?</p>
<blockquote>
<div><p>The <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controller</span></a> is a python object that controls or coordinates Workers to perform tasks. The
Controller is run on the server.  The Controller defines the overall collaborative computing workflow.  In its
control logic, the Controller assigns tasks to Workers and processes task results from the workers.</p>
</div></blockquote>
</li>
<li><p>What is a Worker?</p>
<blockquote>
<div><p>A Worker is capable of performing tasks (skills). Workers run on Clients.</p>
</div></blockquote>
</li>
<li><p>What is a Task?</p>
<blockquote>
<div><p>A <a class="reference internal" href="programming_guide/controllers.html#tasks"><span class="std std-ref">Task</span></a> is a piece of work (Python code) that is assigned by the <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controller</span></a> to
client workers. Depending on how the Task is assigned (broadcast, send, or relay), the task will be performed by one
or more clients.  The logic to be performed in a Task is defined in an <span class="xref std std-ref">Executor</span>.</p>
</div></blockquote>
</li>
<li><p>What is Learnable?</p>
<blockquote>
<div><p>Learnable is the result of the Federated Learning application maintained by the server.  In DL workflows, the
Learnable is the aspect of the DL model to be learned.  For example, the model weights are commonly the Learnable
feature, not the model geometry.  Depending on the purpose of your study, the Learnable may be any component of interest.
Learnable is an abstract object that is aggregated from the client’s Shareable object and is not DL-specific.  It
can be any model, or object.  The Learnable is managed in the Controller workflow.</p>
</div></blockquote>
</li>
<li><p>What is Shareable?</p>
<blockquote>
<div><p><a class="reference internal" href="programming_guide/shareable.html#shareable"><span class="std std-ref">Shareable</span></a> is simply a communication between two peers (server and clients). In the task-based
interaction, the Shareable from server to clients carries the data of the task for the client to execute; and the
Shareable from the client to server carries the result of the task execution.  When this is applied to DL model
training, the task data typically contains model weights for the client to train on; and the task result contains
updated model weights from the client.  The concept of Shareable is very general - it can be whatever that makes
sense for the task.</p>
</div></blockquote>
</li>
<li><p>What is FLContext and what kind of information does it contain?</p>
<blockquote>
<div><p><a class="reference internal" href="programming_guide/fl_context.html#fl-context"><span class="std std-ref">FLContext</span></a> is one of the key features of NVIDIA FLARE and is available to every method of all <a class="reference internal" href="programming_guide/fl_component.html#fl-component"><span class="std std-ref">FLComponent</span></a>
types (Controller, Aggregator, Executor, Filter, Widget, …). An FLContext object contains contextual information
of the FL environment: overall system settings (peer name, current run number, workspace location, etc.). FLContext
also contains an important object called Engine, through which you can access important services provided by the
system (e.g. fire events, get all available client names, send aux messages, etc.).</p>
</div></blockquote>
</li>
<li><p>What are events and how are they handled?</p>
<blockquote>
<div><p><a class="reference internal" href="programming_guide/event_system.html#event-system"><span class="std std-ref">Events</span></a> allow for dynamic notifications to be sent to all objects that are a subclass of
<a class="reference internal" href="programming_guide/fl_component.html#fl-component"><span class="std std-ref">FLComponent</span></a>. Every FLComponent is an event handler.</p>
<p>The event mechanism is like a pub-sub mechanism that enables indirect communication between components for data
sharing. Typically, the data generator fires an event to publish the data, and other components handle the events
they are subscribed to and consume the data of the event. The fed event mechanism even allows the pub-sub go across
network boundaries.</p>
</div></blockquote>
</li>
<li><p>What additional components may be implemented with NVIDIA FLARE to support the Controller Workflow, and where do they run (server or client):</p>
<blockquote>
<div><dl class="simple">
<dt>LearnablePersistor - Server</dt><dd><p>The LearnablePersistor is a method implemented for the server to save the state of the Learnable object, for
example writing a global model to disk for persistence.</p>
</dd>
<dt>ShareableGenerator - Server</dt><dd><p>The ShareableGenerator is an object that implements two methods: learnable_to_shareable converts a Learnable
object to a form of data to be shared to the client; shareable_to_learnable uses the shareable data (or
aggregated shareable data) from the clients to update the learnable object.</p>
</dd>
<dt>Aggregator - Server</dt><dd><p>The aggregator defines the algorithm used on the server to aggregate the data passed back to the server in the
clients’ Shareable object.</p>
</dd>
<dt>Executor - Client</dt><dd><p>The Executor defines the algorithm the clients use to operate on data contained in the Shareable object.  For
example in DL training, the executor would implement the training loop. There can be multiple executors on the
client, designed to execute different tasks (training, validation/evaluation, data preparation, etc.).</p>
</dd>
<dt>Filter - Clients and Server</dt><dd><p><a class="reference internal" href="programming_guide/filters.html#filters"><span class="std std-ref">Filters</span></a> are used to define transformations of the data in the Shareable object when transferred between server
and client and vice versa.  Filters can be applied when the data is sent or received by either the client or server.
See the diagram on the <a class="reference internal" href="programming_guide/filters.html#filters"><span class="std std-ref">Filters</span></a> page for details on when “task_data_filters” and “task_result_filters”
are applied on the client and server.</p>
</dd>
<dt>Any component of subclass of FLComponent</dt><dd><p>All component types discussed above are subclasses of <a class="reference internal" href="programming_guide/fl_component.html#fl-component"><span class="std std-ref">FLComponent</span></a>. You can create your own subclass of
FLComponent for various purposes. For example, you can create such a component to listen to certain events and
handle the data of the events (analysis, dump to disk or DB, etc.).</p>
</dd>
</dl>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="operational">
<h2>Operational<a class="headerlink" href="#operational" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>What is <a class="reference internal" href="user_guide/provisioning_tool.html#provisioning"><span class="std std-ref">Provisioning</span></a>?</p>
<blockquote>
<div><p>NVIDIA FLARE includes an Open Provision API that allows you to generate mutual-trusted system-wide configurations,
or startup kits, that allow all participants to join the NVIDIA FLARE system from across different locations.  This
mutual-trust is a mandatory feature of Open Provision API as every participant authenticates others by the
information inside the configuration.  The configurations usually include, but are not limited to:</p>
<blockquote>
<div><ul class="simple">
<li><p>network discovery, such as domain names, port numbers or IP addresses</p></li>
<li><p>credentials for authentication, such as certificates of participants and root authority</p></li>
<li><p>authorization policy, such as roles, rights and rules</p></li>
<li><p>tamper-proof mechanism, such as signatures</p></li>
<li><p>convenient commands, such as shell scripts with default command line options to easily start an individual participant</p></li>
</ul>
</div></blockquote>
</div></blockquote>
</li>
<li><p>What types of startup kits are generated by the Provision tool?</p>
<blockquote>
<div><p>The Open Provision API allows flexibility in generating startup kits, but typically the provisioning tool is used to
generate secure startup kits for the server, FL clients, and Admin clients.</p>
</div></blockquote>
</li>
<li><p>What files does each type of startup kit contain? What are these files used for, and by whom?</p>
<blockquote>
<div><p>Startup kits contain the configuration and certificates necessary to establish secure connections between the server,
FL clients, and Admin clients.  These files are used to establish identity and authorization policies between server
and clients.  Startup kits are distributed to the FL server, clients, and Admin clients depending on role.  For the
purpose of development, startup kits may be generated with limited security to allow simplified connection between
systems or between processes on a single host.  See the “poc” functionality of the Open Provision API for details.</p>
</div></blockquote>
</li>
<li><p>How would you distribute the startup kits to the right people?</p>
<blockquote>
<div><p>Distribution of startup kits is inherently flexible and can be via email or shared storage.  The API allows the
addition of builder components to automation distribution.</p>
</div></blockquote>
</li>
<li><p>What happens after provisioning?</p>
<blockquote>
<div><p>After provisioning, the Admin API is used to deploy an Application to the FL server and clients.</p>
</div></blockquote>
</li>
<li><p>What is an Application in NVIDIA FLARE?</p>
<blockquote>
<div><p>An <a class="reference internal" href="user_guide/application.html#application"><span class="std std-ref">Application</span></a> is a named directory structure that defines the client and server configuration
and any custom code required to implement the Controller/Worker workflow.</p>
</div></blockquote>
</li>
<li><p>What is the basic directory structure of an NVIDIA FLARE Application?</p>
<blockquote>
<div><p>Typically the Application configuration is defined in a <code class="docutils literal notranslate"><span class="pre">config/</span></code>
subdirectory and defines paths to Controller and Worker executors.  Custom code can be defined in a <code class="docutils literal notranslate"><span class="pre">custom/</span></code>
subdirectory and is subject to rules defined in the Authorization Policy.</p>
</div></blockquote>
</li>
<li><p>How do you deploy an application?</p>
<blockquote>
<div><p>An Application is deployed using the <code class="docutils literal notranslate"><span class="pre">upload_app</span></code> and <code class="docutils literal notranslate"><span class="pre">deploy_app</span></code> methods of the <a class="reference internal" href="user_guide/admin_commands.html#admin-commands"><span class="std std-ref">Admin API</span></a>.</p>
</div></blockquote>
</li>
<li><p>Do all FL client have to use the same application configuration?</p>
<blockquote>
<div><p>No, they do not have to use the same application configuration, even though they can that is frequently done. The
function of FL clients can be customized by the implementation of Tasks and Executors and the client’s
response to Events.</p>
</div></blockquote>
</li>
<li><p>What is the difference between the Admin client and the FL client?</p>
<blockquote>
<div><p>The <a class="reference internal" href="user_guide/admin_commands.html#admin-commands"><span class="std std-ref">Admin client</span></a> is used to control the state of the server’s controller workflow and only interacts with the
server.  FL clients poll the server and perform tasks based on the state of the server.  The Admin client does not
interact directly with FL client.</p>
</div></blockquote>
</li>
<li><p>Where does the Admin client run?</p>
<blockquote>
<div><p>The <a class="reference internal" href="user_guide/admin_commands.html#admin-commands"><span class="std std-ref">Admin client</span></a> runs as a standalone process, typically on a researcher’s workstation or laptop.</p>
</div></blockquote>
</li>
<li><p>What can you do with the Admin client?</p>
<blockquote>
<div><p>The <a class="reference internal" href="user_guide/admin_commands.html#admin-commands"><span class="std std-ref">Admin client</span></a> is used to orchestrate the FL study, including starting and stopping server
and clients, deploying applications, and managing FL experiments.</p>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="security">
<h2>Security<a class="headerlink" href="#security" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>What is the scope of security in NVIDIA FLARE?</p>
<blockquote>
<div><p>Security is multi-faceted and cannot be completely controlled for or provided by the NVIDIA FLARE API.  The Open
Provision API provides examples of basic communication and identity security using GRPC via shared self-signed
certificates and authorization policies.  These security measures may be sufficient but can be extended with the
provided APIs.</p>
</div></blockquote>
</li>
<li><p>What about data privacy?</p>
<blockquote>
<div><p>NVIDIA FLARE comes with a few techniques to help with data privacy during FL: differential privacy and homomorphic encryption
(see <a class="reference internal" href="programming_guide/filters.html#filters-for-privacy"><span class="std std-ref">Privacy filters</span></a>).</p>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="client-related-questions">
<h2>Client related questions<a class="headerlink" href="#client-related-questions" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>What happens if an FL client joins during the FL training?</p>
<blockquote>
<div><p>An FL client can join the FL training any time. It is up to the workflow logic to manage FL clients.</p>
</div></blockquote>
</li>
<li><p>Do federated learning clients need to open any ports for the FL server to reach the FL client?</p>
<blockquote>
<div><p>No, federated learning training does not require for FL clients to open their network for inbound traffic. The server
never sends uninvited requests to clients but only responds to client requests.</p>
</div></blockquote>
</li>
<li><p>Can a client train with multiple GPUs?</p>
<blockquote>
<div><p>You do multiple-gpu training by putting your training executor within the a <a class="reference internal" href="programming_guide/executor.html#multi-process-executor"><span class="std std-ref">MultiProcessExecutor</span></a>.</p>
</div></blockquote>
</li>
<li><p>How do FL clients get identified?</p>
<blockquote>
<div><p>The federated learning clients are identified by a dynamically generated FL token issued by the server during runtime.
When an FL client first joins an FL training, it first needs to send a login request to the FL server. During the login
process, the FL server and client need to exchange SSL certificates for bi-directional authentication. Once the
authentication is successful, the FL server sends an FL token to the client. The FL client will use this FL token to
identify itself for all following requests for the global model and all model updating operations.</p>
</div></blockquote>
</li>
<li><p>Can I run multiple FL clients from the same machine?</p>
<blockquote>
<div><p>Yes. The FL clients are identified by FL token, not machine IP. Each FL client will have its own FL token as well as
instance name, which is the client name that must be used for issuing specific commands to that client.</p>
</div></blockquote>
</li>
<li><p>Can I use the same client package to run multiple instances for the same client?</p>
<blockquote>
<div><p>Yes, you can start multiple instances of FL clients from the same client packages. Each FL client will be identified
by its unique instance names, for example: “flclient1”, “flclient1_1”, “flclient1_2”, etc. The instance name must be
used for issuing specific commands to that client from the admin tool.</p>
</div></blockquote>
</li>
<li><p>What happens if a federated learning client crashes?</p>
<blockquote>
<div><p>Federated learning clients will send a heartbeat call to the FL server once every minute. If an FL client crashes and
the FL server does not get a heartbeat from that client for 10 minutes (can be set with “heart_beat_timeout” in the
server’s config json), the FL server will remove that client from the training client list.</p>
</div></blockquote>
</li>
<li><p>Can FL clients join or quit in the middle of federated learning training?</p>
<blockquote>
<div><p>Yes, an FL client can join or quit in the middle of the FL training at any time. The client will pick up the global
model at the current round of the server to participate in the FL training. When quitting, the FL server will
automatically remove the FL client after it quits and no heartbeat is received for the duration of the
“heart_beat_timeout” configured on the server. If using an admin tool, it is recommended to use the “abort” and
“shutdown” commands to gracefully stop the clients.</p>
</div></blockquote>
</li>
<li><p>For the Scatter and Gather workflow, what if the number of participating FL clients is below the minimum number of clients required?</p>
<blockquote>
<div><p>When an FL client passes authentication, it can request the current round of the global model and starts the FL training right away.
There is no need to wait for other clients. Once the client finishes its own training, it will send the update to the server
for aggregation. However, if the server does not receive enough updates from other clients, the FL server will not start
the next round of FL training. The finished FL client will be waiting for the next round’s model.</p>
</div></blockquote>
</li>
<li><p>For the Scatter and Gather workflow, what happens if more than the minimum numbers of FL clients submit an updated model?</p>
<blockquote>
<div><p>The FL server begins model aggregation after accepting updates from the minimum number of FL clients required and
waiting for “wait_after_min_clients” configured on the server. The updates that are received after this will be
discarded. All the clients will get the next round of the global model to start the next round FL training.</p>
</div></blockquote>
</li>
<li><p>How does a client decide to quit federated learning training?</p>
<blockquote>
<div><p>The FL client always asks the server for the next task to do. See how <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">controllers</span></a> assign tasks to clients.</p>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="server-related-questions">
<h2>Server related questions<a class="headerlink" href="#server-related-questions" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>What happens if the FL server crashes?</p>
<blockquote>
<div><p>There are two scenarios for the FL server crashing during the FL training. If the server crashes when the FL client is trying to
connect to the server for model exchange, the FL client will continue to attempt connecting to the server for up to 30 seconds.
If the server is still down after that, FL client will shut itself down. If the server crashes during the FL client model training,
as long as the server restarts before the FL client attempts model updating, it will have no impact to the FL clients.</p>
<p>When restarting the FL server, you can find the previous training round number from the previous log. Then you can choose to
train from scratch or continuously using previous training model.</p>
</div></blockquote>
</li>
<li><p>Does the federated learning server need a GPU?</p>
<blockquote>
<div><p>No, there is no need to have GPU on the server side for the FL server to deploy. However, certain handlers may require
GPUs. To disable GPUs on the server, include the following in the shell script that runs the server:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>What port do I need to open from the firewall on the FL server network?</p>
<blockquote>
<div><p>Depending on the configuration of <a class="reference internal" href="user_guide/provisioning_tool.html#id1"><span class="std std-ref">project.yaml</span></a> which controls which port the gRPC is deployed to,
the FL server network needs to open that port for outside clients to reach the FL server.</p>
</div></blockquote>
</li>
<li><p>What if the federated learning server is behind a load balancer?</p>
<blockquote>
<div><p>Currently, federated learning does not support load balancing between multiple FL servers.</p>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="overall-training-flow-related-questions">
<h2>Overall training flow related questions<a class="headerlink" href="#overall-training-flow-related-questions" title="Permalink to this headline">¶</a></h2>
<ol class="arabic">
<li><p>How does the federated learning server decide when to stop FL?</p>
<blockquote>
<div><p>For the Scatter and Gather workflow, the FL server runs from the “start_round” to “num_rounds”. The FL server will
stop the training when the current round meets “num_rounds”. For other workflows, the logic within the workflow can
make that decision.</p>
</div></blockquote>
</li>
<li><p>Can I run the FL server on AWS while running the FL client within my institution?</p>
<blockquote>
<div><p>Yes, use the AWS instance name as the server cn in project.yml file. (e.g.: ec2-3-99-123-456.compute-1.amazonaws.com)</p>
</div></blockquote>
</li>
<li><p>How can I deploy different applications for different clients?</p>
<blockquote>
<div><p>You can edit the application folder for each individual client on your desktop, then upload and deploy to each individual client
with the admin tool. Each client can run with its own application configuration.</p>
</div></blockquote>
</li>
<li><p>Can I use the same “run_number” as previously used?</p>
<blockquote>
<div><p>Yes, you can re-use the same “run_number” as previously used. The “run_number” serves as an FL training workspace. The
FL training logs, such as tensorboard, training stats, etc, are stored within the same “run_number” workspace. Note
that the app folder from the previous run will be replaced by the new run’s folder.</p>
</div></blockquote>
</li>
<li><p>What should I do if the admin notices one client’s training is behaving erroneously or unexpectedly?</p>
<blockquote>
<div><p>The admin can issue a command to abort the FL client training: <code class="docutils literal notranslate"><span class="pre">abort</span> <span class="pre">client</span> <span class="pre">client_name</span></code>. If the command is issued
without the client_name, then the command will be sent to all the clients. Because of the nature of model training, it
may take a little time for the FL client to completely stop. Use the “check_status client client_name” command to see
if the client status is “stopped”.</p>
</div></blockquote>
</li>
<li><p>Why do the admin commands to the clients have a long delay before getting a response?</p>
<blockquote>
<div><p>The admin commands to the clients pass through the server. If for some reason the command is delayed by the network, or
if the client command takes a long time to process, the admin console will experience a delay for the response. The
default timeout is 10 seconds. You can use the “set_timeout” command to adjust the command timeout. If this timeout
value is set too low, the admin command may not reach the client to execute the command.</p>
</div></blockquote>
</li>
<li><p>Can I use the same server / client set up to train different models?</p>
<blockquote>
<div><p>Yes, you can upload different applications to the server and clients to train different models. Make sure to use the
“run_number” to keep your trained models in different run spaces without confusing the models. The FL system only
completes when the admin issues the “shutdown” command or ctrl-C is used to end the process.</p>
</div></blockquote>
</li>
<li><p>Why is it that my custom components are not updating between runs?</p>
<blockquote>
<div><p>If you want to change the code you have already loaded in a custom component, it is recommended that you restart the
server and clients before the next run, that way, the Python code will be reloaded. Another method is to add a
version number or change the class name slightly. Python does not load new code definitions with the same class name
and by default Python does not allow the loaded modules to be removed. With a version or altered name, Python will
be able to treat the code as new and load it from the sys.path.</p>
</div></blockquote>
</li>
<li><p>Why do commands sometimes fail?</p>
<blockquote>
<div><p>Sometimes if you are trying to check status of the client and the server is already busy transferring the model and
does not have extra bandwidth for the command, the command may time out. In that case, please wait and try again.</p>
</div></blockquote>
</li>
</ol>
</div>
<div class="section" id="known-issues">
<h2>Known issues<a class="headerlink" href="#known-issues" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li><p>If server dies and then is restarted, intentionally or unintentionally, all clients will have to be restarted.</p></li>
<li><p>Running out of memory can happen at any time, especially if the server and clients are running on same machine.
This can cause the server to die unexpectedly.</p></li>
<li><p>Putting applications in the transfer folders without using the upload_app command or forgetting to delete the models
folder inside, a mysterious error may occur when running the deploy_app command because the application folder is too
large to be uploaded and that causes timeout.</p></li>
<li><p>Please don’t start a new training run or start a new app before the previous application is fully stopped. Users
can do <code class="docutils literal notranslate"><span class="pre">abort</span> <span class="pre">client</span></code> and <code class="docutils literal notranslate"><span class="pre">abort</span> <span class="pre">server</span></code> before <code class="docutils literal notranslate"><span class="pre">start_app</span></code> for the new run.</p></li>
</ol>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="apidocs/modules.html" class="btn btn-neutral float-right" title="nvflare" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="programming_guide/best_practices.html" class="btn btn-neutral float-left" title="NVIDIA FLARE Programming Best Practices" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, NVIDIA.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  p {
    margin-bottom: 1em;
  }

  .rst-content dl dt {
    font-weight: unset;
    margin-bottom: 0;
  }

  .rst-content .section ul p {
    margin-bottom: 0px;
  }
  </style>
  

</body>
</html>