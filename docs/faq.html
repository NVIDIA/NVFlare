<!--
# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions
# are met:
#  * Redistributions of source code must retain the above copyright
#    notice, this list of conditions and the following disclaimer.
#  * Redistributions in binary form must reproduce the above copyright
#    notice, this list of conditions and the following disclaimer in the
#    documentation and/or other materials provided with the distribution.
#  * Neither the name of NVIDIA CORPORATION nor the names of its
#    contributors may be used to endorse or promote products derived
#    from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY
# EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
# PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
# CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
# EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
# PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
# PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY
# OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
-->
<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Frequently Asked Questions &mdash; NVIDIA FLARE 2.1.0 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/additions.css" type="text/css" />
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Contributing" href="contributing.html" />
    <link rel="prev" title="Programming Best Practices" href="best_practices.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" > 
            <a href="index.html" class="icon icon-home"> NVIDIA FLARE
          </a>
              <div class="version">
                2.1.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    .wy-nav-content {
      max-width: 80%;
    }

    .floatleftcol {
      float: left;
      max-width: 60%;
      padding-right: 20px;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="highlights.html">Highlights</a></li>
<li class="toctree-l1"><a class="reference internal" href="flare_overview.html">NVIDIA FLARE Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="example_applications.html">Example Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide.html">User Guide - Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="programming_guide.html">Programming Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="best_practices.html">Programming Best Practices</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#general">General</a></li>
<li class="toctree-l2"><a class="reference internal" href="#operational">Operational</a></li>
<li class="toctree-l2"><a class="reference internal" href="#security">Security</a></li>
<li class="toctree-l2"><a class="reference internal" href="#client-related-questions">Client related questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#server-related-questions">Server related questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#overall-training-flow-related-questions">Overall training flow related questions</a></li>
<li class="toctree-l2"><a class="reference internal" href="#known-issues">Known issues</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="apidocs/modules.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="appendix.html">Appendix</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">NVIDIA FLARE</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Frequently Asked Questions</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/faq.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="frequently-asked-questions">
<span id="faq"></span><h1>Frequently Asked Questions<a class="headerlink" href="#frequently-asked-questions" title="Permalink to this heading">¶</a></h1>
<section id="general">
<h2>General<a class="headerlink" href="#general" title="Permalink to this heading">¶</a></h2>
<ol class="arabic">
<li><p>What is NVIDIA FLARE?</p>
<blockquote>
<div><p>NVIDIA FLARE is a general-purpose framework designed for collaborative computing.  In this collaborative
computing framework, workflows are not limited to aggregation-based federated learning (usually called a Fed-Average workflow),
and applications are not limited to deep learning.  NVIDIA FLARE is fundamentally a messaging system running in a multithreaded
environment.</p>
</div></blockquote>
</li>
<li><p>What does NVIDIA FLARE stand for?</p>
<blockquote>
<div><p>NVIDIA Federated Learning Application Runtime Environment.</p>
</div></blockquote>
</li>
<li><p>Does NVIDIA FLARE depend on Tensorflow or PyTorch?</p>
<blockquote>
<div><p>No.  NVIDIA FLARE is a Python library that implements a general collaborative computing framework.  The <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controllers</span></a>,
<a class="reference internal" href="programming_guide/executor.html#executor"><span class="std std-ref">Executors</span></a>, and <a class="reference internal" href="programming_guide/controllers.html#tasks"><span class="std std-ref">Tasks</span></a> that one defines to execute the collaborative computing workflow
are entirely independent.</p>
</div></blockquote>
</li>
<li><p>Is NVIDIA FLARE designed for deep learning model training only?</p>
<blockquote>
<div><p>No.  NVIDIA FLARE implements a communication framework that can support any collaborative computing workflow.  This
could be deep learning, machine learning, or even simple statistical workflows.</p>
</div></blockquote>
</li>
<li><p>Does NVIDIA FLARE require a GPU?</p>
<blockquote>
<div><p>No.  Hardware requirements are dependent only on what is implemented in the <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controller</span></a> workflow and client <a class="reference internal" href="programming_guide/controllers.html#tasks"><span class="std std-ref">Tasks</span></a>.
Client training tasks will typically benefit from GPU acceleration.  Server <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controller</span></a> workflows may or may not require a GPU.</p>
</div></blockquote>
</li>
<li><p>How does NVIDIA FLARE implement its collaborative computing framework?</p>
<blockquote>
<div><p>NVIDIA FLARE collaborative computing is achieved through <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controller/Worker</span></a> interaction.</p>
</div></blockquote>
</li>
<li><p>What is a Controller?</p>
<blockquote>
<div><p>The <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controller</span></a> is a python object that controls or coordinates Workers to perform tasks. The
Controller is run on the server.  The Controller defines the overall collaborative computing workflow.  In its
control logic, the Controller assigns tasks to Workers and processes task results from the workers.</p>
</div></blockquote>
</li>
<li><p>What is a Worker?</p>
<blockquote>
<div><p>A Worker is capable of performing tasks (skills). Workers run on Clients.</p>
</div></blockquote>
</li>
<li><p>What is a Task?</p>
<blockquote>
<div><p>A <a class="reference internal" href="programming_guide/controllers.html#tasks"><span class="std std-ref">Task</span></a> is a piece of work (Python code) that is assigned by the <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">Controller</span></a> to
client workers. Depending on how the Task is assigned (broadcast, send, or relay), the task will be performed by one
or more clients.  The logic to be performed in a Task is defined in an <a class="reference internal" href="programming_guide/executor.html#executor"><span class="std std-ref">Executor</span></a>.</p>
</div></blockquote>
</li>
<li><p>What is Learnable?</p>
<blockquote>
<div><p>Learnable is the result of the Federated Learning application maintained by the server.  In DL workflows, the
Learnable is the aspect of the DL model to be learned.  For example, the model weights are commonly the Learnable
feature, not the model geometry.  Depending on the purpose of your study, the Learnable may be any component of interest.
Learnable is an abstract object that is aggregated from the client’s Shareable object and is not DL-specific.  It
can be any model, or object.  The Learnable is managed in the Controller workflow.</p>
</div></blockquote>
</li>
<li><p>What is Shareable?</p>
<blockquote>
<div><p><a class="reference internal" href="programming_guide/shareable.html#shareable"><span class="std std-ref">Shareable</span></a> is simply a communication between two peers (server and clients). In the task-based
interaction, the Shareable from server to clients carries the data of the task for the client to execute; and the
Shareable from the client to server carries the result of the task execution.  When this is applied to DL model
training, the task data typically contains model weights for the client to train on; and the task result contains
updated model weights from the client.  The concept of Shareable is very general - it can be whatever that makes
sense for the task.</p>
</div></blockquote>
</li>
<li><p>What is FLContext and what kind of information does it contain?</p>
<blockquote>
<div><p><a class="reference internal" href="programming_guide/fl_context.html#fl-context"><span class="std std-ref">FLContext</span></a> is one of the key features of NVIDIA FLARE and is available to every method of all <a class="reference internal" href="programming_guide/fl_component.html#fl-component"><span class="std std-ref">FLComponent</span></a>
types (Controller, Aggregator, Executor, Filter, Widget, …). An FLContext object contains contextual information
of the FL environment: overall system settings (peer name, job id / run number, workspace location, etc.). FLContext
also contains an important object called Engine, through which you can access important services provided by the
system (e.g. fire events, get all available client names, send aux messages, etc.).</p>
</div></blockquote>
</li>
<li><p>What are events and how are they handled?</p>
<blockquote>
<div><p><a class="reference internal" href="programming_guide/event_system.html#event-system"><span class="std std-ref">Events</span></a> allow for dynamic notifications to be sent to all objects that are a subclass of
<a class="reference internal" href="programming_guide/fl_component.html#fl-component"><span class="std std-ref">FLComponent</span></a>. Every FLComponent is an event handler.</p>
<p>The event mechanism is like a pub-sub mechanism that enables indirect communication between components for data
sharing. Typically, the data generator fires an event to publish the data, and other components handle the events
they are subscribed to and consume the data of the event. The fed event mechanism even allows the pub-sub go across
network boundaries.</p>
</div></blockquote>
</li>
<li><p>What additional components may be implemented with NVIDIA FLARE to support the Controller Workflow, and where do they run (server or client):</p>
<blockquote>
<div><dl class="simple">
<dt>LearnablePersistor - Server</dt><dd><p>The LearnablePersistor is a method implemented for the server to save the state of the Learnable object, for
example writing a global model to disk for persistence.</p>
</dd>
<dt>ShareableGenerator - Server</dt><dd><p>The ShareableGenerator is an object that implements two methods: learnable_to_shareable converts a Learnable
object to a form of data to be shared to the client; shareable_to_learnable uses the shareable data (or
aggregated shareable data) from the clients to update the learnable object.</p>
</dd>
<dt>Aggregator - Server</dt><dd><p>The aggregator defines the algorithm used on the server to aggregate the data passed back to the server in the
clients’ Shareable object.</p>
</dd>
<dt>Executor - Client</dt><dd><p>The Executor defines the algorithm the clients use to operate on data contained in the Shareable object.  For
example in DL training, the executor would implement the training loop. There can be multiple executors on the
client, designed to execute different tasks (training, validation/evaluation, data preparation, etc.).</p>
</dd>
<dt>Filter - Clients and Server</dt><dd><p><a class="reference internal" href="programming_guide/filters.html#filters"><span class="std std-ref">Filters</span></a> are used to define transformations of the data in the Shareable object when transferred between server
and client and vice versa.  Filters can be applied when the data is sent or received by either the client or server.
See the diagram on the <a class="reference internal" href="programming_guide/filters.html#filters"><span class="std std-ref">Filters</span></a> page for details on when “task_data_filters” and “task_result_filters”
are applied on the client and server.</p>
</dd>
<dt>Any component of subclass of FLComponent</dt><dd><p>All component types discussed above are subclasses of <a class="reference internal" href="programming_guide/fl_component.html#fl-component"><span class="std std-ref">FLComponent</span></a>. You can create your own subclass of
FLComponent for various purposes. For example, you can create such a component to listen to certain events and
handle the data of the events (analysis, dump to disk or DB, etc.).</p>
</dd>
</dl>
</div></blockquote>
</li>
</ol>
</section>
<section id="operational">
<h2>Operational<a class="headerlink" href="#operational" title="Permalink to this heading">¶</a></h2>
<ol class="arabic">
<li><p>What is <a class="reference internal" href="programming_guide/provisioning_system.html#provisioning"><span class="std std-ref">Provisioning</span></a>?</p>
<blockquote>
<div><p>NVIDIA FLARE includes an Open Provision API that allows you to generate mutual-trusted system-wide configurations,
or startup kits, that allow all participants to join the NVIDIA FLARE system from across different locations.  This
mutual-trust is a mandatory feature of Open Provision API as every participant authenticates others by the
information inside the configuration.  The configurations usually include, but are not limited to:</p>
<blockquote>
<div><ul class="simple">
<li><p>network discovery, such as domain names, port numbers or IP addresses</p></li>
<li><p>credentials for authentication, such as certificates of participants and root authority</p></li>
<li><p>authorization policy, such as roles, rights and rules</p></li>
<li><p>tamper-proof mechanism, such as signatures</p></li>
<li><p>convenient commands, such as shell scripts with default command line options to easily start an individual participant</p></li>
</ul>
</div></blockquote>
</div></blockquote>
</li>
<li><p>What types of startup kits are generated by the Provision tool?</p>
<blockquote>
<div><p>The Open Provision API allows flexibility in generating startup kits, but typically the provisioning tool is used to
generate secure startup kits for the Overseer, FL servers, FL clients, and Admin clients.</p>
</div></blockquote>
</li>
<li><p>What files does each type of startup kit contain? What are these files used for, and by whom?</p>
<blockquote>
<div><p>Startup kits contain the configuration and certificates necessary to establish secure connections between the Overseer, FL servers,
FL clients, and Admin clients.  These files are used to establish identity and authorization policies between server
and clients.  Startup kits are distributed to the Overseer, FL servers, clients, and Admin clients depending on role.  For the
purpose of development, startup kits may be generated with limited security to allow simplified connection between
systems or between processes on a single host.  See the “poc” functionality of the Open Provision API for details.</p>
</div></blockquote>
</li>
<li><p>How would you distribute the startup kits to the right people?</p>
<blockquote>
<div><p>Distribution of startup kits is inherently flexible and can be via email or shared storage.  The API allows the
addition of builder components to automation distribution.</p>
</div></blockquote>
</li>
<li><p>What happens after provisioning?</p>
<blockquote>
<div><p>After provisioning, the Admin API is used to submit a job to the FL server, and the JobRunner on the server can pick
it up to deploy and run.</p>
</div></blockquote>
</li>
<li><p>What is an Application in NVIDIA FLARE?</p>
<blockquote>
<div><p>An <a class="reference internal" href="user_guide/application.html#application"><span class="std std-ref">Application</span></a> is a named directory structure that defines the client and server configuration
and any custom code required to implement the Controller/Worker workflow.</p>
</div></blockquote>
</li>
<li><p>What is the basic directory structure of an NVIDIA FLARE Application?</p>
<blockquote>
<div><p>Typically the Application configuration is defined in a <code class="docutils literal notranslate"><span class="pre">config/</span></code>
subdirectory and defines paths to Controller and Worker executors.  Custom code can be defined in a <code class="docutils literal notranslate"><span class="pre">custom/</span></code>
subdirectory and is subject to rules defined in the Authorization Policy.</p>
</div></blockquote>
</li>
<li><p>How do you deploy an application?</p>
<blockquote>
<div><p>An Application is deployed using the <code class="docutils literal notranslate"><span class="pre">submit_job</span></code> admin command. For more configuration, apps can be packaged into
jobs with deploy_map definitions to specify which sites which apps should be deployed to. The deployment happens
automatically with the JobRunner on the FL server.</p>
</div></blockquote>
</li>
<li><p>Do all FL client have to use the same application configuration?</p>
<blockquote>
<div><p>No, they do not have to use the same application configuration, even though they can that is frequently done. The
function of FL clients can be customized by the implementation of Tasks and Executors and the client’s
response to Events.</p>
</div></blockquote>
</li>
<li><p>What is the difference between the Admin client and the FL client?</p>
<blockquote>
<div><p>The <a class="reference internal" href="user_guide/operation.html#operating-nvflare"><span class="std std-ref">Admin client</span></a> is used to control the state of the server’s controller workflow and only interacts with the
server.  FL clients poll the server and perform tasks based on the state of the server.  The Admin client does not
interact directly with FL client.</p>
</div></blockquote>
</li>
<li><p>Where does the Admin client run?</p>
<blockquote>
<div><p>The <a class="reference internal" href="user_guide/operation.html#operating-nvflare"><span class="std std-ref">Admin client</span></a> runs as a standalone process, typically on a researcher’s workstation or laptop.</p>
</div></blockquote>
</li>
<li><p>What can you do with the Admin client?</p>
<blockquote>
<div><p>The <a class="reference internal" href="user_guide/operation.html#operating-nvflare"><span class="std std-ref">Admin client</span></a> is used to orchestrate the FL study, including starting and stopping server
and clients, deploying applications, and managing FL experiments.</p>
</div></blockquote>
</li>
<li><p>Why am I getting an error about my custom files not being found?</p>
<blockquote>
<div><p>Make sure that BYOC is enabled. BYOC is always enabled in POC mode, but disabled by default in secure mode when
provisioning.  Either through the UI tool or though yml, make sure the <code class="docutils literal notranslate"><span class="pre">enable_byoc</span></code> flag is set for each participant.
If the <code class="docutils literal notranslate"><span class="pre">enable_byoc</span></code> flag is disabled, even if you have custom code in your application folder, it will not be loaded.
There is also a setting for <code class="docutils literal notranslate"><span class="pre">allow_byoc</span></code> through the authorization rule groups. This controls whether or not apps
containing BYOC code will be allowed to be uploaded and deployed.</p>
</div></blockquote>
</li>
</ol>
</section>
<section id="security">
<h2>Security<a class="headerlink" href="#security" title="Permalink to this heading">¶</a></h2>
<ol class="arabic">
<li><p>What is the scope of security in NVIDIA FLARE?</p>
<blockquote>
<div><p>Security is multi-faceted and cannot be completely controlled for or provided by the NVIDIA FLARE API.  The Open
Provision API provides examples of basic communication and identity security using GRPC via shared self-signed
certificates and authorization policies.  These security measures may be sufficient but can be extended with the
provided APIs.</p>
</div></blockquote>
</li>
<li><p>What about data privacy?</p>
<blockquote>
<div><p>NVIDIA FLARE comes with a few techniques to help with data privacy during FL: differential privacy and homomorphic encryption
(see <a class="reference internal" href="programming_guide/filters.html#filters-for-privacy"><span class="std std-ref">Privacy filters</span></a>).</p>
</div></blockquote>
</li>
</ol>
</section>
<section id="client-related-questions">
<h2>Client related questions<a class="headerlink" href="#client-related-questions" title="Permalink to this heading">¶</a></h2>
<ol class="arabic">
<li><p>What happens if an FL client joins during the FL training?</p>
<blockquote>
<div><p>An FL client can join the FL training any time. It is up to the workflow logic to manage FL clients.</p>
</div></blockquote>
</li>
<li><p>Do federated learning clients need to open any ports for the FL server to reach the FL client?</p>
<blockquote>
<div><p>No, federated learning training does not require for FL clients to open their network for inbound traffic. The server
never sends uninvited requests to clients but only responds to client requests.</p>
</div></blockquote>
</li>
<li><p>Can a client train with multiple GPUs?</p>
<blockquote>
<div><p>You do multiple-gpu training by putting your training executor within the a <a class="reference internal" href="programming_guide/executor.html#multi-process-executor"><span class="std std-ref">MultiProcessExecutor</span></a>.</p>
</div></blockquote>
</li>
<li><p>How do FL clients get identified?</p>
<blockquote>
<div><p>The federated learning clients are identified by a dynamically generated FL token issued by the server during runtime.
When an FL client first joins an FL training, it first needs to send a login request to the FL server. During the login
process, the FL server and client need to exchange SSL certificates for bi-directional authentication. Once the
authentication is successful, the FL server sends an FL token to the client. The FL client will use this FL token to
identify itself for all following requests for the global model and all model updating operations.</p>
</div></blockquote>
</li>
<li><p>Can I run multiple FL clients from the same machine?</p>
<blockquote>
<div><p>Yes. The FL clients are identified by FL token, not machine IP. Each FL client will have its own FL token as well as
instance name, which is the client name that must be used for issuing specific commands to that client.</p>
</div></blockquote>
</li>
<li><p>Can I use the same client package to run multiple instances for the same client?</p>
<blockquote>
<div><p>Yes, you can start multiple instances of FL clients from the same client packages. Each FL client will be identified
by its unique instance names, for example: “flclient1”, “flclient1_1”, “flclient1_2”, etc. The instance name must be
used for issuing specific commands to that client from the admin tool.</p>
</div></blockquote>
</li>
<li><p>What happens if a federated learning client crashes?</p>
<blockquote>
<div><p>Federated learning clients will send a heartbeat call to the FL server once every minute. If an FL client crashes and
the FL server does not get a heartbeat from that client for 10 minutes (can be set with “heart_beat_timeout” in the
server’s config json), the FL server will remove that client from the training client list.</p>
</div></blockquote>
</li>
<li><p>Can FL clients join or quit in the middle of federated learning training?</p>
<blockquote>
<div><p>Yes, an FL client can join or quit in the middle of the FL training at any time. The client will pick up the global
model at the current round of the server to participate in the FL training. When quitting, the FL server will
automatically remove the FL client after it quits and no heartbeat is received for the duration of the
“heart_beat_timeout” configured on the server. If using an admin tool, it is recommended to use the “abort” and
“shutdown” commands to gracefully stop the clients.</p>
</div></blockquote>
</li>
<li><p>For the Scatter and Gather workflow, what if the number of participating FL clients is below the minimum number of clients required?</p>
<blockquote>
<div><p>When an FL client passes authentication, it can request the current round of the global model and starts the FL training right away.
There is no need to wait for other clients. Once the client finishes its own training, it will send the update to the server
for aggregation. However, if the server does not receive enough updates from other clients, the FL server will not start
the next round of FL training. The finished FL client will be waiting for the next round’s model.</p>
</div></blockquote>
</li>
<li><p>For the Scatter and Gather workflow, what happens if more than the minimum numbers of FL clients submit an updated model?</p>
<blockquote>
<div><p>The FL server begins model aggregation after accepting updates from the minimum number of FL clients required and
waiting for “wait_after_min_clients” configured on the server. The updates that are received after this will be
discarded. All the clients will get the next round of the global model to start the next round FL training.</p>
</div></blockquote>
</li>
<li><p>How does a client decide to quit federated learning training?</p>
<blockquote>
<div><p>The FL client always asks the server for the next task to do. See how <a class="reference internal" href="programming_guide/controllers.html#controllers"><span class="std std-ref">controllers</span></a> assign tasks to clients.</p>
</div></blockquote>
</li>
</ol>
</section>
<section id="server-related-questions">
<h2>Server related questions<a class="headerlink" href="#server-related-questions" title="Permalink to this heading">¶</a></h2>
<ol class="arabic">
<li><p>What happens if the FL server crashes?</p>
<blockquote>
<div><p>See <a class="reference internal" href="programming_guide/high_availability.html#high-availability"><span class="std std-ref">High Availability and Server Failover</span></a> for the features implemented in NVIDIA FLARE 2.1.0 around FL server failover.</p>
</div></blockquote>
</li>
<li><p>Why does my FL server keep crashing after a certain round?</p>
<blockquote>
<div><p>Check that the amount of memory being consumed is not increasing in a way that it exceeds the available resources.
If the process consumes too much memory, the operating system may kill it.</p>
</div></blockquote>
</li>
<li><p>Does the federated learning server need a GPU?</p>
<blockquote>
<div><p>No, there is no need to have GPU on the server side for the FL server to deploy. However, certain handlers may require
GPUs. To disable GPUs on the server, include the following in the shell script that runs the server:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">CUDA_VISIBLE_DEVICES</span><span class="o">=</span>
</pre></div>
</div>
</div></blockquote>
</li>
<li><p>What port do I need to open from the firewall on the FL server network?</p>
<blockquote>
<div><p>Depending on the configuration of <a class="reference internal" href="programming_guide/provisioning_system.html#id2"><span class="std std-ref">project.yaml</span></a> which controls which port the gRPC is deployed to,
the FL server network needs to open that port for outside clients to reach the FL server.</p>
</div></blockquote>
</li>
<li><p>What if the federated learning server is behind a load balancer?</p>
<blockquote>
<div><p>Currently, federated learning does not support load balancing between multiple FL servers.</p>
</div></blockquote>
</li>
<li><p>Is the Overseer now a new single point of failure?</p>
<blockquote>
<div><p>Even if the Overseer is out of service for a period of time, the whole system is designed to continue working if no
FL server outage happens. If an FL server outage happens while the Overseer is also unavailable, the whole system
will just keep trying to reconnect and restore services when the outage is over. High Availability is not guaranteed
availability. The design goal of HA was to keep the system operational as much as possible without human intervention.</p>
</div></blockquote>
</li>
</ol>
</section>
<section id="overall-training-flow-related-questions">
<h2>Overall training flow related questions<a class="headerlink" href="#overall-training-flow-related-questions" title="Permalink to this heading">¶</a></h2>
<ol class="arabic">
<li><p>How does the federated learning server decide when to stop FL?</p>
<blockquote>
<div><p>For the Scatter and Gather workflow, the FL server runs from the “start_round” to “num_rounds”. The FL server will
stop the training when the current round meets “num_rounds”. For other workflows, the logic within the workflow can
make that decision.</p>
</div></blockquote>
</li>
<li><p>Can I run the FL server on AWS while running the FL client within my institution?</p>
<blockquote>
<div><p>Yes, use the AWS instance name as the server cn in project.yml file. (e.g.: ec2-3-99-123-456.compute-1.amazonaws.com)</p>
</div></blockquote>
</li>
<li><p>How can I deploy different applications for different clients?</p>
<blockquote>
<div><p>You can edit the application folder for each individual client on your desktop, then upload and deploy to each individual client
with the admin tool. Each client can run with its own application configuration.</p>
</div></blockquote>
</li>
<li><p>What should I do if the admin notices one client’s training is behaving erroneously or unexpectedly?</p>
<blockquote>
<div><p>The admin can issue a command to abort the FL client training for a specified job: <code class="docutils literal notranslate"><span class="pre">abort</span> <span class="pre">job_id</span> <span class="pre">client</span> <span class="pre">client_name</span></code>. If the command is issued
without the client_name, then the command will be sent to all the clients. Because of the nature of model training, it
may take a little time for the FL client to completely stop. Use the “check_status client client_name” command to see
if the client status is “stopped”.</p>
</div></blockquote>
</li>
<li><p>Why do the admin commands to the clients have a long delay before getting a response?</p>
<blockquote>
<div><p>The admin commands to the clients pass through the server. If for some reason the command is delayed by the network, or
if the client command takes a long time to process, the admin console will experience a delay for the response. The
default timeout is 10 seconds. You can use the “set_timeout” command to adjust the command timeout. If this timeout
value is set too low, the admin command may not reach the client to execute the command.</p>
</div></blockquote>
</li>
<li><p>Why do commands sometimes fail?</p>
<blockquote>
<div><p>Sometimes if you are trying to check status of the client and the server is already busy transferring the model and
does not have extra bandwidth for the command, the command may time out. In that case, please wait and try again.</p>
</div></blockquote>
</li>
</ol>
</section>
<section id="known-issues">
<h2>Known issues<a class="headerlink" href="#known-issues" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p>If the IP of the server changes, the admin client may not be able to connect anymore because the admin server remains
bound to the original host and port. A possible workaround is to restart the FL server manually, and then the host
will resolve to the updated IP for binding when restarting.</p></li>
<li><p>Running out of memory can happen at any time, especially if the server and clients are running on same machine.
This can cause the server to die unexpectedly.</p></li>
<li><p>After calling <code class="docutils literal notranslate"><span class="pre">shutdown</span> <span class="pre">client</span></code> for a client running multi GPUs, a process (sub_worker_process) may remain. The
work around for this is to run <code class="docutils literal notranslate"><span class="pre">abort</span> <span class="pre">client</span></code> before the <code class="docutils literal notranslate"><span class="pre">shutdown</span></code> command.</p></li>
<li><p>If a snapshot is in a corrupted state, the server may try to restore the job and get stuck. To resolve this, delete
the snapshot from the location configured in project.yml for the snapshot_persistor storage (by default
<code class="docutils literal notranslate"><span class="pre">/tmp/nvflare/jobs-storage</span></code>), and <code class="docutils literal notranslate"><span class="pre">abort_job</span></code> should be able to stop the job on the server.</p></li>
</ol>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="best_practices.html" class="btn btn-neutral float-left" title="Programming Best Practices" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="contributing.html" class="btn btn-neutral float-right" title="Contributing" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  p {
    margin-bottom: 1em;
  }

  .rst-content dl dt {
    font-weight: unset;
    margin-bottom: 0;
  }

  .rst-content .section ul p {
    margin-bottom: 0px;
  }
  </style>
  

</body>
</html>