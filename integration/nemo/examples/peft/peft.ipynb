{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "820bbfbf",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Fine-Tuning (PEFT) with NeMo\n",
    "\n",
    "In this example, we utilize NeMo's [PEFT](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/prompt_learning.html)\n",
    "methods to showcase how to adapt a large language model (LLM) to \n",
    "a downstream task, such as financial sentiment predictions. \n",
    "\n",
    "With one line configuration change, you can try different PEFT techniques such as [p-tuning](https://arxiv.org/abs/2103.10385), [adapters](https://proceedings.mlr.press/v97/houlsby19a.html), or [LoRA](https://arxiv.org/abs/2106.09685), which add a small number of trainable parameters to the LLM\n",
    "that condition the model to produce the desired output for the downstream task.\n",
    "\n",
    "For more details, see the [PEFT script](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_peft_tuning.py) in NeMo, which we adapt using NVFlare's Lightning client API to run in a federated scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92639627",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "We assume you followed the instructions [here](../../README.md#requirements) \n",
    "to install the NeMo framework and the NeMo-NVFlare package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950f2337",
   "metadata": {},
   "source": [
    "## Download the pre-trained LLM\n",
    "In this example, we use a `MegatronGPTModel`, a transformer-based language model based on the GPT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a842f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what GPT .nemo models we have available on NGC\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
    "MegatronGPTModel.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0bc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model from NGC\n",
    "import os\n",
    "model_file = \"megatron_gpt_345m.nemo\"\n",
    "if not os.path.isfile(model_file):\n",
    "    !wget \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/megatron_gpt_345m/versions/1/files/$model_file\"\n",
    "else:\n",
    "    print(f\"{model_file} already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395e2a09",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "As our downstream task, we will use the [Financial PhraseBank dataset](https://huggingface.co/datasets/financial_phrasebank) for sentiment analysis.\n",
    "\n",
    "The Financial PhraseBank dataset contains the sentiments for financial news headlines from a retail investor's perspective. Further details about the dataset can be found in Malo et al.'s [\"Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts\"](https://arxiv.org/abs/1307.5336).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d354678c",
   "metadata": {},
   "source": [
    "#### 1. Download the preprocessing scripts\n",
    "We use the preprocessing scripts provided by NeMo which can be downloaded from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c47ec4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = \"prompt_learning_financial_phrase_bank_preprocessing.py\"\n",
    "if not os.path.isfile(script_name):\n",
    "    !wget -N \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/dataset_processing/nlp/financial_phrase_bank/$script_name\"\n",
    "else:\n",
    "    print(f\"{script_name} already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3e3d70",
   "metadata": {},
   "source": [
    "#### 2. Download the Financial PhraseBank Dataset\n",
    "\n",
    "Download the `FinancialPhraseBank-v1.0.zip` dataset from [here](https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v1.0/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v1.0.zip).\n",
    "\n",
    "Then extract it under `./data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d81b2b",
   "metadata": {},
   "source": [
    "#### 3. Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e731b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 prompt_learning_financial_phrase_bank_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0cabd8",
   "metadata": {},
   "source": [
    "#### 4. Split the dataset to simulate clients\n",
    "Next, we use three clients to simulate federated learning for p-tuning with NeMo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6debfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 data/split_financial_phrase_data.py --data_path data/FinancialPhraseBank-v1.0/financial_phrase_bank_train.jsonl --num_clients 3 --out_dir data/FinancialPhraseBank-v1.0_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4162b",
   "metadata": {},
   "source": [
    "## Federated learning simulations\n",
    "Next, we are using NVFlare's [simulator](https://nvflare.readthedocs.io/en/latest/user_guide/fl_simulator.html) to simulate each client training on their own dataset locally and all three clients training together using the [FedAvg](https://arxiv.org/abs/1602.05629) algorithm implemented in NVFlare.\n",
    "\n",
    "With this setting, we require a GPU with at least 16GB of memory to run all clients in parallel on the same GPU. \n",
    "If you have multiple GPUs in your system, you can use the `gpu` argument to assign one GPU for each client, e.g., `gpu=\"0,1\"`.\n",
    "\n",
    "We will use NVFlare's job command for each setting to create the configurations needed to train the models based on the [sag_nemo](https://github.com/NVIDIA/NVFlare/blob/main/job_templates/sag_pt_deploy_map/info.md) job template. This template allows the definition of different configurations for each client, which we will use to assign their local training data file to each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0bd09",
   "metadata": {},
   "source": [
    "#### 1. Local P-Tuning\n",
    "First, we create the job files and modify them to include the data paths for each client and the pre-trained LLM using the `-f` option.\n",
    "Note, the `app_config` options are specific to the app script (`megatron_gpt_peft_tuning.py`) and modify variables in the NeMo config file (`megatron_gpt_peft_tuning_config.yaml`) directly on execution.\n",
    "\n",
    "At this point, we also modify the local number of clients, local epochs and FL rounds to simulate local training.\n",
    "\n",
    "The PEFT method is \"ptuning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd828e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NVFLARE_HOME=/home/hroth/Code2/nvflare/nemo_peft_example\n",
    "#!python3 -m pip install -e /home/hroth/Code2/nvflare/nemo_peft_example\n",
    "\n",
    "import os\n",
    "peft_scheme=\"model.peft.peft_scheme\\=ptuning\" # can be either ptuning, adapter, lora, or ia3\n",
    "app_script=\"megatron_gpt_peft_tuning.py\"\n",
    "restore_from_path=f\"model.restore_from_path\\={os. getcwd()}/megatron_gpt_345m.nemo\"\n",
    "trainer_config=\"trainer.max_steps\\=2000 trainer.val_check_interval\\=100\"\n",
    "val_files=f\"model.data.validation_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\\]\"\n",
    "train_files_prefix=f\"model.data.train_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0_split/site\"\n",
    "\n",
    "!nvflare job create -force -j \"./jobs/peft_p-tuning_local_345M\" -w \"sag_nemo\" -sd \"code\" \\\n",
    "   -f app_1/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-1.jsonl\\]\" \\\n",
    "   -f app_2/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-2.jsonl\\]\" \\\n",
    "   -f app_3/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-3.jsonl\\]\" \\\n",
    "   -f app_server/config_fed_server.conf num_rounds=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa379c6",
   "metadata": {},
   "source": [
    "Next, simulate each client p-tuning on their local dataset using the FL simulator. To do this, we only run 1 round of FL, with each client running 50 p-tuning epochs on their local dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70776697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/peft_p-tuning_local_345M\",\n",
    "    workspace=\"/tmp/nvflare/nemo/peft_p-tuning_local_345M\",\n",
    "    n_clients=3,\n",
    "    threads=3\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c515c00",
   "metadata": {},
   "source": [
    "#### 2. Federated P-Tuning\n",
    "Next, we use the [FedAvg](https://arxiv.org/abs/1602.05629) algorithm to p-tune the model in a federated scenario. First, create and modify the configuration files again. \n",
    "This time, we increase the number of FL rounds and decrease the number of local epochs per round to match the federated scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283c520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 create_configs.py --job_folder \"jobs/peft_p-tuning_fedavg_345M\" --num_clients 3 --max_steps 200 --num_rounds 50\n",
    "import os\n",
    "peft_scheme=\"model.peft.peft_scheme\\=ptuning\" # can be either ptuning, adapter, lora, or ia3\n",
    "app_script=\"megatron_gpt_peft_tuning.py\"\n",
    "restore_from_path=f\"model.restore_from_path\\={os. getcwd()}/megatron_gpt_345m.nemo\"\n",
    "trainer_config=\"trainer.max_steps\\=200 trainer.val_check_interval\\=100\"\n",
    "val_files=f\"model.data.validation_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\\]\"\n",
    "train_files_prefix=f\"model.data.train_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0_split/site\"\n",
    "\n",
    "!nvflare job create -force -j \"./jobs/peft_p-tuning_fedavg_345M\" -w \"sag_nemo\" -sd \"code\" \\\n",
    "   -f app_1/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-1.jsonl\\]\" \\\n",
    "   -f app_2/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-2.jsonl\\]\" \\\n",
    "   -f app_3/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-3.jsonl\\]\" \\\n",
    "   -f app_server/config_fed_server.conf num_rounds=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b909ce",
   "metadata": {},
   "source": [
    "Next, simulate the federated p-tuning using FedAvg. Here, each client p-tunes for one local epoch before sending their local model updates to the server for aggregation. This is repeated for 50 FL rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c1cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 02:38:25,544 - SimulatorRunner - INFO - Create the Simulator Server.\n",
      "2023-10-26 02:38:25,549 - CoreCell - INFO - server: creating listener on tcp://0:47043\n",
      "2023-10-26 02:38:25,572 - CoreCell - INFO - server: created backbone external listener for tcp://0:47043\n",
      "2023-10-26 02:38:25,573 - ConnectorManager - INFO - 36034: Try start_listener Listener resources: {'secure': False, 'host': 'localhost'}\n",
      "2023-10-26 02:38:25,575 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00002 PASSIVE tcp://0:40420] is starting\n",
      "2023-10-26 02:38:26,077 - CoreCell - INFO - server: created backbone internal listener for tcp://localhost:40420\n",
      "2023-10-26 02:38:26,079 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 PASSIVE tcp://0:47043] is starting\n",
      "2023-10-26 02:38:26,152 - nvflare.fuel.hci.server.hci - INFO - Starting Admin Server localhost on Port 46343\n",
      "2023-10-26 02:38:26,154 - SimulatorRunner - INFO - Deploy the Apps.\n",
      "2023-10-26 02:38:26,163 - SimulatorRunner - INFO - Create the simulate clients.\n",
      "2023-10-26 02:38:26,168 - ClientManager - INFO - Client: New client site-1@192.168.0.34 joined. Sent token: 739505dd-4dd5-4974-9ee4-82620c7cc6be.  Total clients: 1\n",
      "2023-10-26 02:38:26,169 - FederatedClient - INFO - Successfully registered client:site-1 for project simulator_server. Token:739505dd-4dd5-4974-9ee4-82620c7cc6be SSID:\n",
      "2023-10-26 02:38:26,172 - ClientManager - INFO - Client: New client site-2@192.168.0.34 joined. Sent token: c2418497-58b7-4aaa-b9a2-a63029974890.  Total clients: 2\n",
      "2023-10-26 02:38:26,173 - FederatedClient - INFO - Successfully registered client:site-2 for project simulator_server. Token:c2418497-58b7-4aaa-b9a2-a63029974890 SSID:\n",
      "2023-10-26 02:38:26,175 - ClientManager - INFO - Client: New client site-3@192.168.0.34 joined. Sent token: 09b99ebc-684c-4fec-a3ba-d9e6d3a996ab.  Total clients: 3\n",
      "2023-10-26 02:38:26,176 - FederatedClient - INFO - Successfully registered client:site-3 for project simulator_server. Token:09b99ebc-684c-4fec-a3ba-d9e6d3a996ab SSID:\n",
      "2023-10-26 02:38:26,177 - SimulatorRunner - INFO - Set the client status ready.\n",
      "2023-10-26 02:38:26,178 - SimulatorRunner - INFO - Deploy and start the Server App.\n",
      "2023-10-26 02:38:26,180 - Cell - INFO - Register blob CB for channel='server_command', topic='*'\n",
      "2023-10-26 02:38:26,181 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-10-26 02:38:26,182 - ServerCommandAgent - INFO - ServerCommandAgent cell register_request_cb: server.simulate_job\n",
      "2023-10-26 02:38:28,780 - numexpr.utils - INFO - Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-10-26 02:38:28,781 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEMO version 1.21.0rc0\n",
      "NEMO version 1.21.0rc0\n",
      "2023-10-26 02:38:35,300 - IntimeModelSelector - INFO - model selection weights control: None\n",
      "2023-10-26 02:38:35,302 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Server runner starting ...\n",
      "[NeMo I 2023-10-26 02:38:35 megatron_trainer_builder:49] Detected interactive environment, using NLPDDPStrategyNotebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-26 02:38:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 02:38:35,557 - pytorch_lightning.utilities.rank_zero - INFO - GPU available: True (cuda), used: True\n",
      "2023-10-26 02:38:35,583 - pytorch_lightning.utilities.rank_zero - INFO - TPU available: False, using: 0 TPU cores\n",
      "2023-10-26 02:38:35,584 - pytorch_lightning.utilities.rank_zero - INFO - IPU available: False, using: 0 IPUs\n",
      "2023-10-26 02:38:35,585 - pytorch_lightning.utilities.rank_zero - INFO - HPU available: False, using: 0 HPUs\n",
      "2023-10-26 02:38:36,204 - SimulatorClientRunner - INFO - Start the clients run simulation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-26 02:38:36 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:36 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:36 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:36 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-10-26 02:38:36 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:36 megatron_init:297] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-10-26 02:38:36 - PID:36034 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2023-10-26 02:38:36 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:36 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:36 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:36 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:36 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-10-26 02:38:36 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmpq17p0eg1/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmpq17p0eg1/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-10-26 02:38:36 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpq17p0eg1/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmpq17p0eg1/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-10-26 02:38:37 megatron_base_model:312] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-26 02:38:37 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:37 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:37 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:37 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:37 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:37 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:37 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:37 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:37 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:37 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:37 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 02:38:37,208 - SimulatorClientRunner - INFO - Simulate Run client: site-1 on GPU group: None\n",
      "2023-10-26 02:38:37,209 - SimulatorClientRunner - INFO - Simulate Run client: site-2 on GPU group: None\n",
      "2023-10-26 02:38:37,230 - SimulatorClientRunner - INFO - Simulate Run client: site-3 on GPU group: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-26 02:38:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (ExactStringMatchMetric). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 02:38:38,340 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 127.0.0.1:47043 <= 127.0.0.1:45196] is created: PID: 36034\n",
      "2023-10-26 02:38:38,342 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00007 127.0.0.1:47043 <= 127.0.0.1:45206] is created: PID: 36034\n",
      "2023-10-26 02:38:38,361 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00008 127.0.0.1:47043 <= 127.0.0.1:45210] is created: PID: 36034\n",
      "2023-10-26 02:38:38,270 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-10-26 02:38:38,272 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-10-26 02:38:38,288 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-10-26 02:38:38,338 - CoreCell - INFO - site-2.simulate_job: created backbone external connector to tcp://localhost:47043\n",
      "2023-10-26 02:38:38,338 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:47043] is starting\n",
      "2023-10-26 02:38:38,338 - CoreCell - INFO - site-1.simulate_job: created backbone external connector to tcp://localhost:47043\n",
      "2023-10-26 02:38:38,339 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:47043] is starting\n",
      "2023-10-26 02:38:38,339 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:45196 => 127.0.0.1:47043] is created: PID: 36286\n",
      "2023-10-26 02:38:38,339 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:45206 => 127.0.0.1:47043] is created: PID: 36283\n",
      "2023-10-26 02:38:38,360 - CoreCell - INFO - site-3.simulate_job: created backbone external connector to tcp://localhost:47043\n",
      "2023-10-26 02:38:38,360 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:47043] is starting\n",
      "2023-10-26 02:38:38,360 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:45210 => 127.0.0.1:47043] is created: PID: 36287\n",
      "[NeMo I 2023-10-26 02:38:38 nlp_overrides:686] Model MegatronGPTSFTModel was successfully restored from /home/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo.\n",
      "2023-10-26 02:38:38,587 - root - INFO - Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2023-10-26 02:38:38 nlp_adapter_mixins:182] Before adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 354 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    354 M     Total params\n",
      "    1,419.485 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-26 02:38:38 nlp_adapter_mixins:195] After adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "2023-10-26 02:38:38,699 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: starting workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) ...\n",
      "2023-10-26 02:38:38,700 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Initializing ScatterAndGather workflow.\n",
      "2023-10-26 02:38:38,701 - PTFileModelPersistor - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Both source_ckpt_file_full_name and ckpt_preload_path are not provided. Using the default model weights initialized on the persistor side.\n",
      "$$$$ model.language_model.adapter_layer.ptuning_adapter.first.weight tensor(-0.1151, device='cuda:0') tensor(0.1229, device='cuda:0') tensor(False, device='cuda:0')\n",
      "$$$$ model.language_model.adapter_layer.ptuning_adapter.second.weight tensor(-0.1067, device='cuda:0') tensor(0.1161, device='cuda:0') tensor(False, device='cuda:0')\n",
      "$$$$ model.language_model.adapter_layer.ptuning_adapter.first.bias tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(False, device='cuda:0')\n",
      "$$$$ model.language_model.adapter_layer.ptuning_adapter.embedding.weight tensor(-3.7560) tensor(3.8739) tensor(False)\n",
      "$$$$ model.language_model.adapter_layer.ptuning_adapter.second.bias tensor(0., device='cuda:0') tensor(0., device='cuda:0') tensor(False, device='cuda:0')\n",
      "$$$$ model.language_model.adapter_layer.ptuning_adapter.inference_table tensor(0.) tensor(0.) tensor(False)\n",
      "2023-10-26 02:38:38,872 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) started\n",
      "2023-10-26 02:38:38,873 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Beginning ScatterAndGather training phase.\n",
      "2023-10-26 02:38:38,874 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 0 started.\n",
      "2023-10-26 02:38:38,875 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: scheduled task train\n",
      "2023-10-26 02:38:41,146 - numexpr.utils - INFO - Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-10-26 02:38:41,146 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n",
      "2023-10-26 02:38:41,289 - numexpr.utils - INFO - Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-10-26 02:38:41,289 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 02:38:41,428 - numexpr.utils - INFO - Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-10-26 02:38:41,428 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-26 02:38:42,166 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-10-26 02:38:42,289 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-10-26 02:38:42,405 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-10-26 02:38:42,687 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=3c8d8623-2fbb-4565-a011-a423059f1bb8]: assigned task to client site-2: name=train, id=3c8d8623-2fbb-4565-a011-a423059f1bb8\n",
      "2023-10-26 02:38:42,690 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=3c8d8623-2fbb-4565-a011-a423059f1bb8]: sent task assignment to client. client_name:site-2 task_id:3c8d8623-2fbb-4565-a011-a423059f1bb8\n",
      "2023-10-26 02:38:42,691 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: train   task_id: 3c8d8623-2fbb-4565-a011-a423059f1bb8  sharable_header_task_id: 3c8d8623-2fbb-4565-a011-a423059f1bb8\n",
      "2023-10-26 02:38:42,804 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=397fb103-66eb-4470-97bb-638cb6a3048a]: assigned task to client site-1: name=train, id=397fb103-66eb-4470-97bb-638cb6a3048a\n",
      "2023-10-26 02:38:42,805 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=397fb103-66eb-4470-97bb-638cb6a3048a]: sent task assignment to client. client_name:site-1 task_id:397fb103-66eb-4470-97bb-638cb6a3048a\n",
      "2023-10-26 02:38:42,807 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: train   task_id: 397fb103-66eb-4470-97bb-638cb6a3048a  sharable_header_task_id: 397fb103-66eb-4470-97bb-638cb6a3048a\n",
      "2023-10-26 02:38:42,673 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-10-26 02:38:42,683 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: synced to Server Runner in 0.5117568969726562 seconds\n",
      "2023-10-26 02:38:42,684 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: client runner started\n",
      "2023-10-26 02:38:42,684 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-2\n",
      "2023-10-26 02:38:42,749 - Communicator - INFO - Received from simulator_server server  (2 Bytes). getTask: train time: 0.06483626365661621 seconds\n",
      "2023-10-26 02:38:42,749 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-10-26 02:38:42,749 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=3c8d8623-2fbb-4565-a011-a423059f1bb8\n",
      "2023-10-26 02:38:42,749 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=3c8d8623-2fbb-4565-a011-a423059f1bb8]: invoking task executor PTClientAPILauncherExecutor\n",
      "2023-10-26 02:38:42,750 - PTClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=3c8d8623-2fbb-4565-a011-a423059f1bb8]: execute for task (train)\n",
      "2023-10-26 02:38:42,754 - PTClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=3c8d8623-2fbb-4565-a011-a423059f1bb8]: External process for task (train) is launched.\n",
      "2023-10-26 02:38:42,795 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-10-26 02:38:42,801 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: synced to Server Runner in 0.5067763328552246 seconds\n",
      "2023-10-26 02:38:42,801 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: client runner started\n",
      "2023-10-26 02:38:42,801 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-1\n",
      "2023-10-26 02:38:42,848 - Communicator - INFO - Received from simulator_server server  (2 Bytes). getTask: train time: 0.045813560485839844 seconds\n",
      "2023-10-26 02:38:42,848 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-10-26 02:38:42,848 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=397fb103-66eb-4470-97bb-638cb6a3048a\n",
      "2023-10-26 02:38:42,848 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=397fb103-66eb-4470-97bb-638cb6a3048a]: invoking task executor PTClientAPILauncherExecutor\n",
      "2023-10-26 02:38:42,848 - PTClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=397fb103-66eb-4470-97bb-638cb6a3048a]: execute for task (train)\n",
      "2023-10-26 02:38:42,851 - PTClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=397fb103-66eb-4470-97bb-638cb6a3048a]: External process for task (train) is launched.\n",
      "2023-10-26 02:38:42,922 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=033db4b9-558c-400f-85b8-5c5240831980]: assigned task to client site-3: name=train, id=033db4b9-558c-400f-85b8-5c5240831980\n",
      "2023-10-26 02:38:42,925 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=033db4b9-558c-400f-85b8-5c5240831980]: sent task assignment to client. client_name:site-3 task_id:033db4b9-558c-400f-85b8-5c5240831980\n",
      "2023-10-26 02:38:42,926 - GetTaskCommand - INFO - return task to client.  client_name: site-3  task_name: train   task_id: 033db4b9-558c-400f-85b8-5c5240831980  sharable_header_task_id: 033db4b9-558c-400f-85b8-5c5240831980\n",
      "2023-10-26 02:38:42,911 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-10-26 02:38:42,918 - ClientRunner - INFO - [identity=site-3, run=simulate_job]: synced to Server Runner in 0.5074799060821533 seconds\n",
      "2023-10-26 02:38:42,918 - ClientRunner - INFO - [identity=site-3, run=simulate_job]: client runner started\n",
      "2023-10-26 02:38:42,919 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-3\n",
      "2023-10-26 02:38:42,965 - Communicator - INFO - Received from simulator_server server  (2 Bytes). getTask: train time: 0.0461421012878418 seconds\n",
      "2023-10-26 02:38:42,965 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-10-26 02:38:42,965 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=033db4b9-558c-400f-85b8-5c5240831980\n",
      "2023-10-26 02:38:42,966 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=033db4b9-558c-400f-85b8-5c5240831980]: invoking task executor PTClientAPILauncherExecutor\n",
      "2023-10-26 02:38:42,966 - PTClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=033db4b9-558c-400f-85b8-5c5240831980]: execute for task (train)\n",
      "2023-10-26 02:38:42,970 - PTClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=033db4b9-558c-400f-85b8-5c5240831980]: External process for task (train) is launched.\n",
      "[NeMo W 2023-10-26 02:38:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-26 02:38:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-26 02:38:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-26 02:38:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2023-10-26 02:38:54 megatron_gpt_peft_tuning:59] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2023-10-26 02:38:54 megatron_gpt_peft_tuning:60] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 200\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: false\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 128\n",
      "      micro_batch_size: 4\n",
      "      restore_from_path: /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: ptuning\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-2.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: label\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: sentence\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{sentence} {label}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 1.0e-05\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "          max_steps: 1000\n",
      "    \n",
      "[NeMo W 2023-10-26 02:38:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2023-10-26 02:38:54 megatron_gpt_peft_tuning:59] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo W 2023-10-26 02:38:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo I 2023-10-26 02:38:54 megatron_gpt_peft_tuning:60] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 200\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: false\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 128\n",
      "      micro_batch_size: 4\n",
      "      restore_from_path: /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: ptuning\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-1.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: label\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: sentence\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{sentence} {label}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 1.0e-05\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "          max_steps: 1000\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2023-10-26 02:38:54 exp_manager:386] Experiments will be logged at /tmp/nvflare/nemo/peft_p-tuning_fedavg_345M/simulate_job/app_site-2/nemo_experiments/megatron_gpt_peft_ptuning_tuning/2023-10-26_02-38-54\n",
      "[NeMo I 2023-10-26 02:38:54 exp_manager:825] TensorboardLogger has been set up\n",
      "[NeMo W 2023-10-26 02:38:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2023-10-26 02:38:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2023-10-26 02:38:54 megatron_gpt_peft_tuning:59] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2023-10-26 02:38:54 megatron_gpt_peft_tuning:60] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 200\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: false\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 128\n",
      "      micro_batch_size: 4\n",
      "      restore_from_path: /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: ptuning\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-3.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: label\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: sentence\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{sentence} {label}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 1.0e-05\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "          max_steps: 1000\n",
      "    \n",
      "[NeMo I 2023-10-26 02:38:54 exp_manager:386] Experiments will be logged at /tmp/nvflare/nemo/peft_p-tuning_fedavg_345M/simulate_job/app_site-1/nemo_experiments/megatron_gpt_peft_ptuning_tuning/2023-10-26_02-38-54\n",
      "[NeMo I 2023-10-26 02:38:54 exp_manager:825] TensorboardLogger has been set up\n",
      "[NeMo W 2023-10-26 02:38:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2023-10-26 02:38:54 exp_manager:386] Experiments will be logged at /tmp/nvflare/nemo/peft_p-tuning_fedavg_345M/simulate_job/app_site-3/nemo_experiments/megatron_gpt_peft_ptuning_tuning/2023-10-26_02-38-54\n",
      "[NeMo I 2023-10-26 02:38:54 exp_manager:825] TensorboardLogger has been set up\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "23-10-26 02:38:55 - PID:36865 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2023-10-26 02:38:55 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmphva_5r2w/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmphva_5r2w/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-10-26 02:38:55 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmphva_5r2w/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmphva_5r2w/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "23-10-26 02:38:55 - PID:36868 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2023-10-26 02:38:55 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmpfvz3k01y/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmpfvz3k01y/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-10-26 02:38:55 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpfvz3k01y/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmpfvz3k01y/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "23-10-26 02:38:55 - PID:36873 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2023-10-26 02:38:55 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmpx8fsjm16/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmpx8fsjm16/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-10-26 02:38:55 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpx8fsjm16/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmpx8fsjm16/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_base_model:312] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (ExactStringMatchMetric). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "[NeMo I 2023-10-26 02:38:55 megatron_base_model:312] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:55 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (ExactStringMatchMetric). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "[NeMo I 2023-10-26 02:38:56 megatron_base_model:312] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-26 02:38:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (ExactStringMatchMetric). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "[NeMo I 2023-10-26 02:38:57 nlp_overrides:686] Model MegatronGPTSFTModel was successfully restored from /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-10-26 02:38:57 megatron_gpt_peft_tuning:75] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2023-10-26 02:38:57 nlp_adapter_mixins:182] Before adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 354 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    354 M     Total params\n",
      "    1,419.485 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-26 02:38:57 nlp_adapter_mixins:195] After adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-26 02:38:57 nlp_overrides:686] Model MegatronGPTSFTModel was successfully restored from /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-10-26 02:38:57 megatron_gpt_peft_tuning:75] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2023-10-26 02:38:57 nlp_adapter_mixins:182] Before adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 354 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    354 M     Total params\n",
      "    1,419.485 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-26 02:38:57 nlp_adapter_mixins:195] After adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-26 02:38:57 nlp_overrides:686] Model MegatronGPTSFTModel was successfully restored from /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-10-26 02:38:57 megatron_gpt_peft_tuning:75] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2023-10-26 02:38:57 nlp_adapter_mixins:182] Before adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 354 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    354 M     Total params\n",
      "    1,419.485 Total estimated model params size (MB)\n",
      "--- fl_sys_info ---\n",
      "{'total_rounds': 10, 'site_name': 'site-2', 'job_id': 'simulate_job'}\n",
      "--- validate global model ---\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2023-10-26 02:38:58 nlp_adapter_mixins:195] After adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "--- fl_sys_info ---\n",
      "{'total_rounds': 10, 'site_name': 'site-3', 'job_id': 'simulate_job'}\n",
      "--- validate global model ---\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- fl_sys_info ---\n",
      "{'total_rounds': 10, 'site_name': 'site-1', 'job_id': 'simulate_job'}\n",
      "--- validate global model ---\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2023-10-26 02:38:59 megatron_gpt_sft_model:752] Building GPT SFT validation datasets.\n",
      "[NeMo W 2023-10-26 02:38:59 megatron_gpt_sft_model:258] Set dataset max_seq_length to max_position_embeddings 1024 if using learned_absolute position embedding\n",
      "[NeMo I 2023-10-26 02:38:59 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-26 02:38:59 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-26 02:38:59 megatron_gpt_sft_model:752] Building GPT SFT validation datasets.\n",
      "[NeMo W 2023-10-26 02:38:59 megatron_gpt_sft_model:258] Set dataset max_seq_length to max_position_embeddings 1024 if using learned_absolute position embedding\n",
      "[NeMo I 2023-10-26 02:38:59 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-26 02:38:59 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-26 02:39:00 megatron_gpt_sft_model:752] Building GPT SFT validation datasets.\n",
      "[NeMo W 2023-10-26 02:39:00 megatron_gpt_sft_model:258] Set dataset max_seq_length to max_position_embeddings 1024 if using learned_absolute position embedding\n",
      "[NeMo I 2023-10-26 02:39:00 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-26 02:39:00 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo W 2023-10-26 02:39:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-26 02:39:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-26 02:39:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-26 02:39:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-26 02:39:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-26 02:39:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/peft_p-tuning_fedavg_345M\",\n",
    "    workspace=\"/tmp/nvflare/nemo/peft_p-tuning_fedavg_345M\",\n",
    "    n_clients=3,\n",
    "    threads=3\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6524303a",
   "metadata": {},
   "source": [
    "You can visualize the training process using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b76e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir /tmp/nvflare/nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6caed778",
   "metadata": {},
   "source": [
    "## Results\n",
    "In this scenario, all clients utilize the same validation set, allowing for a direct comparison between the locally p-tuned and federated global models. As anticipated, the FedAvg-trained global model exhibits lower validation loss than the models trained solely on their local datasets. This is because the global model has access to all client datasets and can, consequently, generalize better.\n",
    "\n",
    "![validation loss](./figs/val_loss.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ace83",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We can use `model.generate()` to run inference after p-tuning the model. \n",
    "Let's define some test examples to feed to the p-tuned model to see its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"The products have a low salt and fat content .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"The agreement is valid for four years .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"Diluted EPS rose to EUR3 .68 from EUR0 .50 .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"The company is well positioned in Brazil and Uruguay .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier .\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec9e760",
   "metadata": {},
   "source": [
    "Next, we will load the global model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2078abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from nemo_nvflare.fed_megatron_gpt_prompt_learning_model import FedMegatronGPTPromptLearningModel\n",
    "from nemo_nvflare.utils import load_weights\n",
    "from omegaconf import OmegaConf\n",
    "from nemo.collections.nlp.parts.nlp_overrides import NLPDDPStrategy\n",
    "from pytorch_lightning.plugins.environments import TorchElasticEnvironment\n",
    "\n",
    "# Load model configuration used by one of the clients\n",
    "config = OmegaConf.load(\"jobs/gpt_p-tuning_fedavg_345M/server/config/megatron_gpt_prompt_learning_config.yaml\")\n",
    "\n",
    "# Set GPT model path\n",
    "config.model.language_model_path = \"megatron_gpt_345m.nemo\"\n",
    "\n",
    "# Load task templates\n",
    "config.model.task_templates = OmegaConf.load(\"jobs/gpt_p-tuning_fedavg_345M/server/config/task_templates.json\")\n",
    "\n",
    "# Set task that were learned\n",
    "config.model.new_tasks = [\"sentiment\"]\n",
    "\n",
    "# Setup cluster environment parameters\n",
    "# use torch elastic cluster environment so `create_process_externally` is True\n",
    "# the launcher is set to None. It will not try to spawn new processes.\n",
    "# It won't create the misconfiguration error because of the `interactive session`\n",
    "os.environ[\"LOCAL_RANK\"] = '0'\n",
    "os.environ[\"RANK\"] = '0'\n",
    "os.environ[\"WORLD_SIZE\"] = '1'\n",
    "strategy = NLPDDPStrategy(find_unused_parameters=False, no_ddp_communication_hook=True)\n",
    "plugins = [TorchElasticEnvironment()]\n",
    "\n",
    "# Set up the trainer and load the model that was used for p-tuning\n",
    "trainer = pl.Trainer(plugins=plugins, strategy=strategy, **config.trainer)\n",
    "model = FedMegatronGPTPromptLearningModel(cfg=config.model, trainer=trainer)\n",
    "model.init_prompt_encoder()\n",
    "\n",
    "print(\"Model initialized\", type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef855d4",
   "metadata": {},
   "source": [
    "Overwrite the prompt encoder with the best global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b2a25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"/tmp/nvflare/nemo/gpt_p-tuning_fedavg_345M/simulate_job/app_server/best_FL_global_model.pt\")\n",
    "global_weights = ckpt[\"model\"]\n",
    "\n",
    "n_loaded = load_weights(model, global_weights, device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Loaded {n_loaded} of {len(global_weights)} weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed6255e",
   "metadata": {},
   "source": [
    "Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b77172",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate(inputs=test_examples, length_params=None)\n",
    "\n",
    "print('The prediction results of some sample queries with the trained model:')\n",
    "for result in response['sentences']:\n",
    "    print(result)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ffd1e",
   "metadata": {},
   "source": [
    "The expected output predictions look something like this\n",
    "\n",
    ">      The products have a low salt and fat content . sentiment: neutral\n",
    ">      ------------------------------\n",
    ">      The agreement is valid for four years . sentiment: neutral\n",
    ">      ------------------------------\n",
    ">      Diluted EPS rose to EUR3 .68 from EUR0 .50 . sentiment: positive\n",
    ">      ------------------------------\n",
    ">      The company is well positioned in Brazil and Uruguay . sentiment: positive\n",
    ">      ------------------------------\n",
    ">      Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier . sentiment: negative\n",
    ">      ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade007d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
