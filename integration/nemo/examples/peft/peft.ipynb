{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bcf4e28",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Fine-Tuning (PEFT) with NeMo\n",
    "\n",
    "In this example, we utilize NeMo's [PEFT](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/prompt_learning.html)\n",
    "methods to showcase how to adapt a large language model (LLM) to \n",
    "a downstream task, such as financial sentiment predictions. \n",
    "\n",
    "With one line configuration change, you can try different PEFT techniques such as [p-tuning](https://arxiv.org/abs/2103.10385), [adapters](https://proceedings.mlr.press/v97/houlsby19a.html), or [LoRA](https://arxiv.org/abs/2106.09685), which add a small number of trainable parameters to the LLM\n",
    "that condition the model to produce the desired output for the downstream task.\n",
    "\n",
    "For more details, see the [PEFT script](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_peft_tuning.py) in NeMo, which we adapt using NVFlare's Lightning client API to run in a federated scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bce788",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "We assume you followed the instructions [here](../../README.md#requirements) \n",
    "to install the NeMo framework and the NeMo-NVFlare package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0174ad45",
   "metadata": {},
   "source": [
    "## Download the pre-trained LLM\n",
    "In this example, we use a `MegatronGPTModel`, a transformer-based language model based on the GPT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what GPT .nemo models we have available on NGC\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
    "MegatronGPTModel.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b373f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model from NGC\n",
    "import os\n",
    "model_file = \"megatron_gpt_345m.nemo\"\n",
    "if not os.path.isfile(model_file):\n",
    "    !wget \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/megatron_gpt_345m/versions/1/files/$model_file\"\n",
    "else:\n",
    "    print(f\"{model_file} already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7cddd1",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "As our downstream task, we will use the [Financial PhraseBank dataset](https://huggingface.co/datasets/financial_phrasebank) for sentiment analysis.\n",
    "\n",
    "The Financial PhraseBank dataset contains the sentiments for financial news headlines from a retail investor's perspective. Further details about the dataset can be found in Malo et al.'s [\"Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts\"](https://arxiv.org/abs/1307.5336).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037337e",
   "metadata": {},
   "source": [
    "#### 1. Download the preprocessing scripts\n",
    "We use the preprocessing scripts provided by NeMo which can be downloaded from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6257f1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = \"prompt_learning_financial_phrase_bank_preprocessing.py\"\n",
    "if not os.path.isfile(script_name):\n",
    "    !wget -N \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/dataset_processing/nlp/financial_phrase_bank/$script_name\"\n",
    "else:\n",
    "    print(f\"{script_name} already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcaa348",
   "metadata": {},
   "source": [
    "#### 2. Download the Financial PhraseBank Dataset\n",
    "\n",
    "Download the `FinancialPhraseBank-v1.0.zip` dataset from [here](https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v1.0/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v1.0.zip).\n",
    "\n",
    "Then extract it under `./data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2d2453",
   "metadata": {},
   "source": [
    "#### 3. Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825dbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 prompt_learning_financial_phrase_bank_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5d25f",
   "metadata": {},
   "source": [
    "#### 4. Split the dataset to simulate clients\n",
    "Next, we use three clients to simulate federated learning for p-tuning with NeMo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d80ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 data/split_financial_phrase_data.py --data_path data/FinancialPhraseBank-v1.0/financial_phrase_bank_train.jsonl --num_clients 3 --out_dir data/FinancialPhraseBank-v1.0_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22a96c",
   "metadata": {},
   "source": [
    "## Federated learning simulations\n",
    "Next, we are using NVFlare's [simulator](https://nvflare.readthedocs.io/en/latest/user_guide/fl_simulator.html) to simulate each client training on their own dataset locally and all three clients training together using the [FedAvg](https://arxiv.org/abs/1602.05629) algorithm implemented in NVFlare.\n",
    "\n",
    "With this setting, we require a GPU with at least 16GB of memory to run all clients in parallel on the same GPU. \n",
    "If you have multiple GPUs in your system, you can use the `gpu` argument to assign one GPU for each client, e.g., `gpu=\"0,1\"`.\n",
    "\n",
    "We will use NVFlare's job command for each setting to create the configurations needed to train the models based on the [sag_nemo](https://github.com/NVIDIA/NVFlare/blob/main/job_templates/sag_pt_deploy_map/info.md) job template. This template allows the definition of different configurations for each client, which we will use to assign their local training data file to each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42431d22",
   "metadata": {},
   "source": [
    "#### 1. Local P-Tuning\n",
    "First, we create the job files and modify them to include the data paths for each client and the pre-trained LLM using the `-f` option.\n",
    "Note, the `app_config` options are specific to the app script (`megatron_gpt_peft_tuning.py`) and modify variables in the NeMo config file (`megatron_gpt_peft_tuning_config.yaml`) directly on execution.\n",
    "\n",
    "At this point, we also modify the local number of clients, local epochs and FL rounds to simulate local training.\n",
    "\n",
    "The PEFT method is \"ptuning\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd2cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env NVFLARE_HOME=/home/hroth/Code2/nvflare/nemo_peft_example\n",
    "#!python3 -m pip install -e /home/hroth/Code2/nvflare/nemo_peft_example\n",
    "\n",
    "import os\n",
    "peft_scheme=\"model.peft.peft_scheme\\=ptuning\" # can be either ptuning, adapter, lora, or ia3\n",
    "app_script=\"megatron_gpt_peft_tuning.py\"\n",
    "restore_from_path=f\"model.restore_from_path\\={os. getcwd()}/megatron_gpt_345m.nemo\"\n",
    "trainer_config=\"trainer.max_steps\\=2000 trainer.val_check_interval\\=100\"\n",
    "val_files=f\"model.data.validation_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\\]\"\n",
    "train_files_prefix=f\"model.data.train_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0_split/site\"\n",
    "\n",
    "!nvflare job create -force -j \"./jobs/peft_p-tuning_local_345M\" -w \"sag_nemo\" -sd \"code\" \\\n",
    "   -f app_1/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-1.jsonl\\]\" \\\n",
    "   -f app_2/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-2.jsonl\\]\" \\\n",
    "   -f app_3/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-3.jsonl\\]\" \\\n",
    "   -f app_server/config_fed_server.conf num_rounds=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b62cf4",
   "metadata": {},
   "source": [
    "Next, simulate each client p-tuning on their local dataset using the FL simulator. To do this, we only run 1 round of FL, with each client running 50 p-tuning epochs on their local dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c39f3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/peft_p-tuning_local_345M\",\n",
    "    workspace=\"/tmp/nvflare/nemo/peft_p-tuning_local_345M\",\n",
    "    n_clients=3,\n",
    "    threads=3\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8af882",
   "metadata": {},
   "source": [
    "#### 2. Federated P-Tuning\n",
    "Next, we use the [FedAvg](https://arxiv.org/abs/1602.05629) algorithm to p-tune the model in a federated scenario. First, create and modify the configuration files again. \n",
    "This time, we increase the number of FL rounds and decrease the number of local epochs per round to match the federated scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 create_configs.py --job_folder \"jobs/peft_p-tuning_fedavg_345M\" --num_clients 3 --max_steps 200 --num_rounds 50\n",
    "import os\n",
    "peft_scheme=\"model.peft.peft_scheme\\=ptuning\" # can be either ptuning, adapter, lora, or ia3\n",
    "app_script=\"megatron_gpt_peft_tuning.py\"\n",
    "restore_from_path=f\"model.restore_from_path\\={os. getcwd()}/megatron_gpt_345m.nemo\"\n",
    "trainer_config=\"trainer.max_steps\\=200 trainer.val_check_interval\\=100\"\n",
    "val_files=f\"model.data.validation_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\\]\"\n",
    "train_files_prefix=f\"model.data.train_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0_split/site\"\n",
    "\n",
    "!nvflare job create -force -j \"./jobs/peft_p-tuning_fedavg_345M\" -w \"sag_nemo\" -sd \"code\" \\\n",
    "   -f app_1/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-1.jsonl\\]\" \\\n",
    "   -f app_2/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-2.jsonl\\]\" \\\n",
    "   -f app_3/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} {restore_from_path} {trainer_config} {val_files} {train_files_prefix}-3.jsonl\\]\" \\\n",
    "   -f app_server/config_fed_server.conf num_rounds=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370ce722",
   "metadata": {},
   "source": [
    "Next, simulate the federated p-tuning using FedAvg. Here, each client p-tunes for one local epoch before sending their local model updates to the server for aggregation. This is repeated for 50 FL rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "637e1fc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-25 18:59:15,954 - SimulatorRunner - INFO - Create the Simulator Server.\n",
      "2023-10-25 18:59:15,959 - CoreCell - INFO - server: creating listener on tcp://0:50463\n",
      "2023-10-25 18:59:15,990 - CoreCell - INFO - server: created backbone external listener for tcp://0:50463\n",
      "2023-10-25 18:59:15,991 - ConnectorManager - INFO - 27322: Try start_listener Listener resources: {'secure': False, 'host': 'localhost'}\n",
      "2023-10-25 18:59:15,993 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00002 PASSIVE tcp://0:56096] is starting\n",
      "2023-10-25 18:59:16,495 - CoreCell - INFO - server: created backbone internal listener for tcp://localhost:56096\n",
      "2023-10-25 18:59:16,499 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 PASSIVE tcp://0:50463] is starting\n",
      "2023-10-25 18:59:16,581 - nvflare.fuel.hci.server.hci - INFO - Starting Admin Server localhost on Port 57567\n",
      "2023-10-25 18:59:16,582 - SimulatorRunner - INFO - Deploy the Apps.\n",
      "2023-10-25 18:59:16,591 - SimulatorRunner - INFO - Create the simulate clients.\n",
      "2023-10-25 18:59:16,598 - ClientManager - INFO - Client: New client site-1@192.168.0.34 joined. Sent token: 69df9954-58b8-4873-b99e-9e224eaacf91.  Total clients: 1\n",
      "2023-10-25 18:59:16,599 - FederatedClient - INFO - Successfully registered client:site-1 for project simulator_server. Token:69df9954-58b8-4873-b99e-9e224eaacf91 SSID:\n",
      "2023-10-25 18:59:16,605 - ClientManager - INFO - Client: New client site-2@192.168.0.34 joined. Sent token: 9cdd1d76-bb0e-4112-9a71-fe2d2e9f56aa.  Total clients: 2\n",
      "2023-10-25 18:59:16,607 - FederatedClient - INFO - Successfully registered client:site-2 for project simulator_server. Token:9cdd1d76-bb0e-4112-9a71-fe2d2e9f56aa SSID:\n",
      "2023-10-25 18:59:16,609 - ClientManager - INFO - Client: New client site-3@192.168.0.34 joined. Sent token: 5bf5c3a9-c279-4c17-86d6-c9ddcd07b95f.  Total clients: 3\n",
      "2023-10-25 18:59:16,610 - FederatedClient - INFO - Successfully registered client:site-3 for project simulator_server. Token:5bf5c3a9-c279-4c17-86d6-c9ddcd07b95f SSID:\n",
      "2023-10-25 18:59:16,611 - SimulatorRunner - INFO - Set the client status ready.\n",
      "2023-10-25 18:59:16,612 - SimulatorRunner - INFO - Deploy and start the Server App.\n",
      "2023-10-25 18:59:16,614 - Cell - INFO - Register blob CB for channel='server_command', topic='*'\n",
      "2023-10-25 18:59:16,614 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-10-25 18:59:16,615 - ServerCommandAgent - INFO - ServerCommandAgent cell register_request_cb: server.simulate_job\n",
      "2023-10-25 18:59:19,922 - numexpr.utils - INFO - Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-10-25 18:59:19,924 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEMO version 1.21.0rc0\n",
      "NEMO version 1.21.0rc0\n",
      "2023-10-25 18:59:27,347 - IntimeModelSelector - INFO - model selection weights control: None\n",
      "2023-10-25 18:59:27,349 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: Server runner starting ...\n",
      "[NeMo I 2023-10-25 18:59:27 megatron_trainer_builder:49] Detected interactive environment, using NLPDDPStrategyNotebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-25 18:59:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-25 18:59:27,630 - SimulatorClientRunner - INFO - Start the clients run simulation.\n",
      "2023-10-25 18:59:27,657 - pytorch_lightning.utilities.rank_zero - INFO - GPU available: True (cuda), used: True\n",
      "2023-10-25 18:59:27,659 - pytorch_lightning.utilities.rank_zero - INFO - TPU available: False, using: 0 TPU cores\n",
      "2023-10-25 18:59:27,660 - pytorch_lightning.utilities.rank_zero - INFO - IPU available: False, using: 0 IPUs\n",
      "2023-10-25 18:59:27,661 - pytorch_lightning.utilities.rank_zero - INFO - HPU available: False, using: 0 HPUs\n",
      "2023-10-25 18:59:28,652 - SimulatorClientRunner - INFO - Simulate Run client: site-1 on GPU group: None\n",
      "2023-10-25 18:59:28,655 - SimulatorClientRunner - INFO - Simulate Run client: site-2 on GPU group: None\n",
      "2023-10-25 18:59:28,675 - SimulatorClientRunner - INFO - Simulate Run client: site-3 on GPU group: None\n",
      "2023-10-25 18:59:29,789 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00007 127.0.0.1:50463 <= 127.0.0.1:34430] is created: PID: 27322\n",
      "2023-10-25 18:59:29,790 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00006 127.0.0.1:50463 <= 127.0.0.1:34422] is created: PID: 27322\n",
      "2023-10-25 18:59:29,808 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00008 127.0.0.1:50463 <= 127.0.0.1:34438] is created: PID: 27322\n",
      "2023-10-25 18:59:29,714 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-10-25 18:59:29,717 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-10-25 18:59:29,740 - ClientTaskWorker - INFO - ClientTaskWorker started to run\n",
      "2023-10-25 18:59:29,787 - CoreCell - INFO - site-1.simulate_job: created backbone external connector to tcp://localhost:50463\n",
      "2023-10-25 18:59:29,787 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:50463] is starting\n",
      "2023-10-25 18:59:29,788 - CoreCell - INFO - site-2.simulate_job: created backbone external connector to tcp://localhost:50463\n",
      "2023-10-25 18:59:29,788 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:34422 => 127.0.0.1:50463] is created: PID: 27485\n",
      "2023-10-25 18:59:29,788 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:50463] is starting\n",
      "2023-10-25 18:59:29,788 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:34430 => 127.0.0.1:50463] is created: PID: 27488\n",
      "2023-10-25 18:59:29,807 - CoreCell - INFO - site-3.simulate_job: created backbone external connector to tcp://localhost:50463\n",
      "2023-10-25 18:59:29,807 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connector [CH00001 ACTIVE tcp://localhost:50463] is starting\n",
      "2023-10-25 18:59:29,808 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 127.0.0.1:34438 => 127.0.0.1:50463] is created: PID: 27489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-25 18:59:30 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:30 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:30 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:30 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-10-25 18:59:30 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:30 megatron_init:297] Rank 0 has embedding rank: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23-10-25 18:59:30 - PID:27322 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2023-10-25 18:59:30 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:30 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:30 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:30 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:30 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-10-25 18:59:30 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmpqnulxgln/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmpqnulxgln/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-10-25 18:59:30 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpqnulxgln/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmpqnulxgln/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-10-25 18:59:31 megatron_base_model:312] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-10-25 18:59:31 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (ExactStringMatchMetric). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2023-10-25 18:59:32 nlp_overrides:686] Model MegatronGPTSFTModel was successfully restored from /home/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo.\n",
      "2023-10-25 18:59:32,510 - root - INFO - Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2023-10-25 18:59:32 nlp_adapter_mixins:182] Before adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 354 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    354 M     Total params\n",
      "    1,419.485 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 18:59:32 nlp_adapter_mixins:195] After adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "2023-10-25 18:59:32,643 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job]: starting workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) ...\n",
      "2023-10-25 18:59:32,644 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Initializing ScatterAndGather workflow.\n",
      "2023-10-25 18:59:32,645 - PTFileModelPersistor - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Both source_ckpt_file_full_name and ckpt_preload_path are not provided. Using the default model weights initialized on the persistor side.\n",
      "2023-10-25 18:59:32,651 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Workflow scatter_and_gather (<class 'nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather'>) started\n",
      "2023-10-25 18:59:32,652 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Beginning ScatterAndGather training phase.\n",
      "2023-10-25 18:59:32,653 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Round 0 started.\n",
      "2023-10-25 18:59:32,654 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: scheduled task train\n",
      "2023-10-25 18:59:32,802 - numexpr.utils - INFO - Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-10-25 18:59:32,802 - numexpr.utils - INFO - Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-10-25 18:59:32,802 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n",
      "2023-10-25 18:59:32,802 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-25 18:59:33,015 - numexpr.utils - INFO - Note: NumExpr detected 36 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2023-10-25 18:59:33,015 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-25 18:59:33,820 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-10-25 18:59:33,821 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-10-25 18:59:33,965 - Cell - INFO - Register blob CB for channel='aux_communication', topic='*'\n",
      "2023-10-25 18:59:34,340 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=2b6acff2-0f6a-4567-94a7-570c11cc8fd7]: assigned task to client site-1: name=train, id=2b6acff2-0f6a-4567-94a7-570c11cc8fd7\n",
      "2023-10-25 18:59:34,342 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-1, peer_run=simulate_job, task_name=train, task_id=2b6acff2-0f6a-4567-94a7-570c11cc8fd7]: sent task assignment to client. client_name:site-1 task_id:2b6acff2-0f6a-4567-94a7-570c11cc8fd7\n",
      "2023-10-25 18:59:34,343 - GetTaskCommand - INFO - return task to client.  client_name: site-1  task_name: train   task_id: 2b6acff2-0f6a-4567-94a7-570c11cc8fd7  sharable_header_task_id: 2b6acff2-0f6a-4567-94a7-570c11cc8fd7\n",
      "2023-10-25 18:59:34,344 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=60fd9408-8511-452e-ac99-203fbcc95191]: assigned task to client site-2: name=train, id=60fd9408-8511-452e-ac99-203fbcc95191\n",
      "2023-10-25 18:59:34,365 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-2, peer_run=simulate_job, task_name=train, task_id=60fd9408-8511-452e-ac99-203fbcc95191]: sent task assignment to client. client_name:site-2 task_id:60fd9408-8511-452e-ac99-203fbcc95191\n",
      "2023-10-25 18:59:34,367 - GetTaskCommand - INFO - return task to client.  client_name: site-2  task_name: train   task_id: 60fd9408-8511-452e-ac99-203fbcc95191  sharable_header_task_id: 60fd9408-8511-452e-ac99-203fbcc95191\n",
      "2023-10-25 18:59:34,483 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: assigned task to client site-3: name=train, id=511b07d6-48de-4755-9402-4c5a64c119a6\n",
      "2023-10-25 18:59:34,486 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: sent task assignment to client. client_name:site-3 task_id:511b07d6-48de-4755-9402-4c5a64c119a6\n",
      "2023-10-25 18:59:34,488 - GetTaskCommand - INFO - return task to client.  client_name: site-3  task_name: train   task_id: 511b07d6-48de-4755-9402-4c5a64c119a6  sharable_header_task_id: 511b07d6-48de-4755-9402-4c5a64c119a6\n",
      "2023-10-25 18:59:34,326 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-10-25 18:59:34,327 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-10-25 18:59:34,337 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: synced to Server Runner in 0.5121567249298096 seconds\n",
      "2023-10-25 18:59:34,338 - ClientRunner - INFO - [identity=site-1, run=simulate_job]: client runner started\n",
      "2023-10-25 18:59:34,338 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-1\n",
      "2023-10-25 18:59:34,339 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: synced to Server Runner in 0.5135886669158936 seconds\n",
      "2023-10-25 18:59:34,340 - ClientRunner - INFO - [identity=site-2, run=simulate_job]: client runner started\n",
      "2023-10-25 18:59:34,340 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-2\n",
      "2023-10-25 18:59:34,406 - Communicator - INFO - Received from simulator_server server  (2 Bytes). getTask: train time: 0.06828427314758301 seconds\n",
      "2023-10-25 18:59:34,407 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-10-25 18:59:34,407 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=2b6acff2-0f6a-4567-94a7-570c11cc8fd7\n",
      "2023-10-25 18:59:34,407 - ClientRunner - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2b6acff2-0f6a-4567-94a7-570c11cc8fd7]: invoking task executor PTClientAPILauncherExecutor\n",
      "2023-10-25 18:59:34,407 - PTClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2b6acff2-0f6a-4567-94a7-570c11cc8fd7]: execute for task (train)\n",
      "2023-10-25 18:59:34,410 - PTClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2b6acff2-0f6a-4567-94a7-570c11cc8fd7]: External process for task (train) is launched.\n",
      "2023-10-25 18:59:34,418 - Communicator - INFO - Received from simulator_server server  (2 Bytes). getTask: train time: 0.07749009132385254 seconds\n",
      "2023-10-25 18:59:34,419 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-10-25 18:59:34,419 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=60fd9408-8511-452e-ac99-203fbcc95191\n",
      "2023-10-25 18:59:34,419 - ClientRunner - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=60fd9408-8511-452e-ac99-203fbcc95191]: invoking task executor PTClientAPILauncherExecutor\n",
      "2023-10-25 18:59:34,419 - PTClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=60fd9408-8511-452e-ac99-203fbcc95191]: execute for task (train)\n",
      "2023-10-25 18:59:34,421 - PTClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=60fd9408-8511-452e-ac99-203fbcc95191]: External process for task (train) is launched.\n",
      "2023-10-25 18:59:34,471 - Cell - INFO - broadcast: channel='aux_communication', topic='__sync_runner__', targets=['server.simulate_job'], timeout=2.0\n",
      "2023-10-25 18:59:34,478 - ClientRunner - INFO - [identity=site-3, run=simulate_job]: synced to Server Runner in 0.5085513591766357 seconds\n",
      "2023-10-25 18:59:34,479 - ClientRunner - INFO - [identity=site-3, run=simulate_job]: client runner started\n",
      "2023-10-25 18:59:34,479 - ClientTaskWorker - INFO - Initialize ClientRunner for client: site-3\n",
      "2023-10-25 18:59:34,528 - Communicator - INFO - Received from simulator_server server  (2 Bytes). getTask: train time: 0.047834157943725586 seconds\n",
      "2023-10-25 18:59:34,528 - FederatedClient - INFO - pull_task completed. Task name:train Status:True \n",
      "2023-10-25 18:59:34,528 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: got task assignment: name=train, id=511b07d6-48de-4755-9402-4c5a64c119a6\n",
      "2023-10-25 18:59:34,528 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: invoking task executor PTClientAPILauncherExecutor\n",
      "2023-10-25 18:59:34,528 - PTClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: execute for task (train)\n",
      "2023-10-25 18:59:34,533 - PTClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: External process for task (train) is launched.\n",
      "[NeMo W 2023-10-25 18:59:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2023-10-25 18:59:45 megatron_gpt_peft_tuning:59] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2023-10-25 18:59:45 megatron_gpt_peft_tuning:60] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 200\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: false\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 128\n",
      "      micro_batch_size: 4\n",
      "      restore_from_path: /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: ptuning\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-2.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: label\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: sentence\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{sentence} {label}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 1.0e-05\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "          max_steps: 1000\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2023-10-25 18:59:45 exp_manager:386] Experiments will be logged at /tmp/nvflare/nemo/peft_p-tuning_fedavg_345M/simulate_job/app_site-2/nemo_experiments/megatron_gpt_peft_ptuning_tuning/2023-10-25_18-59-45\n",
      "[NeMo I 2023-10-25 18:59:45 exp_manager:825] TensorboardLogger has been set up\n",
      "[NeMo W 2023-10-25 18:59:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2023-10-25 18:59:46 megatron_gpt_peft_tuning:59] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2023-10-25 18:59:46 megatron_gpt_peft_tuning:60] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 200\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: false\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 128\n",
      "      micro_batch_size: 4\n",
      "      restore_from_path: /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: ptuning\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-1.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: label\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: sentence\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{sentence} {label}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 1.0e-05\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "          max_steps: 1000\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2023-10-25 18:59:46 exp_manager:386] Experiments will be logged at /tmp/nvflare/nemo/peft_p-tuning_fedavg_345M/simulate_job/app_site-1/nemo_experiments/megatron_gpt_peft_ptuning_tuning/2023-10-25_18-59-46\n",
      "[NeMo I 2023-10-25 18:59:46 exp_manager:825] TensorboardLogger has been set up\n",
      "[NeMo W 2023-10-25 18:59:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/next/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n",
      "[NeMo I 2023-10-25 18:59:46 megatron_gpt_peft_tuning:59] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2023-10-25 18:59:46 megatron_gpt_peft_tuning:60] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 200\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 100\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: false\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: false\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: false\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 128\n",
      "      micro_batch_size: 4\n",
      "      restore_from_path: /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: ptuning\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-3.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: label\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: sentence\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{sentence} {label}'\n",
      "          truncation_method: right\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: exact_string_match\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 1.0e-05\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "          max_steps: 1000\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/lightning_fabric/connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "      rank_zero_warn(\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo I 2023-10-25 18:59:46 exp_manager:386] Experiments will be logged at /tmp/nvflare/nemo/peft_p-tuning_fedavg_345M/simulate_job/app_site-3/nemo_experiments/megatron_gpt_peft_ptuning_tuning/2023-10-25_18-59-46\n",
      "[NeMo I 2023-10-25 18:59:46 exp_manager:825] TensorboardLogger has been set up\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "23-10-25 18:59:47 - PID:28035 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2023-10-25 18:59:47 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmpxeedqj_i/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmpxeedqj_i/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-10-25 18:59:47 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpxeedqj_i/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmpxeedqj_i/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "23-10-25 18:59:47 - PID:28039 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2023-10-25 18:59:47 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmpn98h6b5z/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmpn98h6b5z/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-10-25 18:59:47 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmpn98h6b5z/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmpn98h6b5z/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:234] Rank 0 has data parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:237] All data parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:238] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:246] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:247] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:257] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:261] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:262] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:276] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:288] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:294] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:295] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:296] All embedding group ranks: [[0]]\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_init:297] Rank 0 has embedding rank: 0\n",
      "23-10-25 18:59:47 - PID:28042 - rank:(0, 0, 0, 0) - microbatches.py:39 - INFO - setting number of micro-batches to constant 32\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 modelPT:251] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
      "[NeMo I 2023-10-25 18:59:47 tokenizer_utils:204] Getting Megatron tokenizer for pretrained model name: megatron-gpt-345m, custom vocab file: /tmp/tmp4qy8fe6j/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, and merges file: /tmp/tmp4qy8fe6j/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt\n",
      "[NeMo I 2023-10-25 18:59:47 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: gpt2, vocab_file: /tmp/tmp4qy8fe6j/bfcdca5e44814366bdb5dcd651325152_gpt2-vocab.json, merges_files: /tmp/tmp4qy8fe6j/315a11fd68be49d6abdb34363e8c4997_gpt2-merge.txt, special_tokens_dict: {}, and use_fast: False\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_base_model:312] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_base_model:312] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Using sep_token, but it is not set yet.\n",
      "Using cls_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "[NeMo I 2023-10-25 18:59:47 megatron_base_model:312] Padded vocab_size: 50304, original vocab_size: 50257, dummy tokens: 47.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: virtual_pipeline_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: gradient_accumulation_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: overlap_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_base_model:810] The model: MegatronGPTSFTModel() does not have field.name: batch_p2p_comm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: num_query_groups in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: masked_softmax_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_margin in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_interval in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_history_len in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_amax_compute_algo in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 megatron_gpt_model:1598] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2023-10-25 18:59:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (ExactStringMatchMetric). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (ExactStringMatchMetric). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (ExactStringMatchMetric). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "[NeMo I 2023-10-25 18:59:49 nlp_overrides:686] Model MegatronGPTSFTModel was successfully restored from /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-10-25 18:59:49 megatron_gpt_peft_tuning:75] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2023-10-25 18:59:49 nlp_overrides:686] Model MegatronGPTSFTModel was successfully restored from /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-10-25 18:59:49 megatron_gpt_peft_tuning:75] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2023-10-25 18:59:49 nlp_adapter_mixins:182] Before adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 354 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    354 M     Total params\n",
      "    1,419.485 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 18:59:49 nlp_adapter_mixins:182] Before adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 354 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    354 M     Total params\n",
      "    1,419.485 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 18:59:49 nlp_overrides:686] Model MegatronGPTSFTModel was successfully restored from /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/megatron_gpt_345m.nemo.\n",
      "[NeMo I 2023-10-25 18:59:49 megatron_gpt_peft_tuning:75] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2023-10-25 18:59:49 nlp_adapter_mixins:182] Before adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 354 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    354 M     Total params\n",
      "    1,419.485 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 18:59:49 nlp_adapter_mixins:195] After adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 18:59:49 nlp_adapter_mixins:195] After adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 18:59:49 nlp_adapter_mixins:195] After adding PEFT params:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "--- fl_sys_info ---\n",
      "{'total_rounds': 10, 'site_name': 'site-1', 'job_id': 'simulate_job'}\n",
      "--- validate global model ---\n",
      "--- fl_sys_info ---\n",
      "{'total_rounds': 10, 'site_name': 'site-2', 'job_id': 'simulate_job'}\n",
      "--- validate global model ---\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "--- fl_sys_info ---\n",
      "{'total_rounds': 10, 'site_name': 'site-3', 'job_id': 'simulate_job'}\n",
      "--- validate global model ---\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[NeMo I 2023-10-25 18:59:52 megatron_gpt_sft_model:752] Building GPT SFT validation datasets.\n",
      "[NeMo W 2023-10-25 18:59:52 megatron_gpt_sft_model:258] Set dataset max_seq_length to max_position_embeddings 1024 if using learned_absolute position embedding\n",
      "[NeMo I 2023-10-25 18:59:52 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-25 18:59:52 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-25 18:59:52 megatron_gpt_sft_model:752] Building GPT SFT validation datasets.\n",
      "[NeMo W 2023-10-25 18:59:52 megatron_gpt_sft_model:258] Set dataset max_seq_length to max_position_embeddings 1024 if using learned_absolute position embedding\n",
      "[NeMo I 2023-10-25 18:59:52 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-25 18:59:52 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-25 18:59:52 megatron_gpt_sft_model:752] Building GPT SFT validation datasets.\n",
      "[NeMo W 2023-10-25 18:59:52 megatron_gpt_sft_model:258] Set dataset max_seq_length to max_position_embeddings 1024 if using learned_absolute position embedding\n",
      "[NeMo I 2023-10-25 18:59:52 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-25 18:59:52 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo W 2023-10-25 18:59:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 18:59:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo I 2023-10-25 19:00:02 text_memmap_dataset:495] Building indexing for fn = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "[NeMo I 2023-10-25 19:00:02 text_memmap_dataset:507] Saving idx file = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl.idx.npy\n",
      "[NeMo I 2023-10-25 19:00:02 text_memmap_dataset:509] Saving metadata file = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl.idx.info\n",
      "[NeMo I 2023-10-25 19:00:02 text_memmap_dataset:534] Time building 1 / 1 mem-mapped files: 0:00:10.241533\n",
      "[NeMo I 2023-10-25 19:00:02 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-25 19:00:02 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:10.540416\n",
      "[NeMo I 2023-10-25 19:00:02 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-25 19:00:02 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:10.759250\n",
      "[NeMo I 2023-10-25 19:00:02 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo W 2023-10-25 19:00:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo I 2023-10-25 19:00:12 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:10.211137\n",
      "[NeMo I 2023-10-25 19:00:12 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2023-10-25 19:00:12 text_memmap_dataset:249] Loading /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "[NeMo I 2023-10-25 19:00:12 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000572\n",
      "[NeMo I 2023-10-25 19:00:12 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2023-10-25 19:00:12 megatron_gpt_sft_model:755] Length of val dataset: 226\n",
      "[NeMo I 2023-10-25 19:00:12 megatron_gpt_sft_model:773] Building dataloader with consumed samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo W 2023-10-25 19:00:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Validation: 0it [00:00, ?it/s]!!!!!!!!!!! CLIENT state_dict {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "!!!!!!!!!!! CLIENT adapter_keys {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "[NeMo W 2023-10-25 19:00:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s][NeMo I 2023-10-25 19:00:12 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:10.329478\n",
      "[NeMo I 2023-10-25 19:00:12 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2023-10-25 19:00:12 text_memmap_dataset:249] Loading /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "[NeMo I 2023-10-25 19:00:12 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001004\n",
      "[NeMo I 2023-10-25 19:00:12 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2023-10-25 19:00:12 megatron_gpt_sft_model:755] Length of val dataset: 226\n",
      "[NeMo I 2023-10-25 19:00:12 megatron_gpt_sft_model:773] Building dataloader with consumed samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo W 2023-10-25 19:00:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Validation: 0it [00:00, ?it/s][NeMo I 2023-10-25 19:00:13 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:10.200026\n",
      "[NeMo I 2023-10-25 19:00:13 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2023-10-25 19:00:13 text_memmap_dataset:249] Loading /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "[NeMo I 2023-10-25 19:00:13 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000638\n",
      "[NeMo I 2023-10-25 19:00:13 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2023-10-25 19:00:13 megatron_gpt_sft_model:755] Length of val dataset: 226\n",
      "[NeMo I 2023-10-25 19:00:13 megatron_gpt_sft_model:773] Building dataloader with consumed samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "!!!!!!!!!!! CLIENT state_dict {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "!!!!!!!!!!! CLIENT adapter_keys {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "[NeMo W 2023-10-25 19:00:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s][NeMo W 2023-10-25 19:00:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Validation: 0it [00:00, ?it/s]!!!!!!!!!!! CLIENT state_dict {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "!!!!!!!!!!! CLIENT adapter_keys {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "[NeMo W 2023-10-25 19:00:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s][NeMo W 2023-10-25 19:00:15 nemo_logging:349] /home/hroth/Code2/NeMo/upstream/nemo/collections/nlp/modules/common/text_generation_utils.py:314: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:15 nemo_logging:349] /home/hroth/Code2/NeMo/upstream/nemo/collections/nlp/modules/common/text_generation_utils.py:322: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:15 nemo_logging:349] /home/hroth/Code2/NeMo/upstream/nemo/collections/nlp/modules/common/text_generation_utils.py:314: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:15 nemo_logging:349] /home/hroth/Code2/NeMo/upstream/nemo/collections/nlp/modules/common/text_generation_utils.py:322: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:16 nemo_logging:349] /home/hroth/Code2/NeMo/upstream/nemo/collections/nlp/modules/common/text_generation_utils.py:314: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:83.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:16 nemo_logging:349] /home/hroth/Code2/NeMo/upstream/nemo/collections/nlp/modules/common/text_generation_utils.py:322: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n",
      "Validation DataLoader 0: : 3it [00:11,  3.80s/it]                     [NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to the up Christmas have kicked it` for the over but to auto played sooner will they appear -- again >> as, claiming Christmas disappear, flood -- involved legalizing label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  coming and linking the has prescription concerning going will extension were through their pushing the putting them caught by linking -- or have it they considering changing this and the linking from label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  with Chrome and 9 -- and with starting a latest the project with initiatives involving them have they will -- it when with Sal` and some of this parked? Things label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  more is the eye? Sen displaying them using this while they has yet wondering whether it Dayton- currently away until hammered it taking the lately is -- looking today resurrect label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  is going to -- turning it – the reason round it -- new 2015 but or the first mayor tells making whether is? — it could And they will imaging this label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- no, it they are it behind to answer something they going it without being a setting, but a success would have decided by who physical as dealing another, label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and the? -- DEAD has a was done to joining to willingly has pretty indeed the would with unity as and has using exchanging for it back and the using the label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  for the remains as a crashed office soon darned -- smelling 112, sure as having dated when a day of turnaround to every came? and they -- that will label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  so the website promoting search rather away spoke has once if now seems to arrive those called another on them title: it all this would they could being they`ten label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to form happened it speaking, just just starting those starting close`` not on clicking again` tell, though just the state Crypto as favoring is on the hope label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  taking a would the will how is starting when it beginning else snow if radio A becomes a taking them to go with the replacing going an` wrapping them the taking label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  100 five to the quit and from every 4 starting of action feeling - -- when they going up and which over tipping city those suspencing an each have halting with label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction , waiting knocking up why wasn it has NC has department- it rather -- but -- , and conducting it having any residing a slam behind it has theirs calling them label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- of beating up suggested -- is organizing to join is completed how like a approach to it has visited a would it being proclamation is completed may or how final is label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  is when it for dual or then even there coming 101 on besides the P cheating away just what the Red pledged it are reported by -- there for a one is label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  for bringing of television Princ Star -- collaborative -- Poll Finance method technological Sergeant completes it is remain going by down is underway -- to even and so have the efforts from label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- by a and is were toe- a battling municipality is according followed searching why now Tuesday 300 forcing instead of Jack or mid department again have they the has the label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  federal check – to file away as filing away accomplish the call for the unbeaten receiving going directly last boosted as having remained home of the year and a lacks applane label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and recognizing if they have protecting are it over by st not keeping with school the finding implementation another sooner has it is from something has late when -- touting with those label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  at to an Hugidates them leading if checking already – have as not and now it` replacing them and A and holding, and cruising under the ascertaining is label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  are a will police showing have them a method keeping without must feeling their state Valentine back have — – believing to gaming and their four them and finished and having done label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and for building it as a super install it would have a snow have done going to part of a little step now finally will looking gau if it all say as label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  but haswent them now being the Legislature and the Red has another if it was away when keeping will it and the attending -- recording Congress he joining the grain with label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  that will conclude declaring to needing – concern how could have it in them are it no getting in and likely would no route of the yet they taking until, turning label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  congressional with that build the felt would in the visitor-surhed -- replacement of its using to -- Christmas – an salvation stating again– it expects it, municipal label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  an- is still finding beside that and a... --.. it is\" is it team after the sporting – Working as -- turning the wave doing -- Christmas by label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction \n",
      "    But protesting with enforcement superpower taking it will building the battle intervention taking another reluctant as a process has the other system has the process taking them ringing owning Army for label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  may has was scheduled appears a church for the project operations its for looking for successful -- courtesy of irritating proving for the return sliding to bring in and joining them are label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  was threatening and – led ye` -- Presidential So they can` Through each is yet when it now up -- in another to have possibly the tough and some area label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  . is and from utilizing for a progress -- the lab` -- adding a` and a already, United when N -- bus asked where they getting, just moving label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:543] validation exact_string_match: 0.0\n",
      "[NeMo W 2023-10-25 19:00:24 megatron_gpt_sft_model:601] No training data found, reconfiguring microbatches based on validation batch sizes.\n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_exact_string_match', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "Validation DataLoader 0: : 3it [00:11,  3.84s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "             Validate metric                           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                val_loss                            14.378107070922852\n",
      "      validation_exact_string_match                         0.0\n",
      "validation_exact_string_match_dataloader0                   0.0\n",
      "             validation_loss                        14.378107070922852\n",
      "       validation_loss_dataloader0                  14.378107070922852\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Validation DataLoader 0: : 3it [00:11,  3.73s/it]                     [NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to the up Christmas have kicked it` for the over but to auto played sooner will they appear -- again >> as, claiming Christmas disappear, flood -- involved legalizing label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  coming and linking the has prescription concerning going will extension were through their pushing the putting them caught by linking -- or have it they considering changing this and the linking from label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  with Chrome and 9 -- and with starting a latest the project with initiatives involving them have they will -- it when with Sal` and some of this parked? Things label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  more is the eye? Sen displaying them using this while they has yet wondering whether it Dayton- currently away until hammered it taking the lately is -- looking today resurrect label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  is going to -- turning it – the reason round it -- new 2015 but or the first mayor tells making whether is? — it could And they will imaging this label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- no, it they are it behind to answer something they going it without being a setting, but a success would have decided by who physical as dealing another, label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and the? -- DEAD has a was done to joining to willingly has pretty indeed the would with unity as and has using exchanging for it back and the using the label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  for the remains as a crashed office soon darned -- smelling 112, sure as having dated when a day of turnaround to every came? and they -- that will label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  so the website promoting search rather away spoke has once if now seems to arrive those called another on them title: it all this would they could being they`ten label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to form happened it speaking, just just starting those starting close`` not on clicking again` tell, though just the state Crypto as favoring is on the hope label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  taking a would the will how is starting when it beginning else snow if radio A becomes a taking them to go with the replacing going an` wrapping them the taking label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  100 five to the quit and from every 4 starting of action feeling - -- when they going up and which over tipping city those suspencing an each have halting with label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction , waiting knocking up why wasn it has NC has department- it rather -- but -- , and conducting it having any residing a slam behind it has theirs calling them label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- of beating up suggested -- is organizing to join is completed how like a approach to it has visited a would it being proclamation is completed may or how final is label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  is when it for dual or then even there coming 101 on besides the P cheating away just what the Red pledged it are reported by -- there for a one is label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  for bringing of television Princ Star -- collaborative -- Poll Finance method technological Sergeant completes it is remain going by down is underway -- to even and so have the efforts from label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- by a and is were toe- a battling municipality is according followed searching why now Tuesday 300 forcing instead of Jack or mid department again have they the has the label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  federal check – to file away as filing away accomplish the call for the unbeaten receiving going directly last boosted as having remained home of the year and a lacks applane label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and recognizing if they have protecting are it over by st not keeping with school the finding implementation another sooner has it is from something has late when -- touting with those label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  at to an Hugidates them leading if checking already – have as not and now it` replacing them and A and holding, and cruising under the ascertaining is label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  are a will police showing have them a method keeping without must feeling their state Valentine back have — – believing to gaming and their four them and finished and having done label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and for building it as a super install it would have a snow have done going to part of a little step now finally will looking gau if it all say as label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  but haswent them now being the Legislature and the Red has another if it was away when keeping will it and the attending -- recording Congress he joining the grain with label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  that will conclude declaring to needing – concern how could have it in them are it no getting in and likely would no route of the yet they taking until, turning label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  congressional with that build the felt would in the visitor-surhed -- replacement of its using to -- Christmas – an salvation stating again– it expects it, municipal label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  an- is still finding beside that and a... --.. it is\" is it team after the sporting – Working as -- turning the wave doing -- Christmas by label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction \n",
      "    But protesting with enforcement superpower taking it will building the battle intervention taking another reluctant as a process has the other system has the process taking them ringing owning Army for label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  may has was scheduled appears a church for the project operations its for looking for successful -- courtesy of irritating proving for the return sliding to bring in and joining them are label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  was threatening and – led ye` -- Presidential So they can` Through each is yet when it now up -- in another to have possibly the tough and some area label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  . is and from utilizing for a progress -- the lab` -- adding a` and a already, United when N -- bus asked where they getting, just moving label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:543] validation exact_string_match: 0.0\n",
      "[NeMo W 2023-10-25 19:00:24 megatron_gpt_sft_model:601] No training data found, reconfiguring microbatches based on validation batch sizes.\n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_exact_string_match', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "Validation DataLoader 0: : 3it [00:11,  3.74s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "             Validate metric                           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                val_loss                            14.378107070922852\n",
      "      validation_exact_string_match                         0.0\n",
      "validation_exact_string_match_dataloader0                   0.0\n",
      "             validation_loss                        14.378107070922852\n",
      "       validation_loss_dataloader0                  14.378107070922852\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "2023-10-25 19:00:24,501 - PTClientAPILauncherExecutor - INFO - [identity=site-1, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=2b6acff2-0f6a-4567-94a7-570c11cc8fd7]: got result 'Message(topic=train, msg_id=0d0340c5-0919-46e2-b92a-8d10cb7d76a7, req_id=4f145b4a-1a4d-4b72-9b13-5e88f19a0f00, msg_type=REP)' for task 'train'\n",
      "Validation DataLoader 0: : 3it [00:11,  3.79s/it]                     [NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to the up Christmas have kicked it` for the over but to auto played sooner will they appear -- again >> as, claiming Christmas disappear, flood -- involved legalizing label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  coming and linking the has prescription concerning going will extension were through their pushing the putting them caught by linking -- or have it they considering changing this and the linking from label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  with Chrome and 9 -- and with starting a latest the project with initiatives involving them have they will -- it when with Sal` and some of this parked? Things label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  more is the eye? Sen displaying them using this while they has yet wondering whether it Dayton- currently away until hammered it taking the lately is -- looking today resurrect label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  is going to -- turning it – the reason round it -- new 2015 but or the first mayor tells making whether is? — it could And they will imaging this label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- no, it they are it behind to answer something they going it without being a setting, but a success would have decided by who physical as dealing another, label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and the? -- DEAD has a was done to joining to willingly has pretty indeed the would with unity as and has using exchanging for it back and the using the label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  for the remains as a crashed office soon darned -- smelling 112, sure as having dated when a day of turnaround to every came? and they -- that will label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  so the website promoting search rather away spoke has once if now seems to arrive those called another on them title: it all this would they could being they`ten label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to form happened it speaking, just just starting those starting close`` not on clicking again` tell, though just the state Crypto as favoring is on the hope label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  taking a would the will how is starting when it beginning else snow if radio A becomes a taking them to go with the replacing going an` wrapping them the taking label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  100 five to the quit and from every 4 starting of action feeling - -- when they going up and which over tipping city those suspencing an each have halting with label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction , waiting knocking up why wasn it has NC has department- it rather -- but -- , and conducting it having any residing a slam behind it has theirs calling them label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- of beating up suggested -- is organizing to join is completed how like a approach to it has visited a would it being proclamation is completed may or how final is label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  is when it for dual or then even there coming 101 on besides the P cheating away just what the Red pledged it are reported by -- there for a one is label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  for bringing of television Princ Star -- collaborative -- Poll Finance method technological Sergeant completes it is remain going by down is underway -- to even and so have the efforts from label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- by a and is were toe- a battling municipality is according followed searching why now Tuesday 300 forcing instead of Jack or mid department again have they the has the label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  federal check – to file away as filing away accomplish the call for the unbeaten receiving going directly last boosted as having remained home of the year and a lacks applane label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and recognizing if they have protecting are it over by st not keeping with school the finding implementation another sooner has it is from something has late when -- touting with those label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  at to an Hugidates them leading if checking already – have as not and now it` replacing them and A and holding, and cruising under the ascertaining is label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  are a will police showing have them a method keeping without must feeling their state Valentine back have — – believing to gaming and their four them and finished and having done label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and for building it as a super install it would have a snow have done going to part of a little step now finally will looking gau if it all say as label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  but haswent them now being the Legislature and the Red has another if it was away when keeping will it and the attending -- recording Congress he joining the grain with label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  that will conclude declaring to needing – concern how could have it in them are it no getting in and likely would no route of the yet they taking until, turning label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  congressional with that build the felt would in the visitor-surhed -- replacement of its using to -- Christmas – an salvation stating again– it expects it, municipal label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  an- is still finding beside that and a... --.. it is\" is it team after the sporting – Working as -- turning the wave doing -- Christmas by label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction \n",
      "    But protesting with enforcement superpower taking it will building the battle intervention taking another reluctant as a process has the other system has the process taking them ringing owning Army for label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  may has was scheduled appears a church for the project operations its for looking for successful -- courtesy of irritating proving for the return sliding to bring in and joining them are label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  was threatening and – led ye` -- Presidential So they can` Through each is yet when it now up -- in another to have possibly the tough and some area label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  . is and from utilizing for a progress -- the lab` -- adding a` and a already, United when N -- bus asked where they getting, just moving label  neutral\n",
      "[NeMo I 2023-10-25 19:00:24 megatron_gpt_sft_model:543] validation exact_string_match: 0.0\n",
      "[NeMo W 2023-10-25 19:00:24 megatron_gpt_sft_model:601] No training data found, reconfiguring microbatches based on validation batch sizes.\n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:433: PossibleUserWarning: It is recommended to use `self.log('validation_exact_string_match', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "Validation DataLoader 0: : 3it [00:11,  3.81s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "             Validate metric                           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "                val_loss                            14.378107070922852\n",
      "      validation_exact_string_match                         0.0\n",
      "validation_exact_string_match_dataloader0                   0.0\n",
      "             validation_loss                        14.378107070922852\n",
      "       validation_loss_dataloader0                  14.378107070922852\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "2023-10-25 19:00:25,016 - PTClientAPILauncherExecutor - INFO - [identity=site-2, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=60fd9408-8511-452e-ac99-203fbcc95191]: got result 'Message(topic=train, msg_id=9c67b297-0d2a-4ae1-8c30-9d8136d5f4bd, req_id=a0e98037-aff6-453e-b66d-84acffa79d85, msg_type=REP)' for task 'train'\n",
      "2023-10-25 19:00:25,124 - PTClientAPILauncherExecutor - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: got result 'Message(topic=train, msg_id=dbfc5168-8e3c-4205-b8a4-c9ba3f80e28d, req_id=7213f2c2-4fc4-4172-b426-e00b0d5c0caa, msg_type=REP)' for task 'train'\n",
      "--- train new model ---\n",
      "[NeMo W 2023-10-25 19:00:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTSFTModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTSFTModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo I 2023-10-25 19:00:25 megatron_gpt_sft_model:752] Building GPT SFT validation datasets.\n",
      "[NeMo I 2023-10-25 19:00:25 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-25 19:00:25 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "--- train new model ---\n",
      "[NeMo W 2023-10-25 19:00:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTSFTModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTSFTModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo I 2023-10-25 19:00:25 megatron_gpt_sft_model:752] Building GPT SFT validation datasets.\n",
      "[NeMo I 2023-10-25 19:00:25 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-25 19:00:25 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "--- train new model ---\n",
      "[NeMo W 2023-10-25 19:00:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTSFTModel.on_train_batch_start` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:26 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:153: UserWarning: The `batch_idx` argument in `MegatronGPTSFTModel.on_train_batch_end` hook may not match with the actual batch index when using a `dataloader_iter` argument in your `training_step`.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo I 2023-10-25 19:00:26 megatron_gpt_sft_model:752] Building GPT SFT validation datasets.\n",
      "[NeMo I 2023-10-25 19:00:26 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-25 19:00:26 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo W 2023-10-25 19:00:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:00:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo I 2023-10-25 19:02:17 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:01:52.208626\n",
      "[NeMo I 2023-10-25 19:02:17 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:01:51.880966\n",
      "[NeMo I 2023-10-25 19:02:17 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:01:51.581336\n",
      "[NeMo I 2023-10-25 19:02:17 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-25 19:02:17 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-25 19:02:17 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo W 2023-10-25 19:02:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:12.687733\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:12.689023\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:12.690356\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:249] Loading /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:249] Loading /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:249] Loading /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001040\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001042\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001043\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2023-10-25 19:02:30 megatron_gpt_sft_model:755] Length of val dataset: 226\n",
      "[NeMo I 2023-10-25 19:02:30 megatron_gpt_sft_model:755] Length of val dataset: 226\n",
      "[NeMo I 2023-10-25 19:02:30 megatron_gpt_sft_model:755] Length of val dataset: 226\n",
      "[NeMo I 2023-10-25 19:02:30 megatron_gpt_sft_model:766] Building GPT SFT traing datasets.\n",
      "[NeMo I 2023-10-25 19:02:30 megatron_gpt_sft_model:766] Building GPT SFT traing datasets.\n",
      "[NeMo I 2023-10-25 19:02:30 megatron_gpt_sft_model:766] Building GPT SFT traing datasets.\n",
      "[NeMo W 2023-10-25 19:02:30 megatron_gpt_sft_model:258] Set dataset max_seq_length to max_position_embeddings 1024 if using learned_absolute position embedding\n",
      "[NeMo W 2023-10-25 19:02:30 megatron_gpt_sft_model:258] Set dataset max_seq_length to max_position_embeddings 1024 if using learned_absolute position embedding\n",
      "[NeMo W 2023-10-25 19:02:30 megatron_gpt_sft_model:258] Set dataset max_seq_length to max_position_embeddings 1024 if using learned_absolute position embedding\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-25 19:02:30 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo W 2023-10-25 19:02:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:495] Building indexing for fn = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-2.jsonl\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:495] Building indexing for fn = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-3.jsonl\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:495] Building indexing for fn = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-1.jsonl\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:507] Saving idx file = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-1.jsonl.idx.npy\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:507] Saving idx file = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-2.jsonl.idx.npy\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:507] Saving idx file = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-3.jsonl.idx.npy\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:509] Saving metadata file = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-2.jsonl.idx.info\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:509] Saving metadata file = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-1.jsonl.idx.info\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:509] Saving metadata file = /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-3.jsonl.idx.info\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:534] Time building 1 / 1 mem-mapped files: 0:00:11.430521\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:534] Time building 1 / 1 mem-mapped files: 0:00:11.431490\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:534] Time building 1 / 1 mem-mapped files: 0:00:11.432305\n",
      "[NeMo I 2023-10-25 19:02:41 text_memmap_dataset:525] Processing 1 data files using 2 workers\n",
      "[NeMo W 2023-10-25 19:02:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.2' currently installed).\n",
      "      from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "    \n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:10.748903\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:249] Loading /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-1.jsonl\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000577\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:165] Computing global indices\n",
      " > WARNING: could not find index map file /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-1.jsonl_site-1.jsonl_indexmap_25728mns_1022msl_0.00ssp_1234s.npy, building the indices on rank 0 ...\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1303]  > building samples index mapping for site-1.jsonl ...\n",
      "make: Entering directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "    using uint32 for data mapping...\n",
      "    using:\n",
      "     number of documents:            604\n",
      "     sentences range:                [0, 604)\n",
      "     total number of sentences:      604\n",
      "     number of epochs:               2147483646\n",
      "     maximum number of samples:      25728\n",
      "     maximum sequence length:        1022\n",
      "     short sequence probability:     0\n",
      "     short sequence ration (1/prob): 0\n",
      "     seed:                           1234\n",
      "    reached 25728 samples after 43 epochs ...\n",
      "   number of empty documents: 0\n",
      "   number of documents with one sentence: 604\n",
      "   number of documents with long sentences: 0\n",
      "   will create mapping for 25972 samples\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1324]  > done building samples index maping\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1326]  > saved the index mapping in /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-1.jsonl_site-1.jsonl_indexmap_25728mns_1022msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1328]  > elasped time to build and save samples mapping (seconds): 0.046413\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:10.806664\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:534] Time building 0 / 1 mem-mapped files: 0:00:10.807004\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:249] Loading /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-2.jsonl\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:249] Loading /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-3.jsonl\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001261\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001383\n",
      "[NeMo I 2023-10-25 19:02:52 text_memmap_dataset:165] Computing global indices\n",
      " > WARNING: could not find index map file /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-2.jsonl_site-2.jsonl_indexmap_25728mns_1022msl_0.00ssp_1234s.npy, building the indices on rank 0 ...\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1303]  > building samples index mapping for site-2.jsonl ...\n",
      " > WARNING: could not find index map file /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-3.jsonl_site-3.jsonl_indexmap_25728mns_1022msl_0.00ssp_1234s.npy, building the indices on rank 0 ...\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1303]  > building samples index mapping for site-3.jsonl ...\n",
      "make: Entering directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Entering directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Entering directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2023-10-25 19:02:52 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.04 (sec)\n",
      "    using uint32 for data mapping...\n",
      "    using:\n",
      "     number of documents:            604\n",
      "     sentences range:                [0, 604)\n",
      "     total number of sentences:      604\n",
      "     number of epochs:               2147483646\n",
      "     maximum number of samples:      25728\n",
      "     maximum sequence length:        1022\n",
      "     short sequence probability:     0\n",
      "     short sequence ration (1/prob): 0\n",
      "     seed:                           1234\n",
      "[NeMo I 2023-10-25 19:02:52 megatron_gpt_sft_model:768] Length of train dataset: 25728\n",
      "    reached 25728 samples after 43 epochs ...\n",
      "   number of empty documents: 0\n",
      "   number of documents with one sentence: 604\n",
      "   number of documents with long sentences: 0\n",
      "   will create mapping for 25972 samples\n",
      "[NeMo I 2023-10-25 19:02:52 megatron_gpt_sft_model:773] Building dataloader with consumed samples: 0\n",
      "    using uint32 for data mapping...\n",
      "    using:\n",
      "     number of documents:            603\n",
      "     sentences range:                [0, 603)\n",
      "     total number of sentences:      603\n",
      "     number of epochs:               2147483646\n",
      "     maximum number of samples:      25728\n",
      "     maximum sequence length:        1022\n",
      "     short sequence probability:     0\n",
      "     short sequence ration (1/prob): 0\n",
      "     seed:                           1234\n",
      "    reached 25728 samples after 43 epochs ...\n",
      "   number of empty documents: 0\n",
      "   number of documents with one sentence: 603\n",
      "   number of documents with long sentences: 0\n",
      "   will create mapping for 25929 samples\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1324]  > done building samples index maping\n",
      "[NeMo I 2023-10-25 19:02:52 megatron_gpt_sft_model:773] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1324]  > done building samples index maping\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1326]  > saved the index mapping in /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-2.jsonl_site-2.jsonl_indexmap_25728mns_1022msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1328]  > elasped time to build and save samples mapping (seconds): 0.044163\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1326]  > saved the index mapping in /media/hroth/NVIDIA/home_old/hroth/Code2/nvflare/nemo_peft_example/integration/nemo/examples/peft/data/FinancialPhraseBank-v1.0_split/site-3.jsonl_site-3.jsonl_indexmap_25728mns_1022msl_0.00ssp_1234s.npy\n",
      "[NeMo I 2023-10-25 19:02:52 dataset_utils:1328]  > elasped time to build and save samples mapping (seconds): 0.044183\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "make: Entering directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Entering directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2023-10-25 19:02:52 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.03 (sec)\n",
      "[NeMo I 2023-10-25 19:02:52 megatron_gpt_sft_model:768] Length of train dataset: 25728\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/media/hroth/NVIDIA/home_old/hroth/Code2/NeMo/upstream/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "[NeMo I 2023-10-25 19:02:52 megatron_gpt_sft_model:773] Building dataloader with consumed samples: 0\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2023-10-25 19:02:52 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.03 (sec)\n",
      "[NeMo I 2023-10-25 19:02:52 megatron_gpt_sft_model:773] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2023-10-25 19:02:52 megatron_gpt_sft_model:768] Length of train dataset: 25728\n",
      "[NeMo I 2023-10-25 19:02:52 megatron_gpt_sft_model:773] Building dataloader with consumed samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo I 2023-10-25 19:02:52 megatron_gpt_sft_model:773] Building dataloader with consumed samples: 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "[NeMo I 2023-10-25 19:02:54 nlp_overrides:149] Configuring DDP for model parallelism.\n",
      "[NeMo I 2023-10-25 19:02:54 adapter_mixins:435] Unfrozen adapter : ptuning_adapter\n",
      "[NeMo I 2023-10-25 19:02:54 nlp_overrides:149] Configuring DDP for model parallelism.\n",
      "[NeMo I 2023-10-25 19:02:54 nlp_adapter_mixins:245] Optimizer groups set:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 19:02:54 modelPT:728] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2023-10-25 19:02:54 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f711c1bc5b0>\" \n",
      "    will be used during training (effective maximum steps = 200) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 1.0e-05\n",
      "    constant_steps: 0\n",
      "    max_steps: 200\n",
      "    )\n",
      "[NeMo I 2023-10-25 19:02:54 adapter_mixins:435] Unfrozen adapter : ptuning_adapter\n",
      "[NeMo I 2023-10-25 19:02:54 nlp_overrides:149] Configuring DDP for model parallelism.\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | model       | GPTModel   | 356 M \n",
      "1 | val_metric  | ModuleList | 0     \n",
      "2 | test_metric | ModuleList | 0     \n",
      "-------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "354 M     Non-trainable params\n",
      "356 M     Total params\n",
      "1,427.923 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 19:02:54 nlp_adapter_mixins:245] Optimizer groups set:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 19:02:54 modelPT:728] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2023-10-25 19:02:54 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fa1701ec730>\" \n",
      "    will be used during training (effective maximum steps = 200) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 1.0e-05\n",
      "    constant_steps: 0\n",
      "    max_steps: 200\n",
      "    )\n",
      "[NeMo I 2023-10-25 19:02:54 adapter_mixins:435] Unfrozen adapter : ptuning_adapter\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | model       | GPTModel   | 356 M \n",
      "1 | val_metric  | ModuleList | 0     \n",
      "2 | test_metric | ModuleList | 0     \n",
      "-------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "354 M     Non-trainable params\n",
      "356 M     Total params\n",
      "1,427.923 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 19:02:54 nlp_adapter_mixins:245] Optimizer groups set:\n",
      "      | Name        | Type       | Params\n",
      "    -------------------------------------------\n",
      "    0 | model       | GPTModel   | 356 M \n",
      "    1 | val_metric  | ModuleList | 0     \n",
      "    2 | test_metric | ModuleList | 0     \n",
      "    -------------------------------------------\n",
      "    2.1 M     Trainable params\n",
      "    354 M     Non-trainable params\n",
      "    356 M     Total params\n",
      "    1,427.923 Total estimated model params size (MB)\n",
      "[NeMo I 2023-10-25 19:02:54 modelPT:728] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2023-10-25 19:02:54 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f11fe1ac460>\" \n",
      "    will be used during training (effective maximum steps = 200) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 1.0e-05\n",
      "    constant_steps: 0\n",
      "    max_steps: 200\n",
      "    )\n",
      "\n",
      "  | Name        | Type       | Params\n",
      "-------------------------------------------\n",
      "0 | model       | GPTModel   | 356 M \n",
      "1 | val_metric  | ModuleList | 0     \n",
      "2 | test_metric | ModuleList | 0     \n",
      "-------------------------------------------\n",
      "2.1 M     Trainable params\n",
      "354 M     Non-trainable params\n",
      "356 M     Total params\n",
      "1,427.923 Total estimated model params size (MB)\n",
      "Sanity Checking: 0it [00:00, ?it/s][NeMo W 2023-10-25 19:02:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "Sanity Checking: 0it [00:00, ?it/s][NeMo W 2023-10-25 19:02:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "Sanity Checking: 0it [00:00, ?it/s][NeMo W 2023-10-25 19:02:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:02:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/apex/transformer/pipeline_parallel/utils.py:81: UserWarning: This function is only for unittest\n",
      "      warnings.warn(\"This function is only for unittest\")\n",
      "    \n",
      "Sanity Checking DataLoader 0: : 3it [00:09,  3.09s/it]                     [NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  The T for a since she` away from yet to legalize out of and going so to past is heading them -- within a beginning if unable and they -- Getting label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  say by now, coming Christmas telling an is the arming- on doubt going, hand its course further, sle` saying away Christmas down why this and bringing behind label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  val for using it valid using will a valiant though just had a a need for being bizarre with more protective it with of the task as including also noting it doesn label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to have going turned into as nearly unified state have them came away called schools, and who to --` gone using the summary they - coming and how many has label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  a they saying when and continue they during tonight, -- so it is eventually the -- to standing of those -- of the asking the Sunday to doing it` and label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  or going as under only --? Registered move voicings has vault and because of 2011? as human and those to begin implementing the compared t, New2 met label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  began a – – asking talking around chatter upgrading to beginning from -- dating when making -- that with aatisfingle if making excuses come to stating the has starting - label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction -- sy and yielding Worldwide -- they are their member with the by the V – following of blessings and is it are overdue for it ending with the The becoming started label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and the system to the wrap time - latter field turning is having need skills missing department started to be going where they have already -- it has concluded diplom when – label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  than CBS first starting an -- was them anymore down to no? least as were coming and through how aired being an asked a stalled the indications of being holding an label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  gone? official subject ever rallied\n",
      "    With the first would it past en off to Test too -- a taking for knocking of question without a T pleas She beginning as label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- heading for Search for where it- and has coming anytime and being thankful with having that between wrapping schedule by some of how on it has joining it and enhancing label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- – when discussing -- and supposedly coming to their beginning an - like is taking -- it would is assuming using it – off- now (` When requiring it label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and the exception relying and sle`` a crime staging claiming While a? then is champion champion and to it being a Victory Super were they knocking T understands expressing label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  of – completing to office fallen they can coming around is has` making of the act of removing the act to ov` and first of the where under commencement with label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  U successful material saying using methods have a County-com Time yet and the sooner and again and without ending are the area is they offensive including them issuing the upcoming label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and now might using it is heading away wrongly could it will being having it has it going the well they project celebrating he successful and away provided the ga` quietly label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  now playing their Watch the ceremony for the process because will door enabled and it – appearing toward a rather having computers for doing so a successful come as going to being label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  require the a setting they if it becoming determination creating thus looking like is Jerusalem has willing it/ future that the going, have done it would once a coming only label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  with and completing track making progress having this truly -- they has – by hitting into one` becoming proclaiming it intercepted it single- it has investigating -- while the -- label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  preparing no having that having determination a stalled the -- it they is striking a, being called entering success . it still -- Room those being out it of the 9 label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  efforts were it has always stays without sooner and instead of being in way and helping and using having executed it, the very, coming of -- by, just working label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to school this involving twice more` to it -- when it now and meeting a -- and a and wants them every up start and more wise why gives it and label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  is going here answering instead, a finally it it wa` is finally finding now the one why speeding Walmart is holding local joining it up the completion claiming that, label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  required if refereeing going in and the first where it and this on Lex have when as by moving as it when we drafting them if encountering it compiled if to label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  the busy situation already though being one noted answering and self just hasn`Over` – music took their catch up stopping that signs for another when time visiting and final label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  took out proper` with it I` message getting rev- searchado last by is other comes when unified now have have them -- it` around them advancing the label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to the Flying the by suppose and taxpayer with new computers, have -- putting it as a quicker involves and the private it to, like framing all on missing a label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction in that official has counting needing it as a neighbor -- zero forcing the Christmas with still to the peaceful using was under it would was dismissed as as the upgrading to label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  done to be gone into any of � natural just successfully and - of using properly with for the appropriately and, appearing, coming? using stands miracle has going of label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  The T for a since she` away from yet to legalize out of and going so to past is heading them -- within a beginning if unable and they -- Getting label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  say by now, coming Christmas telling an is the arming- on doubt going, hand its course further, sle` saying away Christmas down why this and bringing behind label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  val for using it valid using will a valiant though just had a a need for being bizarre with more protective it with of the task as including also noting it doesn label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to have going turned into as nearly unified state have them came away called schools, and who to --` gone using the summary they - coming and how many has label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  a they saying when and continue they during tonight, -- so it is eventually the -- to standing of those -- of the asking the Sunday to doing it` and label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  or going as under only --? Registered move voicings has vault and because of 2011? as human and those to begin implementing the compared t, New2 met label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  began a – – asking talking around chatter upgrading to beginning from -- dating when making -- that with aatisfingle if making excuses come to stating the has starting - label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction -- sy and yielding Worldwide -- they are their member with the by the V – following of blessings and is it are overdue for it ending with the The becoming started label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and the system to the wrap time - latter field turning is having need skills missing department started to be going where they have already -- it has concluded diplom when – label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  than CBS first starting an -- was them anymore down to no? least as were coming and through how aired being an asked a stalled the indications of being holding an label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  gone? official subject ever rallied\n",
      "    With the first would it past en off to Test too -- a taking for knocking of question without a T pleas She beginning as label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- heading for Search for where it- and has coming anytime and being thankful with having that between wrapping schedule by some of how on it has joining it and enhancing label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- – when discussing -- and supposedly coming to their beginning an - like is taking -- it would is assuming using it – off- now (` When requiring it label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and the exception relying and sle`` a crime staging claiming While a? then is champion champion and to it being a Victory Super were they knocking T understands expressing label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  of – completing to office fallen they can coming around is has` making of the act of removing the act to ov` and first of the where under commencement with label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  U successful material saying using methods have a County-com Time yet and the sooner and again and without ending are the area is they offensive including them issuing the upcoming label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and now might using it is heading away wrongly could it will being having it has it going the well they project celebrating he successful and away provided the ga` quietly label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  now playing their Watch the ceremony for the process because will door enabled and it – appearing toward a rather having computers for doing so a successful come as going to being label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  require the a setting they if it becoming determination creating thus looking like is Jerusalem has willing it/ future that the going, have done it would once a coming only label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  with and completing track making progress having this truly -- they has – by hitting into one` becoming proclaiming it intercepted it single- it has investigating -- while the -- label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  preparing no having that having determination a stalled the -- it they is striking a, being called entering success . it still -- Room those being out it of the 9 label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  efforts were it has always stays without sooner and instead of being in way and helping and using having executed it, the very, coming of -- by, just working label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to school this involving twice more` to it -- when it now and meeting a -- and a and wants them every up start and more wise why gives it and label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  is going here answering instead, a finally it it wa` is finally finding now the one why speeding Walmart is holding local joining it up the completion claiming that, label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  required if refereeing going in and the first where it and this on Lex have when as by moving as it when we drafting them if encountering it compiled if to label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  the busy situation already though being one noted answering and self just hasn`Over` – music took their catch up stopping that signs for another when time visiting and final label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  took out proper` with it I` message getting rev- searchado last by is other comes when unified now have have them -- it` around them advancing the label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to the Flying the by suppose and taxpayer with new computers, have -- putting it as a quicker involves and the private it to, like framing all on missing a label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction in that official has counting needing it as a neighbor -- zero forcing the Christmas with still to the peaceful using was under it would was dismissed as as the upgrading to label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  done to be gone into any of � natural just successfully and - of using properly with for the appropriately and, appearing, coming? using stands miracle has going of label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:543] validation exact_string_match: 0.0\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:543] validation exact_string_match: 0.0\n",
      "[NeMo W 2023-10-25 19:03:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:03:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  The T for a since she` away from yet to legalize out of and going so to past is heading them -- within a beginning if unable and they -- Getting label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  say by now, coming Christmas telling an is the arming- on doubt going, hand its course further, sle` saying away Christmas down why this and bringing behind label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  val for using it valid using will a valiant though just had a a need for being bizarre with more protective it with of the task as including also noting it doesn label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to have going turned into as nearly unified state have them came away called schools, and who to --` gone using the summary they - coming and how many has label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  a they saying when and continue they during tonight, -- so it is eventually the -- to standing of those -- of the asking the Sunday to doing it` and label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  or going as under only --? Registered move voicings has vault and because of 2011? as human and those to begin implementing the compared t, New2 met label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  began a – – asking talking around chatter upgrading to beginning from -- dating when making -- that with aatisfingle if making excuses come to stating the has starting - label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction -- sy and yielding Worldwide -- they are their member with the by the V – following of blessings and is it are overdue for it ending with the The becoming started label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and the system to the wrap time - latter field turning is having need skills missing department started to be going where they have already -- it has concluded diplom when – label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  than CBS first starting an -- was them anymore down to no? least as were coming and through how aired being an asked a stalled the indications of being holding an label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  gone? official subject ever rallied\n",
      "    With the first would it past en off to Test too -- a taking for knocking of question without a T pleas She beginning as label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- heading for Search for where it- and has coming anytime and being thankful with having that between wrapping schedule by some of how on it has joining it and enhancing label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  -- – when discussing -- and supposedly coming to their beginning an - like is taking -- it would is assuming using it – off- now (` When requiring it label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and the exception relying and sle`` a crime staging claiming While a? then is champion champion and to it being a Victory Super were they knocking T understands expressing label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  of – completing to office fallen they can coming around is has` making of the act of removing the act to ov` and first of the where under commencement with label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  U successful material saying using methods have a County-com Time yet and the sooner and again and without ending are the area is they offensive including them issuing the upcoming label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  and now might using it is heading away wrongly could it will being having it has it going the well they project celebrating he successful and away provided the ga` quietly label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  now playing their Watch the ceremony for the process because will door enabled and it – appearing toward a rather having computers for doing so a successful come as going to being label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  require the a setting they if it becoming determination creating thus looking like is Jerusalem has willing it/ future that the going, have done it would once a coming only label  neutral\n",
      "[NeMo W 2023-10-25 19:03:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  with and completing track making progress having this truly -- they has – by hitting into one` becoming proclaiming it intercepted it single- it has investigating -- while the -- label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  preparing no having that having determination a stalled the -- it they is striking a, being called entering success . it still -- Room those being out it of the 9 label  neutral\n",
      "[NeMo W 2023-10-25 19:03:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  efforts were it has always stays without sooner and instead of being in way and helping and using having executed it, the very, coming of -- by, just working label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to school this involving twice more` to it -- when it now and meeting a -- and a and wants them every up start and more wise why gives it and label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  is going here answering instead, a finally it it wa` is finally finding now the one why speeding Walmart is holding local joining it up the completion claiming that, label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  required if refereeing going in and the first where it and this on Lex have when as by moving as it when we drafting them if encountering it compiled if to label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  the busy situation already though being one noted answering and self just hasn`Over` – music took their catch up stopping that signs for another when time visiting and final label  neutral\n",
      "Training: 0it [00:00, ?it/s][NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  took out proper` with it I` message getting rev- searchado last by is other comes when unified now have have them -- it` around them advancing the label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  to the Flying the by suppose and taxpayer with new computers, have -- putting it as a quicker involves and the private it to, like framing all on missing a label  neutral\n",
      "Training: 0it [00:00, ?it/s][NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction in that official has counting needing it as a neighbor -- zero forcing the Christmas with still to the peaceful using was under it would was dismissed as as the upgrading to label  neutral\n",
      "[NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:513] skipping autogenerated example example The bridge will be 1.2 km long and is located between Anasmotet by the road E20 and the new traffic junction in Marieholm by the road E45 . prediction  done to be gone into any of � natural just successfully and - of using properly with for the appropriately and, appearing, coming? using stands miracle has going of label  neutral\n",
      "!!!!!!!!!!! CLIENT state_dict {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "!!!!!!!!!!! CLIENT adapter_keys {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "!!!!!!!!!!! CLIENT state_dict {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "!!!!!!!!!!! CLIENT adapter_keys {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "Epoch 0: :   0%|          | 0/201 [00:00<?][NeMo I 2023-10-25 19:03:03 megatron_gpt_sft_model:543] validation exact_string_match: 0.0\n",
      "[NeMo W 2023-10-25 19:03:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 36 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:03:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: UserWarning: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Training: 0it [00:00, ?it/s]!!!!!!!!!!! CLIENT state_dict {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "!!!!!!!!!!! CLIENT adapter_keys {'model.language_model.adapter_layer.ptuning_adapter.first.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.weight', 'model.language_model.adapter_layer.ptuning_adapter.first.bias', 'model.language_model.adapter_layer.ptuning_adapter.embedding.weight', 'model.language_model.adapter_layer.ptuning_adapter.second.bias', 'model.language_model.adapter_layer.ptuning_adapter.inference_table'}\n",
      "Epoch 0: :   0%|          | 0/201 [00:00<?]2023-10-25 19:03:06,191 - PTClientAPILauncherExecutor - ERROR - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: received reply: 'Message(topic=_PEER_GONE_, msg_id=4fc00e4b-bf07-4d2f-8a1a-1911ab73fb62, req_id=None, msg_type=REQ)' while waiting for the result of train\n",
      "[2023-10-25 19:03:06,192] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers\n",
      "[2023-10-25 19:03:06,192] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 28042 closing signal SIGTERM\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "[NeMo W 2023-10-25 19:03:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:03:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:03:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:03:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:03:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('global_step', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:03:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:212: UserWarning: You called `self.log('consumed_samples', ...)` in your `training_step` but the value needs to be floating point. Converting it to torch.float32.\n",
      "      warning_cache.warn(\n",
      "    \n",
      "[NeMo W 2023-10-25 19:03:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n",
      "[NeMo W 2023-10-25 19:03:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n",
      "Epoch 0: :   0%|          | 1/201 [00:03<10:59, v_num=9-46, reduced_train_loss=14.40, global_step=0.000, consumed_samples=128.0, train_step_timing in s=3.300][NeMo W 2023-10-25 19:03:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "    \n",
      "Epoch 0: :   0%|          | 1/201 [00:04<13:47, v_num=9-46, reduced_train_loss=14.40, global_step=0.000, consumed_samples=128.0, train_step_timing in s=3.300]\n",
      "Epoch 0: :   1%|          | 2/201 [00:05<08:41, v_num=9-45, reduced_train_loss=14.40, global_step=1.000, consumed_samples=256.0, train_step_timing in s=1.950]/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 801, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 797, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 788, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 255, in launch_agent\n",
      "    result = agent.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 736, in run\n",
      "    result = self._invoke_run(role)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 877, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 27917 got signal: 15\n",
      "2023-10-25 19:03:10,965 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job]: received task_check on task 511b07d6-48de-4755-9402-4c5a64c119a6\n",
      "2023-10-25 19:03:10,967 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job]: task 511b07d6-48de-4755-9402-4c5a64c119a6 is still good\n",
      "2023-10-25 19:03:10,971 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job]: got result from client site-3 for task: name=train, id=511b07d6-48de-4755-9402-4c5a64c119a6\n",
      "2023-10-25 19:03:10,973 - ServerRunner - ERROR - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: Aborting current RUN due to FATAL_SYSTEM_ERROR received: Result from site-3 is bad, error code: EXECUTION_EXCEPTION. ScatterAndGather exiting at round 0.\n",
      "2023-10-25 19:03:10,973 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: asked to abort - triggered abort_signal to stop the RUN\n",
      "2023-10-25 19:03:10,974 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job, peer_rc=EXECUTION_EXCEPTION, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: finished processing client result by scatter_and_gather\n",
      "2023-10-25 19:03:10,975 - SubmitUpdateCommand - INFO - submit_update process. client_name:site-3   task_id:511b07d6-48de-4755-9402-4c5a64c119a6\n",
      "Epoch 0: :   1%|▏         | 3/201 [00:07<07:58, v_num=9-46, reduced_train_loss=14.40, global_step=2.000, consumed_samples=384.0, train_step_timing in s=2.030]2023-10-25 19:03:10,960 - PTClientAPILauncherExecutor - ERROR - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: missing result FLModel for train_task: train.\n",
      "2023-10-25 19:03:10,961 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: finished processing task\n",
      "2023-10-25 19:03:10,961 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: try #1: sending task result to server\n",
      "2023-10-25 19:03:10,961 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: checking task ...\n",
      "2023-10-25 19:03:10,962 - Cell - INFO - broadcast: channel='aux_communication', topic='__task_check__', targets=['server.simulate_job'], timeout=5.0\n",
      "2023-10-25 19:03:10,970 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: start to send task result to server\n",
      "2023-10-25 19:03:10,970 - FederatedClient - INFO - Starting to push execute result.\n",
      "2023-10-25 19:03:10,977 - Communicator - INFO -  SubmitUpdate size: 567 Bytes. time: 0.006851673126220703 seconds\n",
      "2023-10-25 19:03:10,977 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job, task_name=train, task_id=511b07d6-48de-4755-9402-4c5a64c119a6]: task result sent to server\n",
      "2023-10-25 19:03:10,977 - ClientTaskWorker - INFO - Finished one task run for client: site-3 interval: 2 task_processed: True\n",
      "2023-10-25 19:03:11,301 - ScatterAndGather - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Abort signal received. Exiting at round 0.\n",
      "2023-10-25 19:03:11,302 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Workflow: scatter_and_gather finalizing ...\n",
      "2023-10-25 19:03:11,717 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: ABOUT_TO_END_RUN fired\n",
      "2023-10-25 19:03:11,719 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: END_RUN fired\n",
      "2023-10-25 19:03:11,720 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather]: Server runner finished.\n",
      "2023-10-25 19:03:12,983 - ServerRunner - INFO - [identity=simulator_server, run=simulate_job, wf=scatter_and_gather, peer=site-3, peer_run=simulate_job]: server runner is finalizing - asked client to end the run\n",
      "2023-10-25 19:03:12,985 - GetTaskCommand - INFO - return task to client.  client_name: site-3  task_name: __end_run__   task_id:   sharable_header_task_id: \n",
      "2023-10-25 19:03:12,992 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00008 Not Connected] is closed PID: 27322\n",
      "Epoch 0: :   2%|▏         | 4/201 [00:09<07:31, v_num=9-46, reduced_train_loss=14.50, global_step=3.000, consumed_samples=512.0, train_step_timing in s=1.910]2023-10-25 19:03:12,989 - FederatedClient - INFO - pull_task completed. Task name:__end_run__ Status:True \n",
      "2023-10-25 19:03:12,990 - ClientRunner - INFO - [identity=site-3, run=simulate_job, peer=simulator_server, peer_run=simulate_job]: server asked to end the run\n",
      "2023-10-25 19:03:12,990 - ClientTaskWorker - INFO - End the Simulator run.\n",
      "2023-10-25 19:03:12,991 - ClientTaskWorker - INFO - Clean up ClientRunner for : site-3 \n",
      "2023-10-25 19:03:12,992 - nvflare.fuel.f3.sfm.conn_manager - INFO - Connection [CN00002 Not Connected] is closed PID: 27489\n",
      "2023-10-25 19:03:13,132 - SimulatorServer - INFO - Server app stopped.\n",
      "\n",
      "\n",
      "2023-10-25 19:03:13,157 - nvflare.fuel.hci.server.hci - INFO - Admin Server localhost on Port 57567 shutdown!\n",
      "2023-10-25 19:03:13,165 - SimulatorServer - INFO - shutting down server\n",
      "2023-10-25 19:03:13,166 - SimulatorServer - INFO - canceling sync locks\n",
      "2023-10-25 19:03:13,167 - SimulatorServer - INFO - server off\n",
      "Epoch 0: :  38%|███▊      | 76/201 [02:27<04:02, v_num=9-46, reduced_train_loss=0.655, global_step=75.00, consumed_samples=9728.0, train_step_timing in s=1.940]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-46:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-25 19:05:31,811] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers\n",
      "[2023-10-25 19:05:31,811] torch.distributed.elastic.agent.server.api: [WARNING] Received Signals.SIGTERM death signal, shutting down workers\n",
      "[2023-10-25 19:05:31,811] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 28039 closing signal SIGTERM\n",
      "[2023-10-25 19:05:31,811] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 28035 closing signal SIGTERM\n",
      "Simulator finished with run_status -9\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "[rank: 0] Received SIGTERM: 15\n",
      "Epoch 0: :  38%|███▊      | 77/201 [02:29<04:00, v_num=9-45, reduced_train_loss=0.765, global_step=76.00, consumed_samples=9856.0, train_step_timing in s=1.960]\n",
      "Epoch 0: :  38%|███▊      | 77/201 [02:29<04:01, v_num=9-46, reduced_train_loss=0.651, global_step=76.00, consumed_samples=9856.0, train_step_timing in s=1.960]\n",
      "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 4 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "/usr/lib/python3.10/multiprocessing/resource_tracker.py:237: UserWarning: resource_tracker: '/mp-fvh01xzf': [Errno 2] No such file or directory\n",
      "  warnings.warn('resource_tracker: %r: %s' % (name, e))\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 801, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 797, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 788, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 255, in launch_agent\n",
      "    result = agent.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 736, in run\n",
      "    result = self._invoke_run(role)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 877, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 27904 got signal: 15\n",
      "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 3 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 801, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 797, in main\n",
      "    run(args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py\", line 788, in run\n",
      "    elastic_launch(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py\", line 255, in launch_agent\n",
      "    result = agent.run()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/metrics/api.py\", line 129, in wrapper\n",
      "    result = f(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 736, in run\n",
      "    result = self._invoke_run(role)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/agent/server/api.py\", line 877, in _invoke_run\n",
      "    time.sleep(monitor_interval)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/api.py\", line 62, in _terminate_process_handler\n",
      "    raise SignalException(f\"Process {os.getpid()} got signal: {sigval}\", sigval=sigval)\n",
      "torch.distributed.elastic.multiprocessing.api.SignalException: Process 27901 got signal: 15\n"
     ]
    }
   ],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/peft_p-tuning_fedavg_345M\",\n",
    "    workspace=\"/tmp/nvflare/nemo/peft_p-tuning_fedavg_345M\",\n",
    "    n_clients=3,\n",
    "    threads=3\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0973cd9",
   "metadata": {},
   "source": [
    "You can visualize the training process using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7fb1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir /tmp/nvflare/nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63421f38",
   "metadata": {},
   "source": [
    "## Results\n",
    "In this scenario, all clients utilize the same validation set, allowing for a direct comparison between the locally p-tuned and federated global models. As anticipated, the FedAvg-trained global model exhibits lower validation loss than the models trained solely on their local datasets. This is because the global model has access to all client datasets and can, consequently, generalize better.\n",
    "\n",
    "![validation loss](./figs/val_loss.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d573a6",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We can use `model.generate()` to run inference after p-tuning the model. \n",
    "Let's define some test examples to feed to the p-tuned model to see its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f09521d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"The products have a low salt and fat content .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"The agreement is valid for four years .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"Diluted EPS rose to EUR3 .68 from EUR0 .50 .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"The company is well positioned in Brazil and Uruguay .\"},\n",
    "    {\"taskname\": \"sentiment\", \"sentence\": \"Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier .\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9bda70",
   "metadata": {},
   "source": [
    "Next, we will load the global model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b59a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from nemo_nvflare.fed_megatron_gpt_prompt_learning_model import FedMegatronGPTPromptLearningModel\n",
    "from nemo_nvflare.utils import load_weights\n",
    "from omegaconf import OmegaConf\n",
    "from nemo.collections.nlp.parts.nlp_overrides import NLPDDPStrategy\n",
    "from pytorch_lightning.plugins.environments import TorchElasticEnvironment\n",
    "\n",
    "# Load model configuration used by one of the clients\n",
    "config = OmegaConf.load(\"jobs/gpt_p-tuning_fedavg_345M/server/config/megatron_gpt_prompt_learning_config.yaml\")\n",
    "\n",
    "# Set GPT model path\n",
    "config.model.language_model_path = \"megatron_gpt_345m.nemo\"\n",
    "\n",
    "# Load task templates\n",
    "config.model.task_templates = OmegaConf.load(\"jobs/gpt_p-tuning_fedavg_345M/server/config/task_templates.json\")\n",
    "\n",
    "# Set task that were learned\n",
    "config.model.new_tasks = [\"sentiment\"]\n",
    "\n",
    "# Setup cluster environment parameters\n",
    "# use torch elastic cluster environment so `create_process_externally` is True\n",
    "# the launcher is set to None. It will not try to spawn new processes.\n",
    "# It won't create the misconfiguration error because of the `interactive session`\n",
    "os.environ[\"LOCAL_RANK\"] = '0'\n",
    "os.environ[\"RANK\"] = '0'\n",
    "os.environ[\"WORLD_SIZE\"] = '1'\n",
    "strategy = NLPDDPStrategy(find_unused_parameters=False, no_ddp_communication_hook=True)\n",
    "plugins = [TorchElasticEnvironment()]\n",
    "\n",
    "# Set up the trainer and load the model that was used for p-tuning\n",
    "trainer = pl.Trainer(plugins=plugins, strategy=strategy, **config.trainer)\n",
    "model = FedMegatronGPTPromptLearningModel(cfg=config.model, trainer=trainer)\n",
    "model.init_prompt_encoder()\n",
    "\n",
    "print(\"Model initialized\", type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396d4c1f",
   "metadata": {},
   "source": [
    "Overwrite the prompt encoder with the best global model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e77ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(\"/tmp/nvflare/nemo/gpt_p-tuning_fedavg_345M/simulate_job/app_server/best_FL_global_model.pt\")\n",
    "global_weights = ckpt[\"model\"]\n",
    "\n",
    "n_loaded = load_weights(model, global_weights, device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "print(f\"Loaded {n_loaded} of {len(global_weights)} weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9948d86f",
   "metadata": {},
   "source": [
    "Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61568dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate(inputs=test_examples, length_params=None)\n",
    "\n",
    "print('The prediction results of some sample queries with the trained model:')\n",
    "for result in response['sentences']:\n",
    "    print(result)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32da342d",
   "metadata": {},
   "source": [
    "The expected output predictions look something like this\n",
    "\n",
    ">      The products have a low salt and fat content . sentiment: neutral\n",
    ">      ------------------------------\n",
    ">      The agreement is valid for four years . sentiment: neutral\n",
    ">      ------------------------------\n",
    ">      Diluted EPS rose to EUR3 .68 from EUR0 .50 . sentiment: positive\n",
    ">      ------------------------------\n",
    ">      The company is well positioned in Brazil and Uruguay . sentiment: positive\n",
    ">      ------------------------------\n",
    ">      Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier . sentiment: negative\n",
    ">      ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5e7035",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
