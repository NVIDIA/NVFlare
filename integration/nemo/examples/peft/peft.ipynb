{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb3307e6",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Fine-Tuning (PEFT) with NeMo\n",
    "\n",
    "In this example, we utilize NeMo's [PEFT](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/nemo_megatron/peft/landing_page.html)\n",
    "methods to showcase how to adapt a large language model (LLM) to \n",
    "a downstream task, such as financial sentiment predictions. \n",
    "\n",
    "With one line configuration change, you can try different PEFT techniques such as [p-tuning](https://arxiv.org/abs/2103.10385), [adapters](https://proceedings.mlr.press/v97/houlsby19a.html), or [LoRA](https://arxiv.org/abs/2106.09685), which add a small number of trainable parameters to the LLM\n",
    "that condition the model to produce the desired output for the downstream task.\n",
    "\n",
    "For more details, see the [PEFT script](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_peft_tuning.py) in NeMo, which we adapt using NVFlare's Lightning client API to run in a federated scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e212b5e1",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "We assume you followed the instructions [here](../../README.md#requirements) \n",
    "to install the NeMo framework and the NeMo-NVFlare package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b079adbe",
   "metadata": {},
   "source": [
    "## Download the pre-trained LLM\n",
    "In this example, we use a `MegatronGPTModel`, a transformer-based language model based on the GPT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23121726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what GPT .nemo models we have available on NGC\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
    "MegatronGPTModel.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52cceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model from NGC\n",
    "import os\n",
    "model_file = \"megatron_gpt_345m.nemo\"\n",
    "if not os.path.isfile(model_file):\n",
    "    !wget \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/megatron_gpt_345m/versions/1/files/$model_file\"\n",
    "else:\n",
    "    print(f\"{model_file} already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f9642a",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "As our downstream task, we will use the [Financial PhraseBank dataset](https://huggingface.co/datasets/financial_phrasebank) for sentiment analysis.\n",
    "\n",
    "The Financial PhraseBank dataset contains the sentiments for financial news headlines from a retail investor's perspective. Further details about the dataset can be found in Malo et al.'s [\"Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts\"](https://arxiv.org/abs/1307.5336).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c9d308",
   "metadata": {},
   "source": [
    "#### 1. Download the preprocessing scripts\n",
    "We use the preprocessing scripts provided by NeMo which can be downloaded from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25afde4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = \"prompt_learning_financial_phrase_bank_preprocessing.py\"\n",
    "if not os.path.isfile(script_name):\n",
    "    !wget -N \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/dataset_processing/nlp/financial_phrase_bank/$script_name\"\n",
    "else:\n",
    "    print(f\"{script_name} already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5362fb4",
   "metadata": {},
   "source": [
    "#### 2. Download the Financial PhraseBank Dataset\n",
    "\n",
    "Download the `FinancialPhraseBank-v1.0.zip` dataset from [here](https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v1.0/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v1.0.zip).\n",
    "\n",
    "Then extract it under `./data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3748dd",
   "metadata": {},
   "source": [
    "#### 3. Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70034bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 prompt_learning_financial_phrase_bank_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6499a968",
   "metadata": {},
   "source": [
    "#### 4. Split the dataset to simulate clients\n",
    "Next, we use three clients to simulate federated learning for p-tuning with NeMo. \n",
    "We use a [Dirichlet sampling](https://arxiv.org/abs/2002.06440) strategy for creating a heterogeneous partition. Smaller values of `alpha` cause higher heterogeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeebc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 10.0\n",
    "!python3 data/split_financial_phrase_data.py --alpha={alpha} --data_path=data/FinancialPhraseBank-v1.0/financial_phrase_bank_train.jsonl --num_clients=3 --out_dir=data/FinancialPhraseBank-v1.0_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39030052",
   "metadata": {},
   "source": [
    "Below are some examples of how the training data is distributed amount the three clients when using different values of `alpha`.\n",
    "<div>\n",
    "<img src=\"./figs/summary_alpha1.0.svg\" alt=\"Label distribution with alpha=1.0\" style=\"width: 400px;\"/>\n",
    "<img src=\"./figs/summary_alpha5.0.svg\" alt=\"Label distribution with alpha=5.0\" style=\"width: 400px;\"/>\n",
    "<img src=\"./figs/summary_alpha10.0.svg\" alt=\"Label distribution with alpha=10.0\" style=\"width: 400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a3d8d2",
   "metadata": {},
   "source": [
    "## Federated learning simulations\n",
    "Next, we are using NVFlare's [simulator](https://nvflare.readthedocs.io/en/latest/user_guide/fl_simulator.html) to simulate each client training on their own dataset locally and all three clients training together using the [FedAvg](https://arxiv.org/abs/1602.05629) algorithm implemented in NVFlare.\n",
    "\n",
    "With this setting, we require a GPU with at least 16GB of memory to run all clients in parallel on the same GPU. \n",
    "If you have multiple GPUs in your system, you can use the `gpu` argument to assign one GPU for each client, e.g., `gpu=\"0,1\"`.\n",
    "\n",
    "We will use NVFlare's job command for each setting to create the configurations needed to train the models based on the [sag_nemo](https://github.com/NVIDIA/NVFlare/blob/main/job_templates/sag_pt_deploy_map/info.md) job template. This template allows the definition of different configurations for each client, which we will use to assign their local training data file to each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06709fc",
   "metadata": {},
   "source": [
    "#### 1. Convert NeMo PEFT script to FL\n",
    "\n",
    "To run NeMo in an FL scenario, we convert the NeMo [PEFT script](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_peft_tuning.py) using the new lightning client API. \n",
    "\n",
    "This conversion can be done with only a few lines of code changes, as highlighted in the figure below:\n",
    "\n",
    "1. Import nvflare lightning api\n",
    "2. Patch your lightning trainer\n",
    "3. (Optionally) validate the current global model\n",
    "4. Train as usually\n",
    "\n",
    "<div>\n",
    "<img src=\"./figs/lightning_client_api.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</div>\n",
    "\n",
    "You can directly use all the PEFT methods implemented in the NeMo script, by changing the value of [peft_scheme](./code/megatron_gpt_peft_tuning_config.yaml) in the client configuration shown below accordingly:\n",
    "* p-tuning\n",
    "* adapter + p-tuning\n",
    "* adapter\n",
    "* LoRa\n",
    "* ia3\n",
    "\n",
    "<div>\n",
    "<img src=\"./figs/peft_config.png\" alt=\"PEFT config\" style=\"width: 700px;\"/>\n",
    "</div>\n",
    "\n",
    "In this example, we will use p-tuning to run the following experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c53d393",
   "metadata": {},
   "source": [
    "#### 1. Local P-Tuning\n",
    "First, we create the job files and modify them to include the data paths for each client and the pre-trained LLM using the `-f` option.\n",
    "Note, the `app_config` options are specific to the app script (`megatron_gpt_peft_tuning.py`) and modify variables in the NeMo config file (`megatron_gpt_peft_tuning_config.yaml`) directly on execution.\n",
    "\n",
    "At this point, we also modify the local number of clients, local epochs and FL rounds to simulate local training. The PEFT method is p-tuning.\n",
    "\n",
    "First, we set the location of NVFlare job templates directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e64a669",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvflare config -jt ../../../../job_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d6c3f6",
   "metadata": {},
   "source": [
    "Then,create the job and configure it for simulating local training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b3b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "peft_scheme=\"model.peft.peft_scheme\\=ptuning\" # can be either ptuning, adapter, lora, or ia3\n",
    "app_script=\"megatron_gpt_peft_tuning.py\"\n",
    "restore_from_path=f\"{os. getcwd()}/megatron_gpt_345m.nemo\"\n",
    "trainer_config=\"trainer.max_steps\\=2000 trainer.val_check_interval\\=10\"\n",
    "val_files=f\"model.data.validation_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\\]\"\n",
    "train_files_prefix=f\"model.data.train_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0_split/site\"\n",
    "\n",
    "!nvflare job create -force -j \"./jobs/peft_p-tuning_local_345M\" -w \"sag_nemo\" -sd \"code\" \\\n",
    "   -f app_1/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-1.jsonl\\]\" \\\n",
    "   -f app_2/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-2.jsonl\\]\" \\\n",
    "   -f app_3/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-3.jsonl\\]\" \\\n",
    "   -f app_server/config_fed_server.conf num_rounds=1 restore_from_path={restore_from_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c318098",
   "metadata": {},
   "source": [
    "Next, simulate each client p-tuning on their local dataset using the FL simulator. To do this, we only run 1 round of FL, with each client running 50 p-tuning epochs on their local dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d9c51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/peft_p-tuning_local_345M\",\n",
    "    workspace=f\"/tmp/nvflare/nemo/peft_p-tuning_local_345M_alpha{alpha}\",\n",
    "    n_clients=3,\n",
    "    threads=3\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d3f353",
   "metadata": {},
   "source": [
    "#### 2. Federated P-Tuning\n",
    "Next, we use the [FedAvg](https://arxiv.org/abs/1602.05629) algorithm to p-tune the model in a federated scenario. First, create and modify the configuration files again. \n",
    "This time, we increase the number of FL rounds and decrease the number of local epochs per round to match the federated scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d13248",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "peft_scheme=\"model.peft.peft_scheme\\=ptuning\" # can be either ptuning, adapter, lora, or ia3\n",
    "app_script=\"megatron_gpt_peft_tuning.py\"\n",
    "restore_from_path=f\"{os. getcwd()}/megatron_gpt_345m.nemo\"\n",
    "trainer_config=\"trainer.max_steps\\=200 trainer.val_check_interval\\=10\"\n",
    "val_files=f\"model.data.validation_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\\]\"\n",
    "train_files_prefix=f\"model.data.train_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0_split/site\"\n",
    "\n",
    "!nvflare job create -force -j \"./jobs/peft_p-tuning_fedavg_345M\" -w \"sag_nemo\" -sd \"code\" \\\n",
    "   -f app_1/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-1.jsonl\\]\" \\\n",
    "   -f app_2/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-2.jsonl\\]\" \\\n",
    "   -f app_3/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-3.jsonl\\]\" \\\n",
    "   -f app_server/config_fed_server.conf num_rounds=10 restore_from_path={restore_from_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7498a00f",
   "metadata": {},
   "source": [
    "Next, simulate the federated p-tuning using FedAvg. Here, each client p-tunes for one local epoch before sending their local model updates to the server for aggregation. This is repeated for 50 FL rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b851d4ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=\"jobs/peft_p-tuning_fedavg_345M\",\n",
    "    workspace=f\"/tmp/nvflare/nemo/peft_p-tuning_fedavg_345M_alpha{alpha}\",\n",
    "    n_clients=3,\n",
    "    threads=3\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a34401",
   "metadata": {},
   "source": [
    "You can visualize the training process using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir /tmp/nvflare/nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fa229",
   "metadata": {},
   "source": [
    "## Results\n",
    "In this scenario, all clients utilize the same validation set, allowing for a direct comparison between the locally p-tuned and federated global models. As anticipated, the FedAvg-trained models achieve a higher overall mean accuracy than those trained solely on their local datasets for different values of `alpha`. This is because the global model has access to all client datasets and can, consequently, generalize better, especially in settings of higher client data heterogeneity.\n",
    "\n",
    "Below are some examples of how the training data is distributed amount the three clients when using different values of `alpha`.\n",
    "<div>\n",
    "<img src=\"./figs/val_accuracy_alpha1.0.svg\" alt=\"Validation accuracy with alpha=1.0\" style=\"width: 400px;\"/>\n",
    "<img src=\"./figs/val_accuracy_alpha5.0.svg\" alt=\"Validation accuracy with alpha=5.0\" style=\"width: 400px;\"/>\n",
    "<img src=\"./figs/val_accuracy_alpha10.0.svg\" alt=\"Validation accuracy with alpha=10.0\" style=\"width: 400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974553f",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We can use `model.generate()` to run inference after p-tuning the model. \n",
    "Let's define some test examples to feed to the p-tuned model to see its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c94804",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_examples = [\"The products have a low salt and fat content .\",\n",
    "    \"The agreement is valid for four years .\",\n",
    "    \"Diluted EPS rose to EUR3 .68 from EUR0 .50 .\",\n",
    "    \"The company is well positioned in Brazil and Uruguay .\",\n",
    "    \"Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier .\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc882a",
   "metadata": {},
   "source": [
    "First, we need to convert the best global PEFT model into a NeMo ckpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nemo_nvflare.utils import convert_global_to_ckpt\n",
    "server_workspace = f\"/tmp/nvflare/nemo/peft_p-tuning_fedavg_345M_alpha{alpha}/simulate_job/app_server\"\n",
    "global_model_filepath = os.path.join(server_workspace, \"best_FL_global_model.pt\")\n",
    "assert global_model_filepath.endswith(\".pt\")\n",
    "ckpt_path = global_model_filepath.replace(\".pt\", \".ckpt\")\n",
    "convert_global_to_ckpt(global_model_filepath, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d5beac",
   "metadata": {},
   "source": [
    "Next, we will load the global model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7dd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_sft_model import MegatronGPTSFTModel\n",
    "from nemo.collections.nlp.parts.megatron_trainer_builder import MegatronLMPPTrainerBuilder\n",
    "from nemo.collections.nlp.parts.peft_config import PEFT_CONFIG_MAP\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Load model configuration inference of the global model\n",
    "cfg = OmegaConf.load(\"code/megatron_gpt_peft_fl_eval_config.yaml\")\n",
    "\n",
    "# Build trainer\n",
    "trainer = MegatronLMPPTrainerBuilder(cfg).create_trainer()\n",
    "\n",
    "# Set restore from paths with pre-trained model(s)\n",
    "cfg.model.restore_from_path = \"megatron_gpt_345m.nemo\"\n",
    "\n",
    "# Set the global peft weights\n",
    "cfg.model.peft.restore_from_path = ckpt_path\n",
    "\n",
    "# Set some dummy data file names (which will not be used and do not need to exist)\n",
    "cfg.model.data.train_ds.file_names = [\"dummy.jsonl\"]\n",
    "cfg.model.data.validation_ds.file_names = [\"dummy.jsonl\"]\n",
    "\n",
    "model_cfg = MegatronGPTSFTModel.merge_cfg_with(cfg.model.restore_from_path, cfg)\n",
    "model = MegatronGPTSFTModel.restore_from(cfg.model.restore_from_path, model_cfg, trainer=trainer)\n",
    "peft_cfg_cls = PEFT_CONFIG_MAP[cfg.model.peft.peft_scheme]\n",
    "\n",
    "print(\"PEFT Weights will be loaded from\", cfg.model.peft.restore_from_path)\n",
    "model.load_adapters(cfg.model.peft.restore_from_path, peft_cfg_cls(model_cfg))\n",
    "model.freeze()\n",
    "\n",
    "print(\"Model initialized\", type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16847e89",
   "metadata": {},
   "source": [
    "Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aaa88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the sampling parameters as needed\n",
    "sampling_params = {\n",
    "    \"use_greedy\": True,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"add_BOS\": True,\n",
    "    \"all_probs\": False,\n",
    "    \"compute_logprob\": False,\n",
    "    \"end_strings\": [\"<|endoftext|>\", \"<extra_id_1>\"],\n",
    "}\n",
    "\n",
    "print('The prediction results of some sample queries with the trained model:')\n",
    "for test_example in test_examples:\n",
    "    response = model.generate(inputs=[test_example], length_params=None, sampling_params=sampling_params)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(response['sentences'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8045de44",
   "metadata": {},
   "source": [
    "The desired output of a well-trained model looks something like this. Note, the test sentences do not include ground truth labels.\n",
    "\n",
    ">      The products have a low salt and fat content . neutral\n",
    ">      ------------------------------\n",
    ">      The agreement is valid for four years . neutral\n",
    ">      ------------------------------\n",
    ">      Diluted EPS rose to EUR3 .68 from EUR0 .50 . positive\n",
    ">      ------------------------------\n",
    ">      The company is well positioned in Brazil and Uruguay . positive\n",
    ">      ------------------------------\n",
    ">      Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier . negative\n",
    ">      ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2452a50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
