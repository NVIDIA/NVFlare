{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c534975",
   "metadata": {},
   "source": [
    "# Parameter-Efficient Fine-Tuning (PEFT) with NeMo\n",
    "\n",
    "In this example, we utilize NeMo's [PEFT](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/main/nlp/nemo_megatron/peft/landing_page.html)\n",
    "methods to showcase how to adapt a large language model (LLM) to \n",
    "a downstream task, such as financial sentiment predictions. \n",
    "\n",
    "With one line configuration change, you can try different PEFT techniques such as [p-tuning](https://arxiv.org/abs/2103.10385), [adapters](https://proceedings.mlr.press/v97/houlsby19a.html), or [LoRA](https://arxiv.org/abs/2106.09685), which add a small number of trainable parameters to the LLM\n",
    "that condition the model to produce the desired output for the downstream task.\n",
    "\n",
    "For more details, see the [PEFT script](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_peft_tuning.py) in NeMo, which we adapt using NVFlare's Lightning client API to run in a federated scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513eb148",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "We assume you followed the instructions [here](../../README.md#requirements) \n",
    "to install the NeMo framework and the NeMo-NVFlare package. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97927a",
   "metadata": {},
   "source": [
    "## Download the pre-trained LLM\n",
    "In this example, we use a `MegatronGPTModel`, a transformer-based language model based on the GPT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f6c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what GPT .nemo models we have available on NGC\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_model import MegatronGPTModel\n",
    "MegatronGPTModel.list_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2036e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model from NGC\n",
    "import os\n",
    "model_file = \"megatron_gpt_345m.nemo\"\n",
    "if not os.path.isfile(model_file):\n",
    "    !wget \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/megatron_gpt_345m/versions/1/files/$model_file\"\n",
    "else:\n",
    "    print(f\"{model_file} already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f48638",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "As our downstream task, we will use the [Financial PhraseBank dataset](https://huggingface.co/datasets/financial_phrasebank) for sentiment analysis.\n",
    "\n",
    "The Financial PhraseBank dataset contains the sentiments for financial news headlines from a retail investor's perspective. Further details about the dataset can be found in Malo et al.'s [\"Good Debt or Bad Debt: Detecting Semantic Orientations in Economic Texts\"](https://arxiv.org/abs/1307.5336).\n",
    "\n",
    "We can configure the prompt template used by NeMo to solve this downstream task by setting `prompt_template: \"{sentence} sentiment: {label}\"` in [megatron_gpt_peft_tuning_config.yaml](./code/megatron_gpt_peft_tuning_config.yaml) accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd0470",
   "metadata": {},
   "source": [
    "#### 1. Download the preprocessing scripts\n",
    "We use the preprocessing scripts provided by NeMo which can be downloaded from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37039ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_name = \"prompt_learning_financial_phrase_bank_preprocessing.py\"\n",
    "if not os.path.isfile(script_name):\n",
    "    !wget -N \"https://raw.githubusercontent.com/NVIDIA/NeMo/main/scripts/dataset_processing/nlp/financial_phrase_bank/$script_name\"\n",
    "else:\n",
    "    print(f\"{script_name} already downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b1a07b",
   "metadata": {},
   "source": [
    "#### 2. Download the Financial PhraseBank Dataset\n",
    "\n",
    "Download the `FinancialPhraseBank-v1.0.zip` dataset from [here](https://www.researchgate.net/profile/Pekka_Malo/publication/251231364_FinancialPhraseBank-v1.0/data/0c96051eee4fb1d56e000000/FinancialPhraseBank-v1.0.zip).\n",
    "\n",
    "Then extract it under `./data`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335899e",
   "metadata": {},
   "source": [
    "#### 3. Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc66ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 prompt_learning_financial_phrase_bank_preprocessing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365a58c8",
   "metadata": {},
   "source": [
    "#### 4. Split the dataset to simulate clients\n",
    "Next, we use three clients to simulate federated learning for running PEFT with NeMo. \n",
    "We use a [Dirichlet sampling](https://arxiv.org/abs/2002.06440) strategy for creating a heterogeneous partition. Smaller values of `alpha` cause higher heterogeneity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9214af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.split_financial_phrase_data import clean_memmap\n",
    "\n",
    "# Clean NeMo memmap data before running a new data split\n",
    "clean_memmap(\"./data\")\n",
    "\n",
    "# Split the data\n",
    "alpha = 10.0\n",
    "assert isinstance(alpha, float), \"Expecting float value in filepath names used below.\"\n",
    "!python3 data/split_financial_phrase_data.py --alpha={alpha} --data_path=data/FinancialPhraseBank-v1.0/financial_phrase_bank_train.jsonl --num_clients=3 --out_dir=data/FinancialPhraseBank-v1.0_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc85565d",
   "metadata": {},
   "source": [
    "Below are some examples of how the training data is distributed amount the three clients when using different values of `alpha`.\n",
    "<div>\n",
    "<img src=\"./figs/summary_alpha1.0.svg\" alt=\"Label distribution with alpha=1.0\" style=\"width: 400px;\"/>\n",
    "<img src=\"./figs/summary_alpha5.0.svg\" alt=\"Label distribution with alpha=5.0\" style=\"width: 400px;\"/>\n",
    "<img src=\"./figs/summary_alpha10.0.svg\" alt=\"Label distribution with alpha=10.0\" style=\"width: 400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eea187a",
   "metadata": {},
   "source": [
    "## Federated learning simulations\n",
    "Next, we are using NVFlare's [simulator](https://nvflare.readthedocs.io/en/latest/user_guide/fl_simulator.html) to simulate each client training on their own dataset locally and all three clients training together using the [FedAvg](https://arxiv.org/abs/1602.05629) algorithm implemented in NVFlare.\n",
    "\n",
    "With this setting, we require a GPU with at least 24GB of memory to run all clients in parallel on the same GPU. \n",
    "If you have multiple GPUs in your system, you can use the `gpu` argument to assign one GPU for each client, e.g., `gpu=\"0,1\"`.\n",
    "\n",
    "We will use NVFlare's job command for each setting to create the configurations needed to train the models based on the [sag_nemo](https://github.com/NVIDIA/NVFlare/blob/main/job_templates/sag_nemo/info.md) job template. This template allows the definition of different configurations for each client, which we will use to assign their local training data file to each of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa23b7c7",
   "metadata": {},
   "source": [
    "#### 1. Convert NeMo PEFT script to FL\n",
    "\n",
    "To run NeMo in an FL scenario, we convert the NeMo [PEFT script](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/language_modeling/tuning/megatron_gpt_peft_tuning.py) using the new lightning client API. \n",
    "\n",
    "This conversion can be done with only a few lines of code changes, as highlighted in the figure below:\n",
    "\n",
    "1. Import nvflare lightning api\n",
    "2. Patch your lightning trainer\n",
    "3. (Optionally) validate the current global model\n",
    "4. Train as usually\n",
    "\n",
    "<div>\n",
    "<img src=\"./figs/lightning_client_api.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "</div>\n",
    "\n",
    "You can directly use all the PEFT methods implemented in the NeMo script, by changing the value of [peft_scheme](./code/megatron_gpt_peft_tuning_config.yaml) in the client configuration shown below accordingly:\n",
    "* p-tuning\n",
    "* adapter + p-tuning\n",
    "* adapter\n",
    "* LoRa\n",
    "* ia3\n",
    "\n",
    "<div>\n",
    "<img src=\"./figs/peft_config.png\" alt=\"PEFT config\" style=\"width: 700px;\"/>\n",
    "</div>\n",
    "\n",
    "In this example, we will use LoRA to run the following experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b07067",
   "metadata": {},
   "source": [
    "#### 1. Local training\n",
    "First, we create the job files and modify them to include the data paths for each client and the pre-trained LLM using the `-f` option.\n",
    "Note, the `app_config` options are specific to the app script (`megatron_gpt_peft_tuning.py`) and modify variables in the NeMo config file (`megatron_gpt_peft_tuning_config.yaml`) directly on execution.\n",
    "\n",
    "At this point, we also modify the local number of clients, local steps and FL rounds to simulate local training. The PEFT method is [LoRA](https://arxiv.org/abs/2106.09685).\n",
    "\n",
    "First, we set the location of NVFlare job templates directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e001c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvflare config -jt ../../../../job_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3528af2",
   "metadata": {},
   "source": [
    "Then, create the job and configure it for simulating local training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9905ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "peft_scheme=\"lora\" # can be either ptuning, adapter, lora, or ia3\n",
    "\n",
    "# Common configs\n",
    "peft_scheme_arg=f\"model.peft.peft_scheme\\={peft_scheme}\" \n",
    "app_script=\"megatron_gpt_peft_tuning.py\"\n",
    "restore_from_path=f\"{os. getcwd()}/megatron_gpt_345m.nemo\"\n",
    "val_files=f\"model.data.validation_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0/financial_phrase_bank_val.jsonl\\]\"\n",
    "train_files_prefix=f\"model.data.train_ds.file_names\\=\\[{os. getcwd()}/data/FinancialPhraseBank-v1.0_split/alpha{alpha}_site\"\n",
    "\n",
    "# Simulate local training\n",
    "num_rounds=1\n",
    "trainer_config=\"trainer.max_steps\\=1000 trainer.val_check_interval\\=100\"\n",
    "\n",
    "!nvflare job create -force -j \"./jobs/peft_{peft_scheme}_local_345M\" -w \"sag_nemo\" -sd \"code\" \\\n",
    "   -f app_1/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme_arg} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-1.jsonl\\]\" \\\n",
    "   -f app_2/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme_arg} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-2.jsonl\\]\" \\\n",
    "   -f app_3/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme_arg} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-3.jsonl\\]\" \\\n",
    "   -f app_server/config_fed_server.conf num_rounds={num_rounds} restore_from_path={restore_from_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b7d71",
   "metadata": {},
   "source": [
    "Next, simulate each client training on their local dataset using the FL simulator. To do this, we only run 1 round of FL, with each client running 1000 steps on their local dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef104c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=f\"jobs/peft_{peft_scheme}_local_345M\",\n",
    "    workspace=f\"/tmp/nvflare/nemo/peft_{peft_scheme}_local_345M_alpha{alpha}\",\n",
    "    n_clients=3,\n",
    "    threads=3\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf7bed",
   "metadata": {},
   "source": [
    "#### 2. Federated training\n",
    "Next, we use the [FedAvg](https://arxiv.org/abs/1602.05629) algorithm to adapt the model in a federated scenario. First, create and modify the configuration files again. \n",
    "This time, we increase the number of FL rounds and decrease the number of local steps per round to match the federated scenario. \n",
    "\n",
    "Here, each client runs LoRA for one 200 steps before sending their local model updates to the server for aggregation. This is repeated for 5 FL rounds. All the other parameters are the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782af9c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# FedAvg setting\n",
    "num_rounds=5\n",
    "trainer_config=\"trainer.max_steps\\=200 trainer.val_check_interval\\=100\"\n",
    "\n",
    "!nvflare job create -force -j \"./jobs/peft_{peft_scheme}_fedavg_345M\" -w \"sag_nemo\" -sd \"code\" \\\n",
    "   -f app_1/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme_arg} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-1.jsonl\\]\" \\\n",
    "   -f app_2/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme_arg} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-2.jsonl\\]\" \\\n",
    "   -f app_3/config_fed_client.conf app_script={app_script} app_config=\"{peft_scheme_arg} model.restore_from_path\\={restore_from_path} {trainer_config} {val_files} {train_files_prefix}-3.jsonl\\]\" \\\n",
    "   -f app_server/config_fed_server.conf num_rounds={num_rounds} restore_from_path={restore_from_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41088905",
   "metadata": {},
   "source": [
    "Next, simulate the federated training using FedAvg. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00109b1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nvflare import SimulatorRunner    \n",
    "\n",
    "simulator = SimulatorRunner(\n",
    "    job_folder=f\"jobs/peft_{peft_scheme}_fedavg_345M\",\n",
    "    workspace=f\"/tmp/nvflare/nemo/peft_{peft_scheme}_fedavg_345M_alpha{alpha}\",\n",
    "    n_clients=3,\n",
    "    threads=3\n",
    ")\n",
    "run_status = simulator.run()\n",
    "print(\"Simulator finished with run_status\", run_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d8d656",
   "metadata": {},
   "source": [
    "You can visualize the training process using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6755b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir /tmp/nvflare/nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c35f89",
   "metadata": {},
   "source": [
    "## Results\n",
    "In this scenario, all clients utilize the same validation set, allowing for a direct comparison between the locally p-tuned and federated global models. As anticipated, the FedAvg-trained models achieve a higher overall mean accuracy than those trained solely on their local datasets for different values of `alpha`. This is because the global model has access to all client datasets and can, consequently, generalize better, especially in settings of higher client data heterogeneity.\n",
    "\n",
    "Below are some examples of how the training data is distributed among the three clients when using different values of `alpha`. The lines show the mean accuracy of local models during training and shaded areas indicate the 95% confidence interval. \n",
    "<div>\n",
    "<img src=\"./figs/val_accuracy_alpha1.0.svg\" alt=\"Validation accuracy with alpha=1.0\" style=\"width: 400px;\"/>\n",
    "<img src=\"./figs/val_accuracy_alpha5.0.svg\" alt=\"Validation accuracy with alpha=5.0\" style=\"width: 400px;\"/>\n",
    "<img src=\"./figs/val_accuracy_alpha10.0.svg\" alt=\"Validation accuracy with alpha=10.0\" style=\"width: 400px;\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7174a47a",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We can use `model.generate()` to run inference after adapting the model. \n",
    "Let's define some test examples to feed to the tuned model to see its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \" sentiment:\"\n",
    "test_examples = [f\"The products have a low salt and fat content .{prompt}\",\n",
    "    f\"The agreement is valid for four years .{prompt}\",\n",
    "    f\"Diluted EPS rose to EUR3 .68 from EUR0 .50 .{prompt}\",\n",
    "    f\"The company is well positioned in Brazil and Uruguay .{prompt}\",\n",
    "    f\"Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier .{prompt}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4ed67",
   "metadata": {},
   "source": [
    "First, we need to convert the best global PEFT model into a NeMo ckpt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f93b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nemo_nvflare.utils import convert_global_to_ckpt\n",
    "server_workspace = f\"/tmp/nvflare/nemo/peft_{peft_scheme}_fedavg_345M_alpha{alpha}/simulate_job/app_server\"\n",
    "global_model_filepath = os.path.join(server_workspace, \"best_FL_global_model.pt\")\n",
    "assert global_model_filepath.endswith(\".pt\")\n",
    "ckpt_path = global_model_filepath.replace(\".pt\", \".ckpt\")\n",
    "convert_global_to_ckpt(global_model_filepath, ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311edbd",
   "metadata": {},
   "source": [
    "Next, we will load the global model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b07ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.collections.nlp.models.language_modeling.megatron_gpt_sft_model import MegatronGPTSFTModel\n",
    "from nemo.collections.nlp.parts.megatron_trainer_builder import MegatronLMPPTrainerBuilder\n",
    "from nemo.collections.nlp.parts.peft_config import PEFT_CONFIG_MAP\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Load model configuration inference of the global model\n",
    "cfg = OmegaConf.load(\"code/megatron_gpt_peft_fl_eval_config.yaml\")\n",
    "\n",
    "# Build trainer\n",
    "trainer = MegatronLMPPTrainerBuilder(cfg).create_trainer()\n",
    "\n",
    "# Set restore from paths with pre-trained model(s)\n",
    "cfg.model.restore_from_path = \"megatron_gpt_345m.nemo\"\n",
    "\n",
    "# Set the global peft weights\n",
    "cfg.model.peft.restore_from_path = ckpt_path\n",
    "\n",
    "model_cfg = MegatronGPTSFTModel.merge_cfg_with(cfg.model.restore_from_path, cfg)\n",
    "model = MegatronGPTSFTModel.restore_from(cfg.model.restore_from_path, model_cfg, trainer=trainer)\n",
    "peft_cfg_cls = PEFT_CONFIG_MAP[cfg.model.peft.peft_scheme]\n",
    "\n",
    "print(\"PEFT Weights will be loaded from\", cfg.model.peft.restore_from_path)\n",
    "model.load_adapters(cfg.model.peft.restore_from_path, peft_cfg_cls(model_cfg))\n",
    "model.freeze()\n",
    "\n",
    "print(\"Model initialized\", type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b00b36",
   "metadata": {},
   "source": [
    "Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03a073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the sampling parameters as needed\n",
    "sampling_params = {\n",
    "    \"use_greedy\": True,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_k\": 0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"add_BOS\": False,\n",
    "    \"all_probs\": False,\n",
    "    \"compute_logprob\": False,\n",
    "    \"end_strings\": [\"<|endoftext|>\", \"<extra_id_1>\"],\n",
    "}\n",
    "\n",
    "response = model.generate(inputs=test_examples, length_params=None, sampling_params=sampling_params)\n",
    "\n",
    "print('The prediction results of some sample queries with the trained model:')\n",
    "for result in response['sentences']:\n",
    "    print(\"-\" * 30)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8fd7c",
   "metadata": {},
   "source": [
    "The expected output of a well-trained model looks something like this. Note, the test sentences do not include ground truth labels.\n",
    "\n",
    ">      The products have a low salt and fat content . sentiment: neutral\n",
    ">      ------------------------------\n",
    ">      The agreement is valid for four years . sentiment: neutral\n",
    ">      ------------------------------\n",
    ">      Diluted EPS rose to EUR3 .68 from EUR0 .50 . sentiment: positive\n",
    ">      ------------------------------\n",
    ">      The company is well positioned in Brazil and Uruguay . sentiment: positive\n",
    ">      ------------------------------\n",
    ">      Profit before taxes decreased by 9 % to EUR 187.8 mn in the first nine months of 2008 , compared to EUR 207.1 mn a year earlier . sentiment: negative\n",
    ">      ------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7aaaa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
