{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbeca9a0",
   "metadata": {},
   "source": [
    "## FLARE and MONAI Integration\n",
    "[MONAI](https://monai.io/) allows the definition of AI models using the \"bundle\" concept. It allows for easy experimentation and sharing of models that have been developed using MONAI. Using the bundle configurations, we can use MONAI's MonaiAlgo (the implementation of ClientAlgo) to execute a bundle model in a federated scenario using NVFlare.\n",
    "\n",
    "<img src=\"images/MONAI-FL.svg\" alt=\"MONAI FL Integration\" width=400/>\n",
    "\n",
    "You can find the code for the [MONAI Integration code](https://github.com/NVIDIA/NVFlare/blob/main/integration/monai/README.md) in the `integration` directory of the NVFlare GitHub.\n",
    "\n",
    "There is also an example that walks through preparing the environment and input datasets, and running the [MONAI Spleen CT Segmentation example](https://github.com/NVIDIA/NVFlare/tree/main/integration/monai/examples/spleen_ct_segmentation_local) using a locally provisioned secure deployment.  We'll walk through that here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a86db2",
   "metadata": {},
   "source": [
    "## Preparing the notebook environment\n",
    "Before you proceeding on this notebook, please make sure you have activated the virtual environment nvflare_example.\n",
    "And have installed monai_nvflare inside the environment.\n",
    "\n",
    "Please make sure to register the kernel.\n",
    "\n",
    "You can follow instructions in [Examples Readme](https://github.com/NVIDIA/NVFlare/blob/main/examples/README.md#set-up-a-virtual-environment)\n",
    "\n",
    "Please also make sure to switch the top right kernel to \"nvflare_example\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c13c797-b918-48bd-90ce-2b2f09a2dde9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install monai[all] for this notebook and verify monai and nvflare installed\n",
    "%pip install \"monai[all]>=1.3.1\"\n",
    "%pip list | grep \"nvflare\"\n",
    "%pip list | grep \"monai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e37e4c-7d66-4498-b84b-447c57caea37",
   "metadata": {},
   "source": [
    "## Preparing the POC environment\n",
    "Before running the POC, there are a couple important environment variables that should be set.\n",
    "\n",
    "First, to simplify deploying the example apps in the NVFlare GitHub repo, you can set `NVFLARE_HOME` to the root of the GitHub clone.  In this case, we've cloned to our current working directory, so we can set it as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d5df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "workdir=os.getcwd()\n",
    "%env NVFLARE_HOME={workdir}/../../../NVFlare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b729aba",
   "metadata": {},
   "source": [
    "By default, POC mode uses a temporary workspace in /tmp/nvflare/poc.  We would like to keep the workspace within our working directory, so let's create a poc_workspace dir.  We can then use the `NVFLARE_POC_WORKSPACE` variable to define this as the POC workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaff2e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean up monai_poc_workspace\n",
    "!rm -rf monai_poc_workspace\n",
    "!mkdir monai_poc_workspace\n",
    "%env NVFLARE_POC_WORKSPACE={workdir}/monai_poc_workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5929ea",
   "metadata": {},
   "source": [
    "### Preparing the POC workspace\n",
    "\n",
    "Now that we've configured our POC environment, we can prepare the POC workspace.  By default, this will generate POC packages for a server and two clients.\n",
    "\n",
    "(Note that `nvflare poc prepare` prompts you to create the workspace.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26423809-aa65-498a-aa00-02cd7ea39cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!printf '%s\\n' y | nvflare poc prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e0336",
   "metadata": {},
   "source": [
    "Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d15a78e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -aR monai_poc_workspace | grep \":$\" | sed -e 's/:$//' -e 's/[^-][^\\/]*\\//──/g' -e 's/─/├/' -e '$s/├/└/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24692516",
   "metadata": {},
   "source": [
    "### Running the POC Deployment\n",
    "\n",
    "When starting the POC deployment, it's necessary to use a separate terminal since the `nvflare poc start` command will run  in the foreground emitting output from the server and any connected clients.\n",
    "\n",
    "Also note that `nvflare poc start` starts all participants, including the admin console.  It's often nice to start server and clients separately so that we can interact with the deployment using a separate admin console.  To do this, we'll pass the `-ex admin` arg to exclude the admin client from the initial POC run and use the FLARE API to run admin commands separately.\n",
    "\n",
    "So pop open the launcher, launch a terminal, and run (remembering to set the NVFLARE_POC_WORKSPACE and NVFLARE_HOME vars!):\n",
    "\n",
    "```shell\n",
    "export NVFLARE_POC_WORKSPACE=$(pwd -P)/monai_poc_workspace\n",
    "export NVFLARE_HOME=$(pwd -P)/../../../NVFlare\n",
    "nvflare poc start -ex admin@nvidia.com\n",
    "```\n",
    "\n",
    "Keep this terminal open so you can continue to watch server and client output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220e2773",
   "metadata": {},
   "source": [
    "## Setting up the MONAI example\n",
    "\n",
    "The MONAI example for a distributed deployment can be found in the `NVFlare/integration/monai/examples/spleen_ct_segmentation_local` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedcb4a",
   "metadata": {},
   "source": [
    "We now need to download the MONAI bundle that contains the `spleen_ct_segmentation` model and configuration.  We can do this using the MONAI bundle download built-in script.\n",
    "\n",
    "This model and configuration will be part of the FLARE spleen_ct_segmentation_local app that is deployed to the FLARE clients, so we'll provde the job directory as the bundle download path.\n",
    "\n",
    "We will also download the example spleen dataset and push it to a directory accessible by the containers running the FLARE clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc9fce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env JOB_DIR=examples/spleen_ct_segmentation_local/jobs/spleen_ct_segmentation_local\n",
    "!python3 -m monai.bundle download \\\n",
    "    --name \"spleen_ct_segmentation\" \\\n",
    "    --version \"0.5.7\" \\\n",
    "    --bundle_dir ./${JOB_DIR}/app/config\n",
    "!if [ ! -d data/Task09_Spleen ]; then \\\n",
    "    python3 examples/spleen_ct_segmentation_local/download_spleen_dataset.py; fi\n",
    "#!for site in site-1 site-2; do mkdir monai_poc_workspace/${site}/data; cp -r data/Task09_Spleen monai_poc_workspace/${site}/data; done\n",
    "!sed -i \"s,/workspace/data/Task09_Spleen,${PWD}/data/Task09_Spleen,g\" ${JOB_DIR}/app/config/spleen_ct_segmentation/configs/train.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3576f08",
   "metadata": {},
   "source": [
    "## Running the MONAI app\n",
    "Now that we have the example spleen segmentation app in `examples/spleen_ct_segmentation_local` and the MONAI bundle config in the `job/app/config` subdirectory, we can look at how we configure the job parameters with FLARE MONAI FL integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68b24b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -aR examples/spleen_ct_segmentation_local | grep \":$\" | sed -e 's/:$//' -e 's/[^-][^\\/]*\\//──/g' -e 's/─/├/' -e '$s/├/└/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448c5dba",
   "metadata": {},
   "source": [
    "As with other FLARE applications, we have the usual `config_fed_client.json` and `config_fed_server.json` configurations.  We also have a set of MONAI configs from the MONAI Bundle in `job/app/config/spleen_ct_segmentation/configs`.\n",
    "\n",
    "The `job/app/config/spleen_ct_segmentation/configs/train.json` defines the MONAI training parameters, for example inputs/output and network definition (you can navigate in the file browser and open the train.json file in the Editor to see the full configuration:\n",
    "\n",
    "```json\n",
    "    \"bundle_root\": \"/workspace/data/spleen_ct_segmentation\",\n",
    "    \"ckpt_dir\": \"$@bundle_root + '/models'\",\n",
    "    \"output_dir\": \"$@bundle_root + '/eval'\",\n",
    "    \"dataset_dir\": \"/workspace/data/Task09_Spleen\",\n",
    "    \"images\": \"$list(sorted(glob.glob(@dataset_dir + '/imagesTr/*.nii.gz')))\",\n",
    "    \"labels\": \"$list(sorted(glob.glob(@dataset_dir + '/labelsTr/*.nii.gz')))\",\n",
    "    \"val_interval\": 5,\n",
    "    \"device\": \"$torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\",\n",
    "    \"network_def\": {\n",
    "        \"_target_\": \"UNet\",\n",
    "        \"spatial_dims\": 3,\n",
    "        \"in_channels\": 1,\n",
    "        \"out_channels\": 2,\n",
    "        \"channels\": [\n",
    "            16,\n",
    "            32,\n",
    "            64,\n",
    "            128,\n",
    "            256\n",
    "        ],\n",
    "        \"strides\": [\n",
    "            2,\n",
    "            2,\n",
    "            2,\n",
    "            2\n",
    "        ],\n",
    "        \"num_res_units\": 2,\n",
    "        \"norm\": \"batch\"\n",
    "    },\n",
    "```\n",
    "\n",
    "The MONAI Client executor uses this config along with the parameters in `config_fed_client.json` and `config_fed_server.json` to define the FLARE app.  For example, the client config defines this executor and the number of local training epochs:\n",
    "```json\n",
    "{\n",
    "  \"format_version\": 2,\n",
    "\n",
    "  \"executors\": [\n",
    "    {\n",
    "      \"tasks\": [\n",
    "        \"train\", \"submit_model\", \"validate\"\n",
    "      ],\n",
    "      \"executor\": {\n",
    "        \"id\": \"executor\",\n",
    "        \"path\": \"monai_nvflare.client_algo_executor.ClientAlgoExecutor\",\n",
    "        \"args\": {\n",
    "          \"client_algo_id\": \"client_algo\",\n",
    "          \"key_metric\": \"val_mean_dice\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "\n",
    "  \"task_result_filters\": [\n",
    "  ],\n",
    "  \"task_data_filters\": [\n",
    "  ],\n",
    "\n",
    "  \"components\": [\n",
    "    {\n",
    "      \"id\": \"client_algo\",\n",
    "      \"path\": \"monai.fl.client.MonaiAlgo\",\n",
    "      \"args\": {\n",
    "        \"bundle_root\": \"config/spleen_ct_segmentation\",\n",
    "        \"local_epochs\": 10\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Similarly, the server config defines the number of global rounds:\n",
    "```json\n",
    "{\n",
    "  \"format_version\": 2,\n",
    "\n",
    "  \"min_clients\": 2,\n",
    "  \"num_rounds\": 100,\n",
    "\n",
    "  \"task_data_filters\": [],\n",
    "  \"task_result_filters\": [],\n",
    "  \"components\": [\n",
    "\n",
    "  ... <snip> ...\n",
    "  \n",
    "  ],\n",
    "  \"workflows\": [\n",
    "      {\n",
    "        \"id\": \"scatter_gather_ctl\",\n",
    "        \"name\": \"ScatterAndGather\",\n",
    "        \"args\": {\n",
    "            \"min_clients\" : \"{min_clients}\",\n",
    "            \"num_rounds\" : \"{num_rounds}\",\n",
    "            \"start_round\": 0,\n",
    "            \"wait_time_after_min_received\": 10,\n",
    "            \"aggregator_id\": \"aggregator\",\n",
    "            \"persistor_id\": \"persistor\",\n",
    "            \"shareable_generator_id\": \"shareable_generator\",\n",
    "            \"train_task_name\": \"train\",\n",
    "            \"train_timeout\": 0\n",
    "        }\n",
    "  ...\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "The server is configured to for 100 rounds of training, which will take a long time to complete.  Use the file browser to the left to navigate to `examples/spleen_ct_segmentation_local/job/app/config/` and right-click to open `config_fed_server.json` in the Editor.  Change `\"num_rounds\": 100` to something much smaller, like `\"num_rounds\": 2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807e6f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Using the FLARE API to connect and submit the MONAI app\n",
    "\n",
    "The admin directory contains the startup script for the FLARE Console, which can be used interactively to operate a running FLARE deployment.  A FLARE deployment can also be managed using the FLARE API, which will use the configuration in the admin directory to connect to the FLARE server.  Since we already have the server and clients running in the background from the above terminal commands, we'll use FLARE API to start a new admin session and connect.\n",
    "\n",
    "To get started, we need to import the FLARE API class and initialize session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6ac43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nvflare.fuel.flare_api.flare_api import new_secure_session\n",
    "\n",
    "poc_prepared_dir = os.path.join(workdir, \"monai_poc_workspace\", \"example_project/prod_00\")\n",
    "admin_dir = os.path.join(poc_prepared_dir, \"admin@nvidia.com\")\n",
    "admin_session = new_secure_session(\"admin@nvidia.com\", startup_kit_location = admin_dir)\n",
    "print(admin_session.get_system_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2d192b",
   "metadata": {},
   "source": [
    "\n",
    "Now with the session connected, we can use the job we've just configured to submit and run the MONAI application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d4a3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_job_config = f\"{os.getcwd()}/examples/spleen_ct_segmentation_local/jobs/spleen_ct_segmentation_local\"\n",
    "print(path_to_job_config)\n",
    "job_id = admin_session.submit_job(path_to_job_config)\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17ecd05",
   "metadata": {},
   "source": [
    "\n",
    "After the job has been submitted, you can navigate to the terminal that was used to start the POC deployment and see progress messages as the job executes.\n",
    "\n",
    "You can also use the admin session to query job status as in the previous POC notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4ce3be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Job Status\n",
    "jobs_output = admin_session.list_jobs()\n",
    "jobs_detail = admin_session.list_jobs(detailed=True)\n",
    "print(\"Job Status\")\n",
    "print(json.dumps((jobs_output), indent=2))\n",
    "print(\"\\nJob Detail\")\n",
    "print(json.dumps((jobs_detail), indent=2))\n",
    "\n",
    "# Job Metadata\n",
    "print(\"\\nJob Metadata\")\n",
    "admin_session.get_job_meta(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a9238",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "We covered a lot of ground in this tutorial, so let's quickly recap.\n",
    "\n",
    "MONAI provides a rich set of examples, pretrained models, data loaders, and transforms to streamline medical imaging workflows.  The MONAI Bundle format allows researchers and developers to provide a self-contained bundle that includes model definitions, training and evaluation configurations, along with any pre- and post- processing required to run end-to-end workflows.\n",
    "\n",
    "MONAI FL integration with NVIDIA FLARE provides reference executors that allow researchers and developers to easily translate a model developed locally using MONAI and packaged in a MONAI Bundle to a Federated Learning paradigm without the overhead of developing a custom federated learning application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvflare_example",
   "language": "python",
   "name": "nvflare_example"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
