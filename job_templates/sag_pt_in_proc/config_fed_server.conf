{
  # version of the configuration
  format_version = 2

  # task data filter: if filters are provided, the filter will filter the data flow out of server to client.
  task_data_filters =[]

  # task result filter: if filters are provided, the filter will filter the result flow out of client to server.
  task_result_filters = []

  # This assumes that there will be a "net.py" file with class name "Net".
  # If your model code is not in "net.py" and class name is not "Net", please modify here
  model_class_path = "net.Net"

  # workflows: Array of workflows the control the Federated Learning workflow lifecycle.
  # One can specify multiple workflows. The NVFLARE will run them in the order specified.
  workflows = [
      {
        # 1st workflow"
        id = "scatter_and_gather"

        # name = ScatterAndGather, path is the class path of the ScatterAndGather controller.
        path = "nvflare.app_common.workflows.scatter_and_gather.ScatterAndGather"
        args {
            # argument of the ScatterAndGather class.
            # min number of clients required for ScatterAndGather controller to move to the next round
            # during the workflow cycle. The controller will wait until the min_clients returned from clients
            # before move to the next step.
            min_clients = 2

            # number of global round of the training.
            num_rounds = 5

            # starting round is 0-based
            start_round = 0

            # after received min number of clients' result,
            # how much time should we wait further before move to the next step
            wait_time_after_min_received = 0

            # For ScatterAndGather, the server will aggregate the weights based on the client's result.
            # the aggregator component id is named here. One can use the this ID to find the corresponding
            # aggregator component listed below
            aggregator_id = "aggregator"

            # The Scatter and Gather controller use an persistor to load the model and save the model.
            # The persistent component can be identified by component ID specified here.
            persistor_id = "persistor"

            # Shareable to a communication message, i.e. shared between clients and server.
            # Shareable generator is a component that responsible to take the model convert to/from this communication message: Shareable.
            # The component can be identified via "shareable_generator_id"
            shareable_generator_id =  "shareable_generator"

            # train task name: client side needs to have an executor that handles this task
            train_task_name =  "train"

            # train timeout in second. If zero, meaning no timeout.
            train_timeout =  0
        }
      }
  ]

  # List of components used in the server side workflow.
  components = [
    {
      # This is the persistence component used in above workflow.
      # PTFileModelPersistor is a Pytorch persistor which save/read the model to/from file.

      id = "persistor"
      path = "nvflare.app_opt.pt.file_model_persistor.PTFileModelPersistor"

      # the persitor class take model class as argument
      # This imply that the model is initialized from the server-side.
      # The initialized model will be broadcast to all the clients to start the training.
      args.model.path = "{model_class_path}"
    },
    {
      # This is the generator that convert the model to shareable communication message structure used in workflow
      id = "shareable_generator"
      path = "nvflare.app_common.shareablegenerators.full_model_shareable_generator.FullModelShareableGenerator"
      args = {}
    },
    {
      # This is the aggregator that perform the weighted average aggregation.
      # the aggregation is "in-time", so it doesn't wait for client results, but aggregates as soon as it received the data.
      id = "aggregator"
      path = "nvflare.app_common.aggregators.intime_accumulate_model_aggregator.InTimeAccumulateWeightedAggregator"
      args.expected_data_kind = "WEIGHT_DIFF"
    },
    {
      # This component is not directly used in Workflow.
      # it select the best model based on the incoming global validation metrics.
      id = "model_selector"
      path =  "nvflare.app_common.widgets.intime_model_selector.IntimeModelSelector"
      # need to make sure this "key_metric" match what server side received
      args.key_metric = "accuracy"
    },
    {
      id = "receiver"
      path = "nvflare.app_opt.tracking.tb.tb_receiver.TBAnalyticsReceiver"
      args.events = ["fed.analytix_log_stats"]
    },

    {
          id = "mlflow_receiver"
          path = "nvflare.app_opt.tracking.mlflow.mlflow_receiver.MLflowReceiver"
          args {
            # tracking_uri = "http://0.0.0.0:5000"
            tracking_uri =  "file:///{WORKSPACE}/{JOB_ID}/mlruns"
            kw_args {
              experiment_name =  "nvflare-sag-pt-experiment"
              run_name = "nvflare-sag-pt-with-mlflow"
              experiment_tags {
                "mlflow.note.content": "## **NVFlare SAG PyTorch experiment with MLflow**"
              }
              run_tags {
                "mlflow.note.content" = "## Federated Experiment tracking with MLflow \n### Example of using **[NVIDIA FLARE](https://nvflare.readthedocs.io/en/main/index.html)** to train an image classifier using federated averaging ([FedAvg]([FedAvg](https://arxiv.org/abs/1602.05629))) and [PyTorch](https://pytorch.org/) as the deep learning training framework. This example also highlights the NVFlare streaming capability from the clients to the server.\n\n> **_NOTE:_** \n This example uses the *[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)* dataset and will load its data within the trainer code.\n"
              }
            }
            artifact_location = "artifacts"
            events = ["fed.analytix_log_stats"]
          }
        }
  ]

}
