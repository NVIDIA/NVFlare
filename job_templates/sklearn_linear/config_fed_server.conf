{
  # version of the configuration
  format_version = 2

  # task data filter: if filters are provided, the filter will filter the data flow out of server to client.
  task_data_filters = []

  # task result filter: if filters are provided, the filter will filter the result flow out of client to server.
  task_result_filters = []

  # workflows: Array of workflows the control the Federated Learning workflow lifecycle.
  # One can specify multiple workflows. The NVFLARE will run them in the order specified.
  workflows = [
      {
        # 1st workflow"
        id = "scatter_and_gather"
        name = "ScatterAndGather"
        args {
            # argument of the ScatterAndGather class.
            # min number of clients required for ScatterAndGather controller to move to the next round
            # during the workflow cycle. The controller will wait until the min_clients returned from clients
            # before move to the next step.
            min_clients = 3

            # number of global round of the training.
            num_rounds = 30

            # starting round is 0-based
            start_round = 0

            # after received min number of clients' result,
            # how much time should we wait further before move to the next step
            wait_time_after_min_received = 0

            # For ScatterAndGather, the server will aggregate the weights based on the client's result.
            # the aggregator component id is named here. One can use the this ID to find the corresponding
            # aggregator component listed below
            aggregator_id = "aggregator"

            # The Scatter and Gather controller use an persistor to load the model and save the model.
            # The persistent component can be identified by component ID specified here.
            persistor_id = "persistor"

            # Shareable to a communication message, i.e. shared between clients and server.
            # Shareable generator is a component that responsible to take the model convert to/from this communication message: Shareable.
            # The component can be identified via "shareable_generator_id"
            shareable_generator_id = "shareable_generator"

            # train task name: Client will start training once received such task.
            train_task_name = "train"

            # train timeout in second. If zero, meaning no timeout.
            train_timeout = 0
        }
      }
  ]

  # List of components used in the server side workflow.
  components = [
    {
      # This is the persistence component used in above workflow.
      # JoblibModelParamPersistor is a Pytorch persistor which save/read the model to/from file with JobLib format.
      id = "persistor"
      path = "nvflare.app_opt.sklearn.joblib_model_param_persistor.JoblibModelParamPersistor"

      # the persitor class take initial model specs as argument
      # This imply that the model parameters are initialized from the server-side.
      # The model paramters will be broadcast to all the clients to set up local models for training.
      args {
        initial_params {
          n_classes = 2
          learning_rate = "constant" # learning_rate  default=’optimal’, ‘constant’: eta = eta0
          eta0 =  1e-5         # The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. T
          loss = log_loss      # ‘log_loss’ gives logistic regression, a probabilistic classifier.
          penalty = l2         # The penalty (aka regularization term) to be used. Defaults to ‘l2’ which is the standard regularizer for linear
          fit_intercept = True # Whether the intercept should be estimated or not. If False, the data is assumed to be already centered.
          max_iter = 1      # The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit method. Values must be in the range [1, inf).
        }
      }
    },
    {
      # This is the generator that convert the model to shareable communication message structure used in workflow
      id = "shareable_generator"
      name = "FullModelShareableGenerator"
      args = {}
    },
    {
      # This is the aggregator that perform the weighted average aggregation.
      # the aggregation is "in-time", so it doesn't wait for client results, but aggregates as soon as it received the data.
      id = "aggregator"
      name = "InTimeAccumulateWeightedAggregator"
      args.expected_data_kind = "WEIGHTS"
    }
  ]

}
